{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.utils as vision_utils\n",
    "import lpips\n",
    "from torchinfo import summary\n",
    "from torchvision.datasets import LSUN\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "from einops import rearrange\n",
    "\n",
    "from models3d import BiomeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the Factorized Visual Tokenization and Generation paper\n",
    "https://github.com/showlab/FQGAN/tree/main/tokenizer/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Minecraft Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockBiomeConverter:\n",
    "    def __init__(self, block_mappings=None, biome_mappings=None):\n",
    "        \"\"\"\n",
    "        Initialize with pre-computed mappings for both blocks and biomes\n",
    "        \n",
    "        Args:\n",
    "            block_mappings: dict containing 'index_to_block' and 'block_to_index'\n",
    "            biome_mappings: dict containing 'index_to_biome' and 'biome_to_index'\n",
    "        \"\"\"\n",
    "        self.index_to_block = block_mappings['index_to_block'] if block_mappings else None\n",
    "        self.block_to_index = block_mappings['block_to_index'] if block_mappings else None\n",
    "        self.index_to_biome = biome_mappings['index_to_biome'] if biome_mappings else None\n",
    "        self.biome_to_index = biome_mappings['biome_to_index'] if biome_mappings else None\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataset(cls, data_path):\n",
    "        \"\"\"Create mappings from a dataset file\"\"\"\n",
    "        data = np.load(data_path, allow_pickle=True)\n",
    "        voxels = data['voxels']\n",
    "        biomes = data['biomes']\n",
    "        \n",
    "        # Create block mappings (blocks are integers)\n",
    "        unique_blocks = np.unique(voxels)\n",
    "        block_to_index = {int(block): idx for idx, block in enumerate(unique_blocks)}\n",
    "        index_to_block = {idx: int(block) for idx, block in enumerate(unique_blocks)}\n",
    "        \n",
    "        # Create biome mappings (biomes are strings)\n",
    "        unique_biomes = np.unique(biomes)\n",
    "        biome_to_index = {str(biome): idx for idx, biome in enumerate(unique_biomes)}\n",
    "        index_to_biome = {idx: str(biome) for idx, biome in enumerate(unique_biomes)}\n",
    "        \n",
    "        block_mappings = {'index_to_block': index_to_block, 'block_to_index': block_to_index}\n",
    "        biome_mappings = {'index_to_biome': index_to_biome, 'biome_to_index': biome_to_index}\n",
    "        \n",
    "        return cls(block_mappings, biome_mappings)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_arrays(cls, voxels, biomes):\n",
    "        \"\"\"Create mappings directly from numpy arrays\"\"\"\n",
    "        # Create block mappings (blocks are integers)\n",
    "        unique_blocks = np.unique(voxels)\n",
    "        block_to_index = {int(block): idx for idx, block in enumerate(unique_blocks)}\n",
    "        index_to_block = {idx: int(block) for idx, block in enumerate(unique_blocks)}\n",
    "        \n",
    "        # Create biome mappings (biomes are strings)\n",
    "        unique_biomes = np.unique(biomes)\n",
    "        biome_to_index = {str(biome): idx for idx, biome in enumerate(unique_biomes)}\n",
    "        index_to_biome = {idx: str(biome) for idx, biome in enumerate(unique_biomes)}\n",
    "        \n",
    "        block_mappings = {'index_to_block': index_to_block, 'block_to_index': block_to_index}\n",
    "        biome_mappings = {'index_to_biome': index_to_biome, 'biome_to_index': biome_to_index}\n",
    "        \n",
    "        return cls(block_mappings, biome_mappings)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_mappings(cls, path):\n",
    "        \"\"\"Load pre-saved mappings\"\"\"\n",
    "        mappings = torch.load(path)\n",
    "        return cls(mappings['block_mappings'], mappings['biome_mappings'])\n",
    "    \n",
    "    def save_mappings(self, path):\n",
    "        \"\"\"Save mappings for later use\"\"\"\n",
    "        torch.save({\n",
    "            'block_mappings': {\n",
    "                'index_to_block': self.index_to_block,\n",
    "                'block_to_index': self.block_to_index\n",
    "            },\n",
    "            'biome_mappings': {\n",
    "                'index_to_biome': self.index_to_biome,\n",
    "                'biome_to_index': self.biome_to_index\n",
    "            }\n",
    "        }, path)\n",
    "    \n",
    "    def convert_to_original_blocks(self, data):\n",
    "        \"\"\"\n",
    "        Convert from indices back to original block IDs.\n",
    "        Handles both one-hot encoded and already-indexed data.\n",
    "        \n",
    "        Args:\n",
    "            data: torch.Tensor of either:\n",
    "                - one-hot encoded blocks [B, C, H, W, D] or [C, H, W, D]\n",
    "                - indexed blocks [B, H, W, D] or [H, W, D]\n",
    "        Returns:\n",
    "            torch.Tensor of original block IDs with shape [B, H, W, D] or [H, W, D]\n",
    "        \"\"\"\n",
    "        # If one-hot encoded (dim == 5 or first dim == num_blocks), convert to indices first\n",
    "        if len(data.shape) == 5 or (len(data.shape) == 4 and data.shape[0] == len(self.block_to_index)):\n",
    "            data = torch.argmax(data, dim=1 if len(data.shape) == 5 else 0)\n",
    "        \n",
    "        # Now convert indices to original blocks\n",
    "        if len(data.shape) == 4:  # Batch dimension present\n",
    "            return torch.tensor([[[[self.index_to_block[int(b)] \n",
    "                                for b in row]\n",
    "                                for row in layer]\n",
    "                                for layer in slice_]\n",
    "                                for slice_ in data])\n",
    "        else:  # No batch dimension\n",
    "            return torch.tensor([[[self.index_to_block[int(b)] \n",
    "                                for b in row]\n",
    "                                for row in layer]\n",
    "                                for layer in data])\n",
    "\n",
    "    def convert_to_original_biomes(self, data):\n",
    "        \"\"\"\n",
    "        Convert from indices back to original biome strings.\n",
    "        Handles both one-hot encoded and already-indexed data.\n",
    "        \n",
    "        Args:\n",
    "            data: torch.Tensor of either:\n",
    "                - one-hot encoded biomes [B, C, H, W, D] or [C, H, W, D]\n",
    "                - indexed biomes [B, H, W, D] or [H, W, D]\n",
    "        Returns:\n",
    "            numpy array of original biome strings with shape [B, H, W, D] or [H, W, D]\n",
    "        \"\"\"\n",
    "        # If one-hot encoded (dim == 5 or first dim == num_biomes), convert to indices first\n",
    "        if len(data.shape) == 5 or (len(data.shape) == 4 and data.shape[0] == len(self.biome_to_index)):\n",
    "            data = torch.argmax(data, dim=1 if len(data.shape) == 5 else 0)\n",
    "        \n",
    "        # Now convert indices to original biomes\n",
    "        if len(data.shape) == 4:  # Batch dimension present\n",
    "            return np.array([[[[self.index_to_biome[int(b)] \n",
    "                            for b in row]\n",
    "                            for row in layer]\n",
    "                            for layer in slice_]\n",
    "                            for slice_ in data])\n",
    "        else:  # No batch dimension\n",
    "            return np.array([[[self.index_to_biome[int(b)] \n",
    "                            for b in row]\n",
    "                            for row in layer]\n",
    "                            for layer in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class MinecraftDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        data_path = Path(data_path)\n",
    "\n",
    "        # Try to load processed data first\n",
    "        # assert processed_data_path.exists() and mappings_path.exists()\n",
    "\n",
    "        print(\"Loading pre-processed data...\")\n",
    "        processed_data = torch.load(data_path)\n",
    "        # Only keep the chunks, discard biome data\n",
    "        self.processed_chunks = processed_data['chunks']\n",
    "        # Delete the biomes to free memory\n",
    "        del processed_data['biomes']\n",
    "        del processed_data\n",
    "        \n",
    "        \n",
    "        print(f\"Loaded {len(self.processed_chunks)} chunks of size {self.processed_chunks.shape[1:]}\")\n",
    "        print(f\"Number of unique block types: {self.processed_chunks.shape[1]}\")\n",
    "        print(f'Unique blocks: {torch.unique(torch.argmax(self.processed_chunks, dim=1)).tolist()}')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.processed_chunks[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_chunks)\n",
    "\n",
    "def get_minecraft_dataloaders(data_path, batch_size=32, val_split=0.1, num_workers=0, save_val_path=None):\n",
    "    \"\"\"\n",
    "    Creates training and validation dataloaders for Minecraft chunks.\n",
    "    \"\"\"\n",
    "    # Create dataset\n",
    "    dataset = MinecraftDataset(data_path)\n",
    "    \n",
    "    # Split into train and validation sets\n",
    "    val_size = int(val_split * len(dataset))\n",
    "    train_size = len(dataset) - val_size\n",
    "    \n",
    "    # Use a fixed seed for reproducibility\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "    \n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, \n",
    "        [train_size, val_size],\n",
    "        generator=generator\n",
    "    )\n",
    "\n",
    "    # Save validation data if path provided\n",
    "    if save_val_path:\n",
    "        print(f'saving validation dataset to file: {save_val_path}')\n",
    "        # Extract validation samples\n",
    "        val_samples = torch.stack([dataset.processed_chunks[i] for i in val_dataset.indices])\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(save_val_path), exist_ok=True)\n",
    "        \n",
    "        # Save validation data\n",
    "        torch.save({\n",
    "            'data': val_samples,\n",
    "            'indices': val_dataset.indices\n",
    "        }, save_val_path)\n",
    "        print(f\"Saved validation data to {save_val_path}\")\n",
    "    \n",
    "    # Create dataloaders with memory pinning\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    \n",
    "    print(f\"\\nDataloader details:\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Training batches: {len(train_loader)}\")\n",
    "    print(f\"Validation batches: {len(val_loader)}\")\n",
    "    \n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render Minecraft data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinecraftVisualizer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the visualizer with the same block color mappings\"\"\"\n",
    "        self.blocks_to_cols = {\n",
    "            0: (0.5, 0.25, 0.0),    # light brown\n",
    "            10: 'black', # bedrock\n",
    "            29: \"#006400\", # cacutus\n",
    "            38: \"#B8860B\",  # clay\n",
    "            60: \"brown\",  # dirt\n",
    "            92: \"gold\",  # gold ore\n",
    "            93: \"green\",  # grass\n",
    "            115: \"brown\",  # ladder...?\n",
    "            119: (.02, .28, .16, 0.8),  # transparent forest green (RGBA) for leaves\n",
    "            120: (.02, .28, .16, 0.8),  # leaves2\n",
    "            194: \"yellow\",  # sand\n",
    "            217: \"gray\",  # stone\n",
    "            240: (0.0, 0.0, 1.0, 0.4),  # water\n",
    "            227: (0.0, 1.0, 0.0, .3), # tall grass\n",
    "            237: (0.33, 0.7, 0.33, 0.3), # vine\n",
    "            40: \"#2F4F4F\",  # coal ore\n",
    "            62: \"#228B22\",  # double plant\n",
    "            108: \"#BEBEBE\",  # iron ore\n",
    "            131: \"saddlebrown\",  # log1\n",
    "            132: \"saddlebrown\",  #log2\n",
    "            95: \"lightgray\",  # gravel\n",
    "            243: \"wheat\",  # wheat. lmao\n",
    "            197: \"limegreen\",  # sapling\n",
    "            166: \"orange\",  #pumpkin\n",
    "            167: \"#FF8C00\",  # pumpkin stem\n",
    "            184: \"#FFA07A\",  # red flower\n",
    "            195: \"tan\",  # sandstone\n",
    "            250: \"white\",  #wool \n",
    "            251: \"gold\",   #yellow flower\n",
    "        }\n",
    "\n",
    "    def visualize_chunk(self, voxels, ax=None):\n",
    "        \"\"\"\n",
    "        Create a 3D visualization of a Minecraft chunk using the original plotting logic.\n",
    "        \n",
    "        Args:\n",
    "            voxels: torch.Tensor [C,H,W,D] (one-hot) or numpy.ndarray [H,W,D] (block IDs)\n",
    "            ax: Optional matplotlib axis\n",
    "        \"\"\"\n",
    "        # Convert one-hot to block IDs if needed\n",
    "        if isinstance(voxels, torch.Tensor):\n",
    "            if voxels.dim() == 4:  # One-hot encoded [C,H,W,D]\n",
    "                voxels = voxels.detach().cpu()\n",
    "                voxels = torch.argmax(voxels, dim=0).numpy()\n",
    "            else:\n",
    "                voxels = voxels.detach().cpu().numpy()\n",
    "\n",
    "        # Apply the same transformations as original\n",
    "        voxels = voxels.transpose(2, 0, 1)\n",
    "        # Rotate the voxels 90 degrees around the height axis\n",
    "        voxels = np.rot90(voxels, 1, (0, 1))\n",
    "\n",
    "        # Create axis if not provided\n",
    "        if ax is None:\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "        # Plot non-air blocks\n",
    "        other_vox = (voxels != 5) & (voxels != -1)\n",
    "        \n",
    "        # Plot each block type with its color\n",
    "        for block_id in np.unique(voxels[other_vox]):\n",
    "            if block_id not in self.blocks_to_cols:\n",
    "                # print(f\"Unknown block id: {block_id}\")\n",
    "                continue\n",
    "            ax.voxels(voxels == block_id, facecolors=self.blocks_to_cols[int(block_id)])\n",
    "            other_vox = other_vox & (voxels != block_id)\n",
    "\n",
    "        # Plot remaining blocks in red with black edges\n",
    "        ax.voxels(other_vox, edgecolor=\"k\", facecolor=\"red\")\n",
    "        \n",
    "        return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_minecraft(vis, mc_visualizer, data, win_name=\"minecraft_display\", title=\"Minecraft Chunks\", nrow=4, save_path=None):\n",
    "    \"\"\"\n",
    "    Display or save multiple minecraft chunks.\n",
    "    \n",
    "    Args:\n",
    "        vis: Visdom instance (can be None if only saving)\n",
    "        data: Tensor of shape [B, 256, 20, 20, 20] or [B, 20, 20, 20]\n",
    "        win_name: Window name for visdom\n",
    "        title: Title for the plot\n",
    "        nrow: Number of images per row\n",
    "        save_path: If provided, saves the figure to this path\n",
    "    \"\"\"\n",
    "    # Convert to original block IDs for visualization\n",
    "    # data = mc_dataset.convert_to_original_blocks(data)\n",
    "    # Convert to one-hot if needed\n",
    "    if len(data.shape) == 4:  # [B, 20, 20, 20]\n",
    "        data = F.one_hot(data.long(), num_classes=256).permute(0, 4, 1, 2, 3).float()\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    batch_size = min(data.shape[0], 16)  # Display up to 16 chunks\n",
    "    ncols = nrow\n",
    "    nrows = (batch_size + ncols - 1) // ncols\n",
    "    \n",
    "    fig = plt.figure(figsize=(4*ncols, 4*nrows))\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        ax = fig.add_subplot(nrows, ncols, i+1, projection='3d')\n",
    "        mc_visualizer.visualize_chunk(data[i], ax)\n",
    "        ax.set_title(f'Chunk {i}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "    \n",
    "    # Display in visdom if instance provided\n",
    "    if vis is not None:\n",
    "        # Convert matplotlib figure to numpy array for visdom\n",
    "        canvas = fig.canvas\n",
    "        canvas.draw()\n",
    "        width, height = canvas.get_width_height()\n",
    "        img_array = np.frombuffer(canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        img_array = img_array.reshape(height, width, 3)\n",
    "        \n",
    "        vis.image(\n",
    "            img_array.transpose(2, 0, 1),  # Convert to CHW format\n",
    "            win=win_name,\n",
    "            opts=dict(\n",
    "                title=title,\n",
    "                caption=f'Batch of {batch_size} chunks'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    plt.close(fig)\n",
    "\n",
    "def save_minecraft(data, mc_visualizer, mc_dataset, save_path, nrow=4, title=\"Minecraft Chunks\"):\n",
    "    \"\"\"\n",
    "    Save multiple minecraft chunks to a file.\n",
    "    \n",
    "    Args:\n",
    "        data: Tensor of shape [B, 256, 20, 20, 20] or [B, 20, 20, 20]\n",
    "        save_path: Path to save the image\n",
    "        nrow: Number of images per row\n",
    "        title: Title for the plot\n",
    "    \"\"\"\n",
    "    # Convert to original block IDs for visualization\n",
    "    data = mc_dataset.convert_to_original_blocks(data)\n",
    "    # Convert to one-hot if needed\n",
    "    if len(data.shape) == 4:  # [B, 20, 20, 20]\n",
    "        data = F.one_hot(data.long(), num_classes=256).permute(0, 4, 1, 2, 3).float()\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    batch_size = min(data.shape[0], 16)  # Save up to 16 chunks\n",
    "    ncols = nrow\n",
    "    nrows = (batch_size + ncols - 1) // ncols\n",
    "    \n",
    "    fig = plt.figure(figsize=(4*ncols, 4*nrows))\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        ax = fig.add_subplot(nrows, ncols, i+1, projection='3d')\n",
    "        mc_visualizer.visualize_chunk(data[i], ax)\n",
    "        ax.set_title(f'Chunk {i}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_minecraft_pyvista(vis, mc_visualizer, data, win_name=\"minecraft_display\", title=\"Minecraft Chunks\", nrow=4, save_path=None):\n",
    "    \"\"\"\n",
    "    Display or save multiple minecraft chunks using PyVista.\n",
    "    \"\"\"\n",
    "    # Convert to one-hot if needed\n",
    "    if len(data.shape) == 4:  # [B, 20, 20, 20]\n",
    "        data = F.one_hot(data.long(), num_classes=256).permute(0, 4, 1, 2, 3).float()\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    batch_size = min(data.shape[0], 16)  # Display up to 16 chunks\n",
    "    ncols = nrow\n",
    "    nrows = (batch_size + ncols - 1) // ncols\n",
    "    \n",
    "    # Calculate the size of the combined image\n",
    "    single_size = 400  # Size of each subplot in pixels\n",
    "    \n",
    "    # Create a list to store individual chunk images\n",
    "    chunk_images = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Use the visualizer's method to create the plot\n",
    "        plotter = mc_visualizer.visualize_chunk(data[i])\n",
    "        \n",
    "        # Render to image\n",
    "        img = plotter.screenshot(window_size=(single_size, single_size), \n",
    "                               transparent_background=True, \n",
    "                               return_img=True)\n",
    "        chunk_images.append(img)\n",
    "        plotter.close()\n",
    "    \n",
    "    # Combine images into a grid\n",
    "    grid_rows = []\n",
    "    for row in range(nrows):\n",
    "        row_images = chunk_images[row * ncols : (row + 1) * ncols]\n",
    "        # Pad the last row if needed\n",
    "        while len(row_images) < ncols:\n",
    "            row_images.append(np.zeros_like(chunk_images[0]))\n",
    "        grid_rows.append(np.concatenate(row_images, axis=1))\n",
    "    \n",
    "    combined_img = np.concatenate(grid_rows, axis=0)\n",
    "    \n",
    "    # Save if path provided\n",
    "     # Save if path provided\n",
    "    if save_path:\n",
    "        # Just save directly without trying to create directories\n",
    "        plt.imsave(save_path, combined_img)\n",
    "    \n",
    "    # Display in visdom if instance provided\n",
    "    if vis is not None:\n",
    "        vis.image(\n",
    "            combined_img.transpose(2, 0, 1),  # Convert to CHW format\n",
    "            win=win_name,\n",
    "            opts=dict(\n",
    "                title=title,\n",
    "                caption=f'Batch of {batch_size} chunks'\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "\n",
    "class MinecraftVisualizerPyVista:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize with same block color mappings\"\"\"\n",
    "        self.blocks_to_cols = {\n",
    "            0: (0.5, 0.25, 0.0),    # light brown\n",
    "            10: 'black', # bedrock\n",
    "            29: \"#006400\", # cacutus\n",
    "            38: \"#B8860B\",  # clay\n",
    "            60: \"brown\",  # dirt\n",
    "            92: \"gold\",  # gold ore\n",
    "            93: \"green\",  # grass\n",
    "            115: \"brown\",  # ladder...?\n",
    "            119: (.02, .28, .16, 0.9),  # transparent forest green (RGBA) for leaves\n",
    "            120: (.02, .28, .16, 0.9),  # leaves2\n",
    "            194: \"yellow\",  # sand\n",
    "            217: \"gray\",  # stone\n",
    "            240: (0.0, 0.0, 1.0, 0.4),  # water\n",
    "            227: (0.0, 1.0, 0.0, .3), # tall grass\n",
    "            237: (0.33, 0.7, 0.33, 0.3), # vine\n",
    "            40: \"#2F4F4F\",  # coal ore\n",
    "            62: \"#228B22\",  # double plant\n",
    "            108: \"#BEBEBE\",  # iron ore\n",
    "            131: \"saddlebrown\",  # log1\n",
    "            132: \"saddlebrown\",  #log2\n",
    "            95: \"lightgray\",  # gravel\n",
    "            243: \"wheat\",  # wheat\n",
    "            197: \"limegreen\",  # sapling\n",
    "            166: \"orange\",  #pumpkin\n",
    "            167: \"#FF8C00\",  # pumpkin stem\n",
    "            184: \"#FFA07A\",  # red flower\n",
    "            195: \"tan\",  # sandstone\n",
    "            250: \"white\",  #wool \n",
    "            251: \"gold\",   #yellow flower\n",
    "        }\n",
    "        try:\n",
    "            import panel as pn\n",
    "            pn.extension('vtk')\n",
    "            pv.set_jupyter_backend('trame')\n",
    "        except ImportError:\n",
    "            print(\"Please install panel with: pip install panel\")\n",
    "        \n",
    "    def visualize_chunk(self, voxels, plotter=None):\n",
    "        \"\"\"Visualize a single chunk with consistent styling\"\"\"\n",
    "        # Convert to numpy if needed\n",
    "        if isinstance(voxels, torch.Tensor):\n",
    "            if voxels.dim() == 4:  # One-hot encoded [C,H,W,D]\n",
    "                voxels = voxels.detach().cpu()\n",
    "                voxels = torch.argmax(voxels, dim=0).numpy()\n",
    "            else:\n",
    "                voxels = voxels.detach().cpu().numpy()\n",
    "                \n",
    "        # Apply the same transformations as original\n",
    "        voxels = voxels.transpose(2, 0, 1)\n",
    "        # Rotate the voxels 90 degrees around the height axis\n",
    "        voxels = np.rot90(voxels, 1, (0, 1))\n",
    "                \n",
    "        # Create grid\n",
    "        grid = pv.ImageData()\n",
    "        grid.dimensions = np.array(voxels.shape) + 1\n",
    "        grid.cell_data[\"values\"] = voxels.flatten(order=\"F\")\n",
    "        \n",
    "        # Create plotter if not provided\n",
    "        if plotter is None:\n",
    "            plotter = pv.Plotter(off_screen=True)\n",
    "        \n",
    "        # Remove existing lights\n",
    "        plotter.remove_all_lights()\n",
    "        \n",
    "        # Add the three-point lighting setup\n",
    "        plotter.add_light(pv.Light(\n",
    "            position=(1, -1, 1),\n",
    "            intensity=1.0,\n",
    "            color='white'\n",
    "        ))\n",
    "        \n",
    "        plotter.add_light(pv.Light(\n",
    "            position=(-1, 1, 0.5),\n",
    "            intensity=0.5,\n",
    "            color='white'\n",
    "        ))\n",
    "        \n",
    "        plotter.add_light(pv.Light(\n",
    "            position=(-0.5, -0.5, -1),\n",
    "            intensity=0.3,\n",
    "            color='white'\n",
    "        ))\n",
    "        \n",
    "        # Plot each block type\n",
    "        mask = (voxels != 5) & (voxels != -1)\n",
    "        unique_blocks = np.unique(voxels[mask])\n",
    "        \n",
    "        for block_id in unique_blocks:\n",
    "            threshold = grid.threshold([block_id-0.5, block_id+0.5])\n",
    "            if block_id in self.blocks_to_cols:\n",
    "                color = self.blocks_to_cols[int(block_id)]\n",
    "                opacity = 1.0 if isinstance(color, str) or len(color) == 3 else color[3]\n",
    "            else:\n",
    "                color = (1.0, 0.0, 0.0)\n",
    "                opacity = 0.2\n",
    "            \n",
    "            plotter.add_mesh(threshold, \n",
    "                        color=color,\n",
    "                        opacity=opacity,\n",
    "                        show_edges=True,\n",
    "                        edge_color='black',\n",
    "                        line_width=.2,\n",
    "                        edge_opacity=0.2,\n",
    "                        lighting=True)\n",
    "        \n",
    "        # Add dummy cube for bounds\n",
    "        outline = pv.Cube(bounds=(0, 24, 0, 24, 0, 24))\n",
    "        plotter.add_mesh(outline, opacity=0.0)\n",
    "        \n",
    "        # Add bounds with consistent settings\n",
    "        plotter.show_bounds(\n",
    "            grid='back',\n",
    "            location='back',\n",
    "            font_size=8,\n",
    "            bold=False,\n",
    "            font_family='arial',\n",
    "            use_2d=False,\n",
    "            bounds=[0, 24, 0, 24, 0, 24],\n",
    "            axes_ranges=[0, 24, 0, 24, 0, 24],\n",
    "            padding=0.0,\n",
    "            n_xlabels=2,\n",
    "            n_ylabels=2,\n",
    "            n_zlabels=2\n",
    "        )\n",
    "        \n",
    "        # Set camera position and zoom\n",
    "        plotter.camera_position = 'iso'\n",
    "        plotter.camera.zoom(1)\n",
    "        \n",
    "        return plotter\n",
    "    \n",
    "    def visualize_interactive(self, voxels):\n",
    "        # Convert to numpy if needed\n",
    "        if isinstance(voxels, torch.Tensor):\n",
    "            if voxels.dim() == 4:  # One-hot encoded [C,H,W,D]\n",
    "                voxels = voxels.detach().cpu()\n",
    "                voxels = torch.argmax(voxels, dim=0).numpy()\n",
    "            else:\n",
    "                voxels = voxels.detach().cpu().numpy()\n",
    "                \n",
    "        # Apply the same transformations as original\n",
    "        voxels = voxels.transpose(2, 0, 1)\n",
    "        # Rotate the voxels 90 degrees around the height axis\n",
    "        voxels = np.rot90(voxels, 1, (0, 1))\n",
    "                \n",
    "        # Create grid\n",
    "        grid = pv.ImageData()\n",
    "        grid.dimensions = np.array(voxels.shape) + 1\n",
    "        grid.cell_data[\"values\"] = voxels.flatten(order=\"F\")\n",
    "        \n",
    "        # Create plotter\n",
    "        plotter = pv.Plotter(notebook=True)\n",
    "        \n",
    "        # Remove existing lights\n",
    "        plotter.remove_all_lights()\n",
    "        \n",
    "        # Add custom lights\n",
    "        # Main light from top-front-right (sun-like)\n",
    "        # Add a headlight (light from camera position)\n",
    "        # Key light (main light, 45 degrees from front-right)\n",
    "        plotter.add_light(pv.Light(\n",
    "            position=(1, -1, 1),\n",
    "            intensity=1.0,\n",
    "            color='white'\n",
    "        ))\n",
    "        \n",
    "        # Fill light (softer light from opposite side)\n",
    "        plotter.add_light(pv.Light(\n",
    "            position=(-1, 1, 0.5),\n",
    "            intensity=0.5,\n",
    "            color='white'\n",
    "        ))\n",
    "        \n",
    "        # Back light (rim lighting from behind)\n",
    "        plotter.add_light(pv.Light(\n",
    "            position=(-0.5, -0.5, -1),\n",
    "            intensity=0.3,\n",
    "            color='white'\n",
    "    ))\n",
    "        \n",
    "        # Plot each block type\n",
    "        mask = (voxels != 5) & (voxels != -1)\n",
    "        unique_blocks = np.unique(voxels[mask])\n",
    "        \n",
    "        for block_id in unique_blocks:\n",
    "            threshold = grid.threshold([block_id-0.5, block_id+0.5])\n",
    "            if block_id in self.blocks_to_cols:\n",
    "                color = self.blocks_to_cols[int(block_id)]\n",
    "                opacity = 1.0 if isinstance(color, str) or len(color) == 3 else color[3]\n",
    "            else:\n",
    "                color = (1.0, 0.0, 0.0)\n",
    "                opacity = 0.2\n",
    "            \n",
    "            plotter.add_mesh(threshold, \n",
    "                        color=color,\n",
    "                        opacity=opacity,\n",
    "                        show_edges=True,\n",
    "                        edge_color='black',\n",
    "                        line_width=.2,   # Thin edges\n",
    "                        edge_opacity=0.2,\n",
    "                        lighting=True)\n",
    "        \n",
    "        # Add a dummy cube to force the bounds\n",
    "        outline = pv.Cube(bounds=(0, 24, 0, 24, 0, 24))\n",
    "        plotter.add_mesh(outline, opacity=0.0)  # Invisible cube to set bounds\n",
    "        \n",
    "        # Add clean axes with consistent range\n",
    "        plotter.show_bounds(\n",
    "            grid='back',\n",
    "            location='back',\n",
    "            # all_edges=True,\n",
    "            # ticks=None,\n",
    "            font_size=8,\n",
    "            bold=False,\n",
    "            font_family='arial',\n",
    "            use_2d=False,\n",
    "            bounds=[0, 24, 0, 24, 0, 24],\n",
    "            axes_ranges=[0, 24, 0, 24, 0, 24],\n",
    "            padding=0.0,\n",
    "            n_xlabels=2,\n",
    "            n_ylabels=2,\n",
    "            n_zlabels=2,\n",
    "            # show_xlabels=False,\n",
    "            # show_ylabels=False,\n",
    "            # show_zlabels=False\n",
    "        )\n",
    "        \n",
    "        # Set camera position and zoom\n",
    "        plotter.camera_position = 'iso'\n",
    "        plotter.camera.zoom(1)\n",
    "        \n",
    "        return plotter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Quantizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorized Nearest Neighbor Quantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_entropy_loss(affinity, loss_type=\"softmax\", temperature=0.01):\n",
    "#     flat_affinity = affinity.reshape(-1, affinity.shape[-1])\n",
    "#     flat_affinity /= temperature\n",
    "#     probs = F.softmax(flat_affinity, dim=-1)\n",
    "#     log_probs = F.log_softmax(flat_affinity + 1e-5, dim=-1)\n",
    "#     if loss_type == \"softmax\":\n",
    "#         target_probs = probs\n",
    "#     else:\n",
    "#         raise ValueError(\"Entropy loss {} not supported\".format(loss_type))\n",
    "#     avg_probs = torch.mean(target_probs, dim=0)\n",
    "#     avg_entropy = - torch.sum(avg_probs * torch.log(avg_probs + 1e-5))\n",
    "#     sample_entropy = - torch.mean(torch.sum(target_probs * log_probs, dim=-1))\n",
    "#     loss = sample_entropy - avg_entropy\n",
    "#     return loss\n",
    "\n",
    "# class FQVectorQuantizer(nn.Module):\n",
    "#     def __init__(self, n_e, e_dim, beta, entropy_loss_ratio, l2_norm, show_usage):\n",
    "#         super().__init__()\n",
    "#         # Same initialization as original\n",
    "#         self.n_e = n_e\n",
    "#         self.e_dim = e_dim\n",
    "#         self.beta = beta\n",
    "#         self.entropy_loss_ratio = entropy_loss_ratio\n",
    "#         self.l2_norm = l2_norm\n",
    "#         self.show_usage = show_usage\n",
    "\n",
    "#         self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
    "#         self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
    "#         if self.l2_norm:\n",
    "#             self.embedding.weight.data = F.normalize(self.embedding.weight.data, p=2, dim=-1)\n",
    "#         if self.show_usage:\n",
    "#             self.register_buffer(\"codebook_used\", nn.Parameter(torch.zeros(65536)))\n",
    "\n",
    "#     def forward(self, z):\n",
    "#         # reshape z -> (batch, height, width, depth, channel) and flatten\n",
    "#         z = torch.einsum('b c h w d -> b h w d c', z).contiguous()  # Changed permute to handle 3D\n",
    "#         z_flattened = z.view(-1, self.e_dim)\n",
    "\n",
    "#         if self.l2_norm:\n",
    "#             z = F.normalize(z, p=2, dim=-1)\n",
    "#             z_flattened = F.normalize(z_flattened, p=2, dim=-1)\n",
    "#             embedding = F.normalize(self.embedding.weight, p=2, dim=-1)\n",
    "#         else:\n",
    "#             embedding = self.embedding.weight\n",
    "\n",
    "#         d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n",
    "#             torch.sum(embedding**2, dim=1) - 2 * \\\n",
    "#             torch.einsum('bd,dn->bn', z_flattened, torch.einsum('n d -> d n', embedding))\n",
    "\n",
    "#         min_encoding_indices = torch.argmin(d, dim=1)\n",
    "#         z_q = embedding[min_encoding_indices].view(z.shape)\n",
    "\n",
    "#         # Rest of the function remains the same\n",
    "#         perplexity = None\n",
    "#         min_encodings = None\n",
    "#         vq_loss = None\n",
    "#         commit_loss = None\n",
    "#         entropy_loss = None\n",
    "#         codebook_usage = 0\n",
    "\n",
    "#         # calculate the losses even if we aren't training, otherwise we get Nones when trying to eval on validation\n",
    "#         vq_loss = torch.mean((z_q - z.detach()) ** 2)\n",
    "#         commit_loss = self.beta * torch.mean((z_q.detach() - z) ** 2)\n",
    "#         entropy_loss = self.entropy_loss_ratio * compute_entropy_loss(-d)\n",
    "\n",
    "#         if self.show_usage and self.training:\n",
    "#             cur_len = min_encoding_indices.shape[0]\n",
    "#             self.codebook_used[:-cur_len] = self.codebook_used[cur_len:].clone()\n",
    "#             self.codebook_used[-cur_len:] = min_encoding_indices\n",
    "#             codebook_usage = len(torch.unique(self.codebook_used)) / self.n_e\n",
    "#         else:\n",
    "#             codebook_usage = 0\n",
    "\n",
    "#         # if self.training:\n",
    "#         #     vq_loss = torch.mean((z_q - z.detach()) ** 2)\n",
    "#         #     commit_loss = self.beta * torch.mean((z_q.detach() - z) ** 2)\n",
    "#         #     entropy_loss = self.entropy_loss_ratio * compute_entropy_loss(-d)\n",
    "\n",
    "#         # preserve gradients\n",
    "#         z_q = z + (z_q - z).detach()\n",
    "\n",
    "#         # reshape back to match original input shape\n",
    "#         z_q = torch.einsum('b h w d c -> b c h w d', z_q)  # Changed permute to handle 3D\n",
    "\n",
    "#         return z_q, (vq_loss, commit_loss, entropy_loss, codebook_usage), (perplexity, min_encodings, min_encoding_indices)\n",
    "\n",
    "#     def get_codebook_entry(self, indices, shape=None, channel_first=True):\n",
    "#         # shape = (batch, channel, height, width, depth) if channel_first else (batch, height, width, depth, channel)\n",
    "#         if self.l2_norm:\n",
    "#             embedding = F.normalize(self.embedding.weight, p=2, dim=-1)\n",
    "#         else:\n",
    "#             embedding = self.embedding.weight\n",
    "#         z_q = embedding[indices]\n",
    "\n",
    "#         if shape is not None:\n",
    "#             if channel_first:\n",
    "#                 z_q = z_q.reshape(shape[0], shape[2], shape[3], shape[4], shape[1])\n",
    "#                 # reshape back to match original input shape\n",
    "#                 z_q = z_q.permute(0, 4, 1, 2, 3).contiguous()\n",
    "#             else:\n",
    "#                 z_q = z_q.view(shape)\n",
    "#         return z_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorizer EMA Quantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FactorizedEMAQuantizer(nn.Module):\n",
    "#     def __init__(self, structure_codebook_size, style_codebook_size, emb_dim, decay=0.99):\n",
    "#         super().__init__()\n",
    "#         self.structure_codebook_size = structure_codebook_size\n",
    "#         self.style_codebook_size = style_codebook_size\n",
    "#         self.emb_dim = emb_dim\n",
    "#         self.decay = decay\n",
    "\n",
    "#         # Structure codebook\n",
    "#         self.register_buffer('structure_cluster_size', torch.zeros(structure_codebook_size))\n",
    "#         self.register_buffer('structure_embedding_avg', torch.zeros(structure_codebook_size, emb_dim))\n",
    "#         self.register_buffer('structure_embedding', torch.randn(structure_codebook_size, emb_dim))\n",
    "        \n",
    "#         # Style codebook\n",
    "#         self.register_buffer('style_cluster_size', torch.zeros(style_codebook_size))\n",
    "#         self.register_buffer('style_embedding_avg', torch.zeros(style_codebook_size, emb_dim))\n",
    "#         self.register_buffer('style_embedding', torch.randn(style_codebook_size, emb_dim))\n",
    "\n",
    "#     def forward(self, z):\n",
    "#         # Quantize with structure codebook\n",
    "#         struct_encoding_indices, struct_encodings = self.quantize(\n",
    "#             z, \n",
    "#             self.structure_embedding,\n",
    "#             self.structure_cluster_size,\n",
    "#             self.structure_embedding_avg,\n",
    "#             self.structure_codebook_size\n",
    "#         )\n",
    "        \n",
    "#         # Quantize with style codebook\n",
    "#         style_encoding_indices, style_encodings = self.quantize(\n",
    "#             z,\n",
    "#             self.style_embedding,\n",
    "#             self.style_cluster_size,\n",
    "#             self.style_embedding_avg,\n",
    "#             self.style_codebook_size\n",
    "#         )\n",
    "\n",
    "#         # Calculate disentanglement loss\n",
    "#         struct_norm = F.normalize(struct_encodings, dim=-1)\n",
    "#         style_norm = F.normalize(style_encodings, dim=-1)\n",
    "#         disentangle_loss = torch.mean((struct_norm * style_norm).sum(-1) ** 2)\n",
    "\n",
    "#         # Average the encodings since they're in the same space\n",
    "#         z_q = (struct_encodings + style_encodings) / 2\n",
    "\n",
    "#         return z_q, disentangle_loss, {\n",
    "#             \"struct_indices\": struct_encoding_indices,\n",
    "#             \"style_indices\": style_encoding_indices\n",
    "#         }\n",
    "\n",
    "#     def quantize(self, z, embedding, cluster_size, embedding_avg, n_codes):\n",
    "#         # Reshape z -> (batch, height, width, depth, channel)\n",
    "#         z = torch.einsum('b c h w d -> b h w d c', z)\n",
    "#         z_flattened = z.reshape(-1, self.emb_dim)\n",
    "        \n",
    "#         # Distances to embeddings\n",
    "#         d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n",
    "#             torch.sum(embedding ** 2, dim=1) - \\\n",
    "#             2 * torch.einsum('bd,nd->bn', z_flattened, embedding)\n",
    "        \n",
    "#         # Find nearest codebook entries\n",
    "#         encoding_indices = torch.argmin(d, dim=1)\n",
    "#         encodings = F.one_hot(encoding_indices, n_codes).type_as(z_flattened)\n",
    "        \n",
    "#         # EMA update of embeddings\n",
    "#         if self.training:\n",
    "#             n_total = encodings.sum(0)\n",
    "#             cluster_size.data.mul_(self.decay).add_(n_total, alpha=1 - self.decay)\n",
    "            \n",
    "#             dw = torch.einsum('bn,bd->nd', encodings, z_flattened)\n",
    "#             embedding_avg.data.mul_(self.decay).add_(dw, alpha=1 - self.decay)\n",
    "\n",
    "#             n = cluster_size.sum()\n",
    "#             cluster_size_balanced = (cluster_size + 1e-5) / (n + n_codes * 1e-5) * n\n",
    "            \n",
    "#             embedding.data = embedding_avg / cluster_size_balanced.unsqueeze(1)\n",
    "        \n",
    "#         # Quantize z\n",
    "#         z_q = torch.matmul(encodings, embedding)\n",
    "#         z_q = z_q.view(z.shape)\n",
    "        \n",
    "#         # Reshape back\n",
    "#         z_q = torch.einsum('b h w d c -> b c h w d', z_q)\n",
    "        \n",
    "#         return encoding_indices, z_q\n",
    "\n",
    "#     def get_codebook_entry(self, indices, shape, codebook=\"structure\"):\n",
    "#         # Select appropriate codebook\n",
    "#         embedding = self.structure_embedding if codebook == \"structure\" else self.style_embedding\n",
    "        \n",
    "#         # Get quantized latents\n",
    "#         z_q = embedding[indices]\n",
    "\n",
    "#         if shape is not None:\n",
    "#             z_q = z_q.view(shape)\n",
    "\n",
    "#         return z_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EMAQuantizer(nn.Module):\n",
    "#     def __init__(self, codebook_size, emb_dim, decay=0.99, eps=1e-5):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.codebook_size = codebook_size\n",
    "#         self.emb_dim = emb_dim\n",
    "#         self.decay = decay\n",
    "#         self.eps = eps\n",
    "\n",
    "#         # Initialize embeddings with randn, transposed from original\n",
    "#         embed = torch.randn(emb_dim, codebook_size).t()\n",
    "#         self.register_buffer(\"embedding\", embed)\n",
    "#         self.register_buffer(\"cluster_size\", torch.zeros(codebook_size))\n",
    "#         self.register_buffer(\"embed_avg\", embed.clone())\n",
    "\n",
    "#     def forward(self, z):\n",
    "#         # Save input shape and flatten\n",
    "#         b, c, h, w, d = z.shape  # Now includes depth dimension\n",
    "#         z_flattened = z.permute(0, 2, 3, 4, 1).reshape(-1, self.emb_dim)\n",
    "        \n",
    "#         # Calculate distances\n",
    "#         dist = (\n",
    "#             z_flattened.pow(2).sum(1, keepdim=True)\n",
    "#             - 2 * z_flattened @ self.embedding.t()\n",
    "#             + self.embedding.pow(2).sum(1, keepdim=True).t()\n",
    "#         )\n",
    "        \n",
    "#         # Get closest encodings\n",
    "#         _, min_encoding_indices = (-dist).max(1)\n",
    "#         min_encodings = F.one_hot(min_encoding_indices, self.codebook_size).type(z_flattened.dtype)\n",
    "        \n",
    "#         # Get quantized latent vectors\n",
    "#         z_q = torch.matmul(min_encodings, self.embedding)\n",
    "        \n",
    "#         # EMA updates during training\n",
    "#         if self.training:\n",
    "#             embed_onehot_sum = min_encodings.sum(0)\n",
    "#             embed_sum = z_flattened.transpose(0, 1) @ min_encodings\n",
    "            \n",
    "#             self.cluster_size.data.mul_(self.decay).add_(\n",
    "#                 embed_onehot_sum, alpha=1 - self.decay\n",
    "#             )\n",
    "#             self.embed_avg.data.mul_(self.decay).add_(\n",
    "#                 embed_sum.t(), alpha=1 - self.decay\n",
    "#             )\n",
    "\n",
    "#             n = self.cluster_size.sum()\n",
    "#             cluster_size = (\n",
    "#                 (self.cluster_size + self.eps) / (n + self.codebook_size * self.eps) * n\n",
    "#             )\n",
    "            \n",
    "#             embed_normalized = self.embed_avg / cluster_size.unsqueeze(1)\n",
    "#             self.embedding.data.copy_(embed_normalized)\n",
    "            \n",
    "#         # Reshape z_q and apply straight-through estimator\n",
    "#         z_q = z_q.view(b, h, w, d, c)  # Added depth dimension\n",
    "#         z_q = z_q.permute(0, 4, 1, 2, 3).contiguous()  # [B, C, H, W, D]\n",
    "        \n",
    "#         # Straight-through estimator\n",
    "#         z_q = z + (z_q - z).detach()\n",
    "        \n",
    "#         # Calculate perplexity\n",
    "#         e_mean = torch.mean(min_encodings, dim=0)\n",
    "#         perplexity = torch.exp(-torch.sum(e_mean * torch.log(e_mean + 1e-10)))\n",
    "\n",
    "#         return z_q, torch.tensor(0.0, device=z.device), {\n",
    "#             \"perplexity\": perplexity,\n",
    "#             \"min_encodings\": min_encodings,\n",
    "#             \"min_encoding_indices\": min_encoding_indices.view(b, h, w, d),\n",
    "#             \"mean_distance\": dist.mean()\n",
    "#         }\n",
    "\n",
    "#     def get_codebook_entry(self, indices, shape):\n",
    "#         min_encodings = F.one_hot(indices, self.codebook_size).type(torch.float)\n",
    "#         z_q = torch.matmul(min_encodings, self.embedding)\n",
    "\n",
    "#         if shape is not None:\n",
    "#             z_q = z_q.view(shape).permute(0, 4, 1, 2, 3).contiguous()\n",
    "\n",
    "#         return z_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorized Adapter Head\n",
    "Processes encoder features before they go into each sub-codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResidualAttentionBlock(nn.Module):\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             d_model,\n",
    "#             n_head,\n",
    "#             mlp_ratio=4.0,\n",
    "#             act_layer=nn.GELU,\n",
    "#             norm_layer=nn.LayerNorm\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.ln_1 = norm_layer(d_model)\n",
    "#         self.attn = nn.MultiheadAttention(d_model, n_head)\n",
    "#         self.mlp_ratio = mlp_ratio\n",
    "#         if mlp_ratio > 0:\n",
    "#             self.ln_2 = norm_layer(d_model)\n",
    "#             mlp_width = int(d_model * mlp_ratio)\n",
    "#             self.mlp = nn.Sequential(OrderedDict([\n",
    "#                 (\"c_fc\", nn.Linear(d_model, mlp_width)),\n",
    "#                 (\"gelu\", act_layer()),\n",
    "#                 (\"c_proj\", nn.Linear(mlp_width, d_model))\n",
    "#             ]))\n",
    "\n",
    "#     def attention(self, x: torch.Tensor):\n",
    "#         return self.attn(x, x, x, need_weights=False)[0]\n",
    "\n",
    "#     def forward(self, x: torch.Tensor):\n",
    "#         attn_output = self.attention(x=self.ln_1(x))\n",
    "#         x = x + attn_output\n",
    "#         if self.mlp_ratio > 0:\n",
    "#             x = x + self.mlp(self.ln_2(x))\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FactorizedAdapter(nn.Module):\n",
    "#     def __init__(self, down_factor):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Modified for 3D: grid_size now represents volume size\n",
    "#         self.grid_size = 24 // down_factor  # volume size // down-sample ratio\n",
    "#         self.width = 256  # same dim as VQ encoder output\n",
    "#         self.num_layers = 6\n",
    "#         self.num_heads = 8\n",
    "\n",
    "#         scale = self.width ** -0.5\n",
    "#         # Modified for 3D: positional embedding now handles cubic volume\n",
    "#         self.positional_embedding = nn.Parameter(scale * torch.randn(self.grid_size ** 3, self.width))\n",
    "#         self.ln_pre = nn.LayerNorm(self.width)\n",
    "#         self.transformer = nn.ModuleList([\n",
    "#             ResidualAttentionBlock(self.width, self.num_heads, mlp_ratio=4.0)\n",
    "#             for _ in range(self.num_layers)\n",
    "#         ])\n",
    "#         self.ln_post = nn.LayerNorm(self.width)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Modified for 3D: reshape from 5D to sequence\n",
    "#         h = x.shape[-1]  # depth dimension\n",
    "#         x = rearrange(x, 'b c h w d -> b (h w d) c')  # flatten 3D volume to sequence\n",
    "\n",
    "#         x = x + self.positional_embedding.to(x.dtype)\n",
    "#         x = self.ln_pre(x)\n",
    "#         x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "#         for transformer in self.transformer:\n",
    "#             x = transformer(x)\n",
    "#         x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "#         x = self.ln_post(x)\n",
    "\n",
    "#         # Modified for 3D: reshape back to 5D\n",
    "#         x = rearrange(\n",
    "#             x, \n",
    "#             'b (h w d) c -> b c h w d', \n",
    "#             h=self.grid_size, \n",
    "#             w=self.grid_size, \n",
    "#             d=self.grid_size\n",
    "#         )\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Down/Up Sample Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Downsample(nn.Module):\n",
    "#     def __init__(self, in_channels):\n",
    "#         super().__init__()\n",
    "#         self.conv = torch.nn.Conv3d(in_channels, in_channels, kernel_size=3, stride=2, padding=0)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         pad = (0, 1, 0, 1, 0, 1)  # Padding for all 3 dimensions\n",
    "#         x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0) #   padding the right and the bottom with 0s\n",
    "#         x = self.conv(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class Upsample(nn.Module):\n",
    "#     def __init__(self, in_channels):\n",
    "#         super().__init__()\n",
    "#         self.conv = nn.Conv3d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n",
    "#         x = self.conv(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Res Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize(in_channels):\n",
    "#     return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)   #   divides the channels into 32 groups, and normalizes each group. More effective for smaller batch size than batch norm\n",
    "\n",
    "# @torch.jit.script\n",
    "# def swish(x):\n",
    "#     return x*torch.sigmoid(x)   #  swish activation function, compiled using torch.jit.script. Smooth, non-linear activation function, works better than ReLu in some cases. swish (x) = x * sigmoid(x)\n",
    "\n",
    "# class ResBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels=None):\n",
    "#         super(ResBlock, self).__init__()\n",
    "#         self.in_channels = in_channels\n",
    "#         self.out_channels = in_channels if out_channels is None else out_channels\n",
    "#         self.norm1 = normalize(in_channels)\n",
    "#         self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "#         self.norm2 = normalize(out_channels)\n",
    "#         self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "#         self.conv_out = nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "#     def forward(self, x_in):\n",
    "#         x = x_in\n",
    "#         x = self.norm1(x)\n",
    "#         x = swish(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.norm2(x)\n",
    "#         x = swish(x)\n",
    "#         x = self.conv2(x)\n",
    "#         if self.in_channels != self.out_channels:\n",
    "#             x_in = self.conv_out(x_in)\n",
    "\n",
    "#         return x + x_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AttnBlock(nn.Module):\n",
    "#     def __init__(self, in_channels):\n",
    "#         super().__init__()\n",
    "#         self.in_channels = in_channels\n",
    "\n",
    "#         self.norm = normalize(in_channels)\n",
    "#         # Convert all 2D convolutions to 3D\n",
    "#         self.q = torch.nn.Conv3d(\n",
    "#             in_channels,\n",
    "#             in_channels,\n",
    "#             kernel_size=1,\n",
    "#             stride=1,\n",
    "#             padding=0\n",
    "#         )\n",
    "#         self.k = torch.nn.Conv3d(\n",
    "#             in_channels,\n",
    "#             in_channels,\n",
    "#             kernel_size=1,\n",
    "#             stride=1,\n",
    "#             padding=0\n",
    "#         )\n",
    "#         self.v = torch.nn.Conv3d(\n",
    "#             in_channels,\n",
    "#             in_channels,\n",
    "#             kernel_size=1,\n",
    "#             stride=1,\n",
    "#             padding=0\n",
    "#         )\n",
    "#         self.proj_out = torch.nn.Conv3d(\n",
    "#             in_channels,\n",
    "#             in_channels,\n",
    "#             kernel_size=1,\n",
    "#             stride=1,\n",
    "#             padding=0\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         h_ = x\n",
    "#         h_ = self.norm(h_)\n",
    "#         q = self.q(h_)\n",
    "#         k = self.k(h_)\n",
    "#         v = self.v(h_)\n",
    "\n",
    "#         # compute attention\n",
    "#         b, c, h, w, d = q.shape\n",
    "#         q = q.reshape(b, c, h*w*d)    # Flatten all spatial dimensions\n",
    "#         q = q.permute(0, 2, 1)        # b, hwd, c\n",
    "#         k = k.reshape(b, c, h*w*d)    # b, c, hwd\n",
    "#         w_ = torch.bmm(q, k)          # b, hwd, hwd    \n",
    "#         w_ = w_ * (int(c)**(-0.5))    # Scale dot products\n",
    "#         w_ = F.softmax(w_, dim=2)     # Softmax over spatial positions\n",
    "\n",
    "#         # attend to values\n",
    "#         v = v.reshape(b, c, h*w*d)\n",
    "#         w_ = w_.permute(0, 2, 1)      # b, hwd, hwd (first hwd of k, second of q)\n",
    "#         h_ = torch.bmm(v, w_)         # b, c, hwd\n",
    "#         h_ = h_.reshape(b, c, h, w, d) # Restore spatial structure\n",
    "\n",
    "#         h_ = self.proj_out(h_)\n",
    "\n",
    "#         return x + h_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Normalize(in_channels, norm_type='group'):\n",
    "#     assert norm_type in ['group', 'batch']\n",
    "#     if norm_type == 'group':\n",
    "#         return nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n",
    "#     elif norm_type == 'batch':\n",
    "#         return nn.SyncBatchNorm(in_channels)\n",
    "    \n",
    "# def nonlinearity(x):\n",
    "#     # swish\n",
    "#     return x*torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, in_channels, nf, out_channels, ch_mult, num_res_blocks, resolution):\n",
    "#         super().__init__()\n",
    "#         self.nf = nf\n",
    "#         self.num_resolutions = len(ch_mult)\n",
    "#         self.num_res_blocks = num_res_blocks\n",
    "#         self.resolution = resolution\n",
    "\n",
    "#         self.conv_in = nn.Conv3d(in_channels, nf, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "#         in_ch_mult = (1,) + tuple(ch_mult)\n",
    "\n",
    "#         self.conv_blocks = nn.ModuleList()\n",
    "#         for i_level in range(self.num_resolutions):\n",
    "#             conv_block = nn.Module()\n",
    "#             # res & attn\n",
    "#             res_block = nn.ModuleList()\n",
    "#             attn_block = nn.ModuleList()\n",
    "#             block_in = nf * in_ch_mult[i_level]\n",
    "#             block_out = nf * ch_mult[i_level]\n",
    "#             for _ in range(self.num_res_blocks):\n",
    "#                 res_block.append(ResBlock(block_in, block_out))\n",
    "#                 block_in = block_out\n",
    "#                 if i_level == self.num_resolutions - 1:\n",
    "#                     attn_block.append(AttnBlock(block_in))\n",
    "#             conv_block.res = res_block\n",
    "#             conv_block.attn = attn_block\n",
    "#             # downsample\n",
    "#             if i_level != self.num_resolutions-1:\n",
    "#                 conv_block.downsample = Downsample(block_in)\n",
    "#             self.conv_blocks.append(conv_block)\n",
    "\n",
    "#         # middle\n",
    "#         self.mid = nn.ModuleList()\n",
    "#         self.mid.append(ResBlock(block_in, block_in))\n",
    "#         self.mid.append(AttnBlock(block_in))\n",
    "#         self.mid.append(ResBlock(block_in, block_in))\n",
    "\n",
    "\n",
    "#         if self.num_resolutions == 5:\n",
    "#             down_factor = 16\n",
    "#         elif self.num_resolutions == 4:\n",
    "#             down_factor = 8\n",
    "#         elif self.num_resolutions == 3:\n",
    "#             down_factor = 4\n",
    "#         else:\n",
    "#             raise NotImplementedError\n",
    "        \n",
    "#         # semantic head\n",
    "#         self.style_head = nn.ModuleList()\n",
    "#         self.style_head.append(FactorizedAdapter(down_factor))\n",
    "\n",
    "#         # structural details head\n",
    "#         self.structure_head = nn.ModuleList()\n",
    "#         self.structure_head.append(FactorizedAdapter(down_factor))\n",
    "\n",
    "#         # end\n",
    "#         self.norm_out_style = Normalize(block_in)\n",
    "#         self.conv_out_style = nn.Conv3d(block_in, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "#         self.norm_out_struct = Normalize(block_in)\n",
    "#         self.conv_out_struct = nn.Conv3d(block_in, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "#         # blocks = []\n",
    "#         # # Initial convolution - now 3D\n",
    "#         # blocks.append(\n",
    "#         #     nn.Conv3d(\n",
    "#         #         in_channels, \n",
    "#         #         nf, \n",
    "#         #         kernel_size=3, \n",
    "#         #         stride=1, \n",
    "#         #         padding=1\n",
    "#         #     )\n",
    "#         # )\n",
    "\n",
    "#         # # Residual and downsampling blocks, with attention on specified resolutions\n",
    "#         # for i in range(self.num_resolutions):\n",
    "#         #     block_in_ch = nf * in_ch_mult[i]\n",
    "#         #     block_out_ch = nf * ch_mult[i]\n",
    "            \n",
    "#         #     # Add ResBlocks\n",
    "#         #     for _ in range(self.num_res_blocks):\n",
    "#         #         blocks.append(ResBlock(block_in_ch, block_out_ch))\n",
    "#         #         block_in_ch = block_out_ch\n",
    "                \n",
    "#         #         # Add attention if we're at the right resolution\n",
    "#         #         if curr_res in attn_resolutions:\n",
    "#         #             blocks.append(AttnBlock(block_in_ch))\n",
    "\n",
    "#         #     # Add downsampling block if not the last resolution\n",
    "#         #     if i != self.num_resolutions - 1:\n",
    "#         #         blocks.append(Downsample(block_in_ch))\n",
    "#         #         curr_res = curr_res // 2\n",
    "\n",
    "#         # # Final blocks\n",
    "#         # blocks.append(ResBlock(block_in_ch, block_in_ch))\n",
    "#         # blocks.append(AttnBlock(block_in_ch))\n",
    "#         # blocks.append(ResBlock(block_in_ch, block_in_ch))\n",
    "\n",
    "#         # # Normalize and convert to latent size\n",
    "#         # blocks.append(normalize(block_in_ch))\n",
    "#         # blocks.append(\n",
    "#         #     nn.Conv3d(\n",
    "#         #         block_in_ch, \n",
    "#         #         out_channels, \n",
    "#         #         kernel_size=3, \n",
    "#         #         stride=1, \n",
    "#         #         padding=1\n",
    "#         #     )\n",
    "#         # )\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         h = self.conv_in(x)\n",
    "#         # downsampling\n",
    "#         for i_level, block in enumerate(self.conv_blocks):\n",
    "#             for i_block in range(self.num_res_blocks):\n",
    "#                 h = block.res[i_block](h)\n",
    "#                 if len(block.attn) > 0:\n",
    "#                     h = block.attn[i_block](h)\n",
    "#             if i_level != self.num_resolutions - 1:\n",
    "#                 h = block.downsample(h)\n",
    "        \n",
    "#         # middle\n",
    "#         for mid_block in self.mid:\n",
    "#             h = mid_block(h)\n",
    "#         h_style = h\n",
    "#         h_struct = h\n",
    "\n",
    "#         # style head\n",
    "#         for blk in self.style_head:\n",
    "#             h_style = blk(h_style)\n",
    "\n",
    "#         h_style = self.norm_out_style(h_style)\n",
    "#         h_style = nonlinearity(h_style)\n",
    "#         h_style = self.conv_out_style(h_style)\n",
    "\n",
    "#         # structure head\n",
    "#         for blk in self.structure_head:\n",
    "#             h_struct = blk(h_struct)\n",
    "\n",
    "#         h_struct = self.norm_out_struct(h_struct)\n",
    "#         h_struct = nonlinearity(h_struct)\n",
    "#         h_struct = self.conv_out_struct(h_struct)\n",
    "\n",
    "#         return h_style, h_struct\n",
    "#         # for block in self.blocks:\n",
    "#         #     x = block(x)\n",
    "#         # return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator / Decoderclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, H, z_channels=256):\n",
    "#         super().__init__()\n",
    "#         self.nf = H.nf\n",
    "#         self.ch_mult = H.ch_mult\n",
    "#         self.num_resolutions = len(self.ch_mult)\n",
    "#         self.num_res_blocks = H.res_blocks\n",
    "#         self.resolution = H.img_size\n",
    "#         self.attn_resolutions = H.attn_resolutions\n",
    "#         self.in_channels = H.emb_dim\n",
    "#         self.out_channels = H.n_channels\n",
    "\n",
    "#         block_in = self.nf * self.ch_mult[self.num_resolutions-1]\n",
    "\n",
    "\n",
    "#         # z to block_in\n",
    "#         # self.conv_in = nn.Conv3d(z_channels * 2, block_in, kernel_size=3, stride=1, padding=1)\n",
    "#         #TODO: trying addition instead of concat\n",
    "#         self.conv_in = nn.Conv3d(z_channels, block_in, kernel_size=3, stride=1, padding=1)\n",
    "#         # middle\n",
    "#         self.mid = nn.ModuleList()\n",
    "#         self.mid.append(ResBlock(block_in, block_in))\n",
    "#         # self.mid.append(AttnBlock(block_in))\n",
    "#         self.mid.append(ResBlock(block_in, block_in))\n",
    "\n",
    "#         # upsampling\n",
    "#         self.conv_blocks = nn.ModuleList()\n",
    "#         for i_level in reversed(range(self.num_resolutions)):\n",
    "#             conv_block = nn.Module()\n",
    "#             # res & attn\n",
    "#             res_block = nn.ModuleList()\n",
    "#             attn_block = nn.ModuleList()\n",
    "#             block_out = self.nf * self.ch_mult[i_level]\n",
    "#             for _ in range(self.num_res_blocks + 1):\n",
    "#                 res_block.append(ResBlock(block_in, block_out))\n",
    "#                 block_in = block_out\n",
    "#                 if i_level == self.num_resolutions - 1:\n",
    "#                     attn_block.append(AttnBlock(block_in))\n",
    "#             conv_block.res = res_block\n",
    "#             conv_block.attn = attn_block\n",
    "#             # downsample\n",
    "#             if i_level != 0:\n",
    "#                 conv_block.upsample = Upsample(block_in)\n",
    "#             self.conv_blocks.append(conv_block)\n",
    "\n",
    "#         # end\n",
    "#         self.norm_out = Normalize(block_in)\n",
    "#         self.conv_out = nn.Conv3d(block_in, H.n_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "#         # block_in_ch = self.nf * self.ch_mult[-1]\n",
    "#         # curr_res = self.resolution // 2 ** (self.num_resolutions-1)\n",
    "\n",
    "#         # print(f'resolution: {self.resolution}, num_resolutions: {self.num_resolutions}, '\n",
    "#         #       f'num_res_blocks: {self.num_res_blocks}, attn_resolutions: {self.attn_resolutions}, '\n",
    "#         #       f'in_channels: {self.in_channels}, out_channels: {self.out_channels}, '\n",
    "#         #       f'block_in_ch: {block_in_ch}, curr_res: {curr_res}')\n",
    "\n",
    "#         # blocks = []\n",
    "#         # # Initial conv - now 3D\n",
    "#         # blocks.append(nn.Conv3d(self.in_channels, block_in_ch, kernel_size=3, stride=1, padding=1))\n",
    "\n",
    "#         # # Non-local attention block\n",
    "#         # blocks.append(ResBlock(block_in_ch, block_in_ch))\n",
    "#         # blocks.append(AttnBlock(block_in_ch))\n",
    "#         # blocks.append(ResBlock(block_in_ch, block_in_ch))\n",
    "\n",
    "#         # # Upsampling blocks\n",
    "#         # for i in reversed(range(self.num_resolutions)):\n",
    "#         #     block_out_ch = self.nf * self.ch_mult[i]\n",
    "\n",
    "#         #     for _ in range(self.num_res_blocks):\n",
    "#         #         blocks.append(ResBlock(block_in_ch, block_out_ch))\n",
    "#         #         block_in_ch = block_out_ch\n",
    "\n",
    "#         #         if curr_res in self.attn_resolutions:\n",
    "#         #             blocks.append(AttnBlock(block_in_ch))\n",
    "\n",
    "#         #     if i != 0:\n",
    "#         #         blocks.append(Upsample(block_in_ch))\n",
    "#         #         curr_res = curr_res * 2\n",
    "\n",
    "#         # # Final processing\n",
    "#         # blocks.append(normalize(block_in_ch))\n",
    "#         # blocks.append(nn.Conv3d(block_in_ch, self.out_channels, kernel_size=3, stride=1, padding=1))\n",
    "\n",
    "#         # self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "#         # # Used for calculating ELBO - fine tuned after training\n",
    "#         # self.logsigma = nn.Sequential(\n",
    "#         #     nn.Conv3d(block_in_ch, block_in_ch, kernel_size=3, stride=1, padding=1),\n",
    "#         #     nn.ReLU(),\n",
    "#         #     nn.Conv3d(block_in_ch, H.n_channels, kernel_size=1, stride=1, padding=0)\n",
    "#         # ).cuda()\n",
    "#     @property\n",
    "#     def last_layer(self):\n",
    "#         return self.conv_out.weight\n",
    "    \n",
    "#     def forward(self, z):\n",
    "#         #TODO: trying addition instead of concat\n",
    "#         B, C, H, W, D = z.shape\n",
    "#         z_style = z[:, :C//2]  # First half of channels\n",
    "#         z_struct = z[:, C//2:]  # Second half of channels\n",
    "        \n",
    "#         # Add the vectors\n",
    "#         z_add = z_style + z_struct\n",
    "\n",
    "#         # z to block_in\n",
    "#         h = self.conv_in(z_add)\n",
    "\n",
    "#         # middle\n",
    "#         for mid_block in self.mid:\n",
    "#             h = mid_block(h)\n",
    "        \n",
    "#         # upsampling\n",
    "#         for i_level, block in enumerate(self.conv_blocks):\n",
    "#             for i_block in range(self.num_res_blocks + 1):\n",
    "#                 h = block.res[i_block](h)\n",
    "#                 if len(block.attn) > 0:\n",
    "#                     h = block.attn[i_block](h)\n",
    "#             if i_level != self.num_resolutions - 1:\n",
    "#                 h = block.upsample(h)\n",
    "\n",
    "#         # end\n",
    "#         h = self.norm_out(h)\n",
    "#         h = nonlinearity(h)\n",
    "#         h = self.conv_out(h)\n",
    "\n",
    "#         return h\n",
    "#         # for block in self.blocks:\n",
    "#         #     x = block(x)\n",
    "#         # return x\n",
    "\n",
    "#     # def probabilistic(self, x):\n",
    "#     #     with torch.no_grad():\n",
    "#     #         for block in self.blocks[:-1]:\n",
    "#     #             x = block(x)\n",
    "#     #         mu = self.blocks[-1](x)\n",
    "#     #     logsigma = self.logsigma(x)\n",
    "#     #     return mu, logsigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator class (patch based, gives likelyhood each patch is real or fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PatchGAN3DDiscriminator(nn.Module):\n",
    "#     \"\"\"3D PatchGAN discriminator adapted for Minecraft voxel data\"\"\"\n",
    "#     def __init__(self, input_nc, ndf=64, n_layers=3):\n",
    "#         \"\"\"\n",
    "#         Parameters:\n",
    "#             input_nc (int)  -- number of input channels (block types)\n",
    "#             ndf (int)       -- number of filters in first conv layer\n",
    "#             n_layers (int)  -- number of conv layers\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         norm_layer = nn.BatchNorm3d\n",
    "\n",
    "        \n",
    "#         use_bias = norm_layer != nn.BatchNorm3d\n",
    "\n",
    "#         kw = 4  # kernel size\n",
    "#         padw = 1\n",
    "#         sequence = [\n",
    "#             nn.Conv3d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n",
    "#             nn.LeakyReLU(0.2, True)\n",
    "#         ]\n",
    "        \n",
    "#         nf_mult = 1\n",
    "#         nf_mult_prev = 1\n",
    "#         for n in range(1, n_layers):\n",
    "#             nf_mult_prev = nf_mult\n",
    "#             nf_mult = min(2 ** n, 8)\n",
    "#             sequence += [\n",
    "#                 nn.Conv3d(ndf * nf_mult_prev, ndf * nf_mult, \n",
    "#                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
    "#                 norm_layer(ndf * nf_mult),\n",
    "#                 nn.LeakyReLU(0.2, True)\n",
    "#             ]\n",
    "\n",
    "#         nf_mult_prev = nf_mult\n",
    "#         nf_mult = min(2 ** n_layers, 8)\n",
    "#         sequence += [\n",
    "#             nn.Conv3d(ndf * nf_mult_prev, ndf * nf_mult,\n",
    "#                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
    "#             norm_layer(ndf * nf_mult),\n",
    "#             nn.LeakyReLU(0.2, True)\n",
    "#         ]\n",
    "\n",
    "#         sequence += [\n",
    "#             nn.Conv3d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)\n",
    "#         ]\n",
    "\n",
    "#         self.main = nn.Sequential(*sequence)\n",
    "#         self.apply(self._init_weights)\n",
    "    \n",
    "#     def _init_weights(self, module):    \n",
    "#         if isinstance(module, nn.Conv3d):\n",
    "#             nn.init.normal_(module.weight.data, 0.0, 0.02)\n",
    "#         elif isinstance(module, nn.BatchNorm3d):\n",
    "#             nn.init.normal_(module.weight.data, 1.0, 0.02)\n",
    "#             nn.init.constant_(module.bias.data, 0)\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Discriminator3D(nn.Module):\n",
    "#     def __init__(self, input_nc=43, ndf=64, n_layers=3):\n",
    "#         \"\"\"Simple 3D convolutional discriminator\n",
    "        \n",
    "#         Args:\n",
    "#             input_nc (int): Number of input channels (number of block types)\n",
    "#             ndf (int): Number of filters in first conv layer\n",
    "#             n_layers (int): Number of conv layers\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "        \n",
    "#         # Initial convolution\n",
    "#         layers = [\n",
    "#             nn.Conv3d(input_nc, ndf, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.LeakyReLU(0.2, inplace=True)\n",
    "#         ]\n",
    "        \n",
    "#         # Increasing number of filters with each layer\n",
    "#         current_channels = ndf\n",
    "#         for i in range(n_layers - 1):\n",
    "#             next_channels = min(current_channels * 2, 512)\n",
    "#             layers.extend([\n",
    "#                 nn.Conv3d(current_channels, next_channels, \n",
    "#                          kernel_size=4, stride=2, padding=1),\n",
    "#                 nn.BatchNorm3d(next_channels),\n",
    "#                 nn.LeakyReLU(0.2, inplace=True)\n",
    "#             ])\n",
    "#             current_channels = next_channels\n",
    "        \n",
    "#         # Final layers\n",
    "#         layers.extend([\n",
    "#             nn.Conv3d(current_channels, current_channels,\n",
    "#                      kernel_size=4, stride=1, padding=1),\n",
    "#             nn.BatchNorm3d(current_channels),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Conv3d(current_channels, 1, kernel_size=4, stride=1, padding=1)\n",
    "#         ])\n",
    "        \n",
    "#         self.model = nn.Sequential(*layers)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             x: Input tensor of shape [B, C, D, H, W]\n",
    "#                 where C is number of block types (one-hot encoded)\n",
    "#         Returns:\n",
    "#             Tensor of shape [B, 1, D', H', W'] containing realness scores\n",
    "#         \"\"\"\n",
    "#         return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biome \"Tower\"\n",
    "Similar to the CLIP vision tower they use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BiomeFeatureModel(nn.Module):\n",
    "#     def __init__(self, biome_classifier_path):\n",
    "#         super().__init__()\n",
    "#         self.biome_classifier = BiomeClassifier(\n",
    "#             num_block_types=43,\n",
    "#             num_biomes=13,\n",
    "#             feature_dim=256\n",
    "#         ).cuda()\n",
    "        \n",
    "#         # Load pretrained weights for biome classifier\n",
    "#         self.biome_classifier.load_state_dict(torch.load(biome_classifier_path))\n",
    "#         self.biome_classifier.eval()\n",
    "#         for param in self.biome_classifier.parameters():\n",
    "#             param.requires_grad = False\n",
    "\n",
    "\n",
    "#     # @torch.no_grad()\n",
    "#     def forward(self, inputs, style_features):\n",
    "#         # Get biome features from real input\n",
    "#         biome_features = self.biome_classifier.get_intermediate_features(inputs)  # [B, C, 6, 6, 6]\n",
    "#         B, C, H, W, D = biome_features.shape\n",
    "#         biome_features = biome_features.permute(0, 2, 3, 4, 1)  # [B, 6, 6, 6, C]\n",
    "#         biome_features = biome_features.reshape(B, H*W*D, C)    # [B, 216, C]\n",
    "        \n",
    "#         # Normalize features\n",
    "#         # biome_features = F.normalize(biome_features, dim=-1).detach()\n",
    "#         # style_features = F.normalize(style_features, dim=-1)\n",
    "\n",
    "#         biome_features = biome_features.detach()\n",
    "\n",
    "#         # Print feature statistics\n",
    "#         # print(\"Biome features mean/std:\", biome_features.mean().item(), biome_features.std().item())\n",
    "#         # print(\"Style features mean/std:\", style_features.mean().item(), style_features.std().item())\n",
    "        \n",
    "#         # # Simple cosine similarity loss\n",
    "#         # loss = 1 - F.cosine_similarity(style_features, biome_features, dim=-1).mean()\n",
    "#         # or MSE loss\n",
    "#         loss = F.mse_loss(style_features, biome_features)\n",
    "#         return loss\n",
    "#     # @torch.no_grad()\n",
    "#     # def forward(self, inputs, style_features):\n",
    "#     #     # Extract biome features from real input using pretrained classifier\n",
    "#     #     biome_features = self.biome_classifier.get_intermediate_features(inputs)  # [B, C, 6, 6, 6]\n",
    "#     #     B, C, H, W, D = biome_features.shape\n",
    "#     #     biome_features = biome_features.permute(0, 2, 3, 4, 1)  # [B, 6, 6, 6, C]\n",
    "#     #     biome_features = biome_features.reshape(B, H*W*D, C)    # [B, 216, C]\n",
    "        \n",
    "#     #     # Normalize both features\n",
    "#     #     biome_features = F.normalize(biome_features, p=2, dim=-1).detach()  # Fixed target\n",
    "#     #     style_features = F.normalize(style_features, p=2, dim=-1)  # These will be optimized\n",
    "        \n",
    "#     #     # Compute similarity matrix and InfoNCE loss\n",
    "#     #     loss_mat = torch.bmm(style_features, biome_features.transpose(1, 2))\n",
    "#     #     loss_mat = loss_mat.exp()\n",
    "        \n",
    "#     #     loss_diag = torch.diagonal(loss_mat, dim1=1, dim2=2)  # [B, 216]\n",
    "#     #     loss_denom = loss_mat.sum(dim=2)  # [B, 216]\n",
    "        \n",
    "#     #     loss_InfoNCE = -(loss_diag / loss_denom).log().mean()\n",
    "        \n",
    "#     #     return loss_InfoNCE\n",
    "#     # @torch.no_grad()\n",
    "#     # def forward(self, inputs, semantic_feat):\n",
    "#     #     # Get features from biome classifier (B, C, 6, 6, 6)\n",
    "#     #     real_features = self.biome_classifier.get_intermediate_features(inputs)\n",
    "        \n",
    "#     #     # Print shapes for debugging\n",
    "#     #     print(\"Before reshape:\")\n",
    "#     #     print(\"real_features:\", real_features.shape)\n",
    "#     #     print(\"semantic_feat:\", semantic_feat.shape)\n",
    "        \n",
    "#     #     # Reshape to (B, N, C) where N = 6*6*6\n",
    "#     #     B, C, H, W, D = real_features.shape\n",
    "#     #     real_features = real_features.view(B, C, -1).permute(0, 2, 1)  # B, 216, C\n",
    "        \n",
    "#     #     print(\"After reshape:\")\n",
    "#     #     print(\"real_features:\", real_features.shape)\n",
    "#     #     print(\"semantic_feat:\", semantic_feat.shape)\n",
    "        \n",
    "#     #     # Normalize both feature sets\n",
    "#     #     real_features = F.normalize(real_features, p=2, dim=-1)\n",
    "#     #     semantic_feat = F.normalize(semantic_feat, p=2, dim=-1)\n",
    "        \n",
    "#     #     # Matrix multiplication should be:\n",
    "#     #     # (B, 216, C) @ (B, 216, C).transpose(-2, -1) -> (B, 216, 216)\n",
    "#     #     loss_mat = (semantic_feat @ real_features.detach().mT)  # Use mT and proper dimensions\n",
    "#     #     loss_diag = loss_mat.diag()\n",
    "#     #     loss_denom = loss_mat.sum(1)\n",
    "#     #     loss_InfoNCE = -(loss_diag / loss_denom).log().mean()\n",
    "        \n",
    "#     #     return loss_InfoNCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual Codebook Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VQLossDualCodebook(nn.Module):\n",
    "#     def __init__(self, H):\n",
    "#         super().__init__()\n",
    "#         # Discriminator parameters\n",
    "#         if H.disc_type == 'conv':\n",
    "#             # Use simpler 3D discriminator\n",
    "#             self.discriminator = Discriminator3D(\n",
    "#                 input_nc=H.n_channels,  # number of block types\n",
    "#                 ndf=H.ndf  # base number of filters\n",
    "#             ).cuda()\n",
    "#         elif H.disc_type == 'patch':\n",
    "#             self.discriminator = PatchGAN3DDiscriminator(\n",
    "#                 input_nc=H.n_channels,\n",
    "#                 ndf=H.ndf,\n",
    "#                 n_layers=H.disc_layers\n",
    "#             ).cuda()\n",
    "        \n",
    "#         # Initialize BiomeClassifier for feature extraction\n",
    "#         self.biome_feature_model = BiomeFeatureModel(H.biome_classifier_path)\n",
    "        \n",
    "#         # Loss weights and parameters\n",
    "#         self.disc_start_step = H.disc_start_step\n",
    "#         self.disc_weight_max = H.disc_weight_max\n",
    "#         self.disc_weight_min = 0.0\n",
    "#         self.disc_adaptive_weight = H.disc_adaptive_weight\n",
    "#         self.reconstruction_weight = H.reconstruction_weight\n",
    "#         self.codebook_weight = H.codebook_weight\n",
    "#         self.biome_weight = H.biome_weight\n",
    "#         self.disentanglement_ratio = H.disentanglement_ratio\n",
    "        \n",
    "#         # Loss functions\n",
    "#         self.disc_loss = non_saturating_d_loss\n",
    "#         self.gen_loss = hinge_gen_loss\n",
    "\n",
    "#     def adopt_weight(self, weight, global_step, threshold=0, value=0.):\n",
    "#         \"\"\"Gradually adopt weight after threshold step\"\"\"\n",
    "#         if global_step < threshold:\n",
    "#             weight = value\n",
    "#         return weight\n",
    "    \n",
    "#     def calculate_adaptive_weight(self, nll_loss, g_loss, last_layer=None):\n",
    "#         \"\"\"Dynamically adjust discriminator weight to balance with other losses\"\"\"\n",
    "#         if last_layer is not None:\n",
    "#             nll_grads = torch.autograd.grad(nll_loss, last_layer, retain_graph=True)[0]\n",
    "#             g_grads = torch.autograd.grad(g_loss, last_layer, retain_graph=True)[0]\n",
    "            \n",
    "#             d_weight = torch.norm(nll_grads) / (torch.norm(g_grads) + 1e-4)\n",
    "#             d_weight = torch.clamp(d_weight, self.disc_weight_min, self.disc_weight_max).detach()\n",
    "#             return d_weight\n",
    "#         return 1.0\n",
    "\n",
    "    \n",
    "#     def compute_biome_feature_loss(self, inputs, reconstructions):\n",
    "#         \"\"\"Compute feature matching loss using BiomeClassifier's intermediate features\"\"\"\n",
    "#         with torch.no_grad():\n",
    "#             real_features = self.biome_classifier.get_intermediate_features(inputs)\n",
    "#         fake_features = self.biome_classifier.get_intermediate_features(reconstructions)\n",
    "        \n",
    "#         # Normalize features\n",
    "#         real_features = F.normalize(real_features, p=2, dim=1)\n",
    "#         fake_features = F.normalize(fake_features, p=2, dim=1)\n",
    "        \n",
    "#         # Compute feature matching loss\n",
    "#         feature_loss = F.mse_loss(fake_features, real_features)\n",
    "        \n",
    "#         return feature_loss\n",
    "\n",
    "#     def forward(self, codebook_loss_style, codebook_loss_struct,\n",
    "#                 inputs, reconstructions, disentangle_loss, biome_feat,\n",
    "#                 optimizer_idx, global_step, last_layer=None):\n",
    "        \n",
    "#         if optimizer_idx == 0:\n",
    "#             rec_loss = F.cross_entropy(\n",
    "#                 reconstructions.contiguous(), \n",
    "#                 torch.argmax(inputs, dim=1).contiguous()\n",
    "#             ) * self.reconstruction_weight\n",
    "                        \n",
    "#             # Codebook losses\n",
    "#             style_loss = sum(codebook_loss_style[:3]) * self.codebook_weight\n",
    "#             struct_loss = sum(codebook_loss_struct[:3]) * self.codebook_weight\n",
    "            \n",
    "#             # Biome feature loss using InfoNCE\n",
    "#             if biome_feat is not None:\n",
    "#                 biome_feat_loss = self.biome_feature_model(inputs.contiguous(), biome_feat)\n",
    "#                 biome_feat_loss = self.biome_weight * biome_feat_loss\n",
    "#             else:\n",
    "#                 biome_feat_loss = 0.0\n",
    "\n",
    "#             # Check if biome loss is being scaled properly\n",
    "#             # print(\"Raw biome loss:\", biome_feat_loss.item())\n",
    "#             # print(\"Scaled biome loss:\", (self.biome_weight * biome_feat_loss).item())\n",
    "            \n",
    "#             # Disentanglement loss\n",
    "#             disent_loss = self.disentanglement_ratio * disentangle_loss if disentangle_loss is not None else 0.0\n",
    "            \n",
    "#             # Generator adversarial loss with adaptive weight\n",
    "#             disc_weight = self.adopt_weight(self.disc_weight_max, global_step, \n",
    "#                                           threshold=self.disc_start_step, value=0.0)\n",
    "            \n",
    "#             logits_fake = self.discriminator(reconstructions.contiguous())\n",
    "#             g_loss = self.gen_loss(logits_fake)\n",
    "            \n",
    "#             if self.disc_adaptive_weight:\n",
    "#                 null_loss = rec_loss + biome_feat_loss\n",
    "#                 disc_adaptive_weight = self.calculate_adaptive_weight(null_loss, g_loss, last_layer)\n",
    "#                 g_loss = g_loss * disc_weight * disc_adaptive_weight\n",
    "#             else:\n",
    "#                 g_loss = g_loss * disc_weight\n",
    "            \n",
    "#             # Total loss\n",
    "#             loss = rec_loss + style_loss + struct_loss + biome_feat_loss + disent_loss + g_loss\n",
    "            \n",
    "#             return {\n",
    "#                 'loss': loss,\n",
    "#                 'rec_loss': rec_loss,\n",
    "#                 'style_loss': style_loss,\n",
    "#                 'struct_loss': struct_loss,\n",
    "#                 'biome_feat_loss': biome_feat_loss,\n",
    "#                 'disent_loss': disent_loss,\n",
    "#                 'g_loss': g_loss,\n",
    "#                 'disc_weight': disc_weight,\n",
    "#                 'disc_adaptive_weight': disc_adaptive_weight if self.disc_adaptive_weight else 1.0,\n",
    "#                 'codebook_usage_style': codebook_loss_style[3],\n",
    "#                 'codebook_usage_struct': codebook_loss_struct[3]\n",
    "#             }\n",
    "            \n",
    "#         # Discriminator update\n",
    "#         elif optimizer_idx == 1:\n",
    "#             # Get discriminator predictions\n",
    "#             logits_real = self.discriminator(inputs.contiguous().detach())\n",
    "#             logits_fake = self.discriminator(reconstructions.contiguous().detach())\n",
    "\n",
    "#             # Calculate discriminator loss with weight adoption\n",
    "#             disc_weight = self.adopt_weight(self.disc_weight_max, global_step, \n",
    "#                                           threshold=self.disc_start_step, value=0.0)\n",
    "#             d_loss = self.disc_loss(logits_real, logits_fake) * disc_weight\n",
    "            \n",
    "#             return {\n",
    "#                 'd_loss': d_loss,\n",
    "#                 'logits_real': logits_real.mean(),\n",
    "#                 'logits_fake': logits_fake.mean(),\n",
    "#                 'disc_weight': disc_weight\n",
    "#             }\n",
    "        \n",
    "\n",
    "# def hinge_d_loss(logits_real, logits_fake):\n",
    "#     loss_real = torch.mean(F.relu(1. - logits_real))\n",
    "#     loss_fake = torch.mean(F.relu(1. + logits_fake))\n",
    "\n",
    "#     d_loss = 0.5 * (loss_real + loss_fake)\n",
    "#     return d_loss\n",
    "\n",
    "\n",
    "# def vanilla_d_loss(logits_real, logits_fake):\n",
    "#     loss_real = torch.mean(F.softplus(-logits_real))\n",
    "#     loss_fake = torch.mean(F.softplus(logits_fake))\n",
    "#     d_loss = 0.5 * (loss_real + loss_fake)\n",
    "#     return d_loss\n",
    "\n",
    "\n",
    "# def non_saturating_d_loss(logits_real, logits_fake):\n",
    "#     loss_real = torch.mean(F.binary_cross_entropy_with_logits(torch.ones_like(logits_real),  logits_real))\n",
    "#     loss_fake = torch.mean(F.binary_cross_entropy_with_logits(torch.zeros_like(logits_fake), logits_fake))\n",
    "#     d_loss = 0.5 * (loss_real + loss_fake)\n",
    "#     return d_loss\n",
    "\n",
    "# def hinge_gen_loss(logits_fake):\n",
    "#     return -torch.mean(logits_fake)\n",
    "\n",
    "# def vanilla_d_loss(logits_real, logits_fake):\n",
    "#     loss_real = torch.mean(F.softplus(-logits_real))\n",
    "#     loss_fake = torch.mean(F.softplus(logits_fake))\n",
    "#     d_loss = 0.5 * (loss_real + loss_fake)\n",
    "#     return d_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FQVAE class - composed of a encoder, quantizer, and decoder (aka generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def _expand_token(token, batch_size: int):\n",
    "#     return token.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "# class FeatPredHead(nn.Module):\n",
    "#     def __init__(self, input_dim=256, down_factor=16):\n",
    "#         super().__init__()\n",
    "#         self.grid_size = 24 // down_factor\n",
    "#         self.width = 256\n",
    "#         self.num_layers = 3\n",
    "#         self.num_heads = 8\n",
    "\n",
    "#         self.upscale = nn.Sequential(\n",
    "#             nn.Linear(input_dim, self.width),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(self.width, self.width)\n",
    "#         )\n",
    "\n",
    "#         scale = self.width ** -0.5\n",
    "#         # Remove class embedding\n",
    "#         self.positional_embedding = nn.Parameter(scale * torch.randn(self.grid_size ** 3, self.width))\n",
    "#         self.ln_pre = nn.LayerNorm(self.width)\n",
    "#         self.transformer = nn.ModuleList([\n",
    "#             ResidualAttentionBlock(self.width, self.num_heads, mlp_ratio=4.0)\n",
    "#             for _ in range(self.num_layers)\n",
    "#         ])\n",
    "#         self.ln_post = nn.LayerNorm(self.width)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = rearrange(x, 'b c h w d -> b (h w d) c')\n",
    "#         x = self.upscale(x)\n",
    "\n",
    "#         # No class token addition\n",
    "#         x = x + self.positional_embedding.to(x.dtype)\n",
    "#         x = self.ln_pre(x)\n",
    "#         x = x.permute(1, 0, 2)\n",
    "#         for layer in self.transformer:\n",
    "#             x = layer(x)\n",
    "#         x = x.permute(1, 0, 2)\n",
    "#         x = self.ln_post(x)\n",
    "#         return x  # Shape will be [B, 216, C]\n",
    "    \n",
    "# class FQModel(nn.Module):\n",
    "#     def __init__(self, H):\n",
    "#         super().__init__()\n",
    "#         # Basic parameters\n",
    "#         self.in_channels = H.n_channels\n",
    "#         self.nf = H.nf\n",
    "#         self.n_blocks = H.res_blocks\n",
    "#         self.codebook_size = H.codebook_size\n",
    "#         self.embed_dim = H.emb_dim\n",
    "#         self.ch_mult = H.ch_mult\n",
    "#         self.num_resolutions = len(self.ch_mult)\n",
    "#         self.resolution = H.img_size\n",
    "#         self.z_channels = H.z_channels\n",
    "#         self.with_biome_supervision = H.with_biome_supervision\n",
    "#         self.with_disentanglement = H.with_disentanglement\n",
    "#         self.disentanglement_ratio = H.disentanglement_ratio\n",
    "        \n",
    "#         # Two head encoder\n",
    "#         self.encoder = Encoder(\n",
    "#             self.in_channels,\n",
    "#             self.nf,\n",
    "#             self.embed_dim,\n",
    "#             self.ch_mult,\n",
    "#             self.n_blocks,\n",
    "#             self.resolution\n",
    "#         )\n",
    "\n",
    "#         # Quantizer for style head (semantic)\n",
    "#         self.quantize_style = FQVectorQuantizer(\n",
    "#             # self.codebook_size, \n",
    "#             20, \n",
    "#             self.embed_dim,\n",
    "#             H.beta, \n",
    "#             H.entropy_loss_ratio,\n",
    "#             H.codebook_l2_norm, \n",
    "#             H.codebook_show_usage\n",
    "#         )\n",
    "#         self.quant_conv_style = nn.Conv3d(self.z_channels, self.embed_dim, 1)\n",
    "\n",
    "#         # Quantizer for structural head (visual)\n",
    "#         self.quantize_struct = FQVectorQuantizer(\n",
    "#             self.codebook_size, \n",
    "#             self.embed_dim,\n",
    "#             H.beta, \n",
    "#             H.entropy_loss_ratio,\n",
    "#             H.codebook_l2_norm, \n",
    "#             H.codebook_show_usage\n",
    "#         )\n",
    "#         self.quant_conv_struct = nn.Conv3d(self.z_channels, self.embed_dim, 1)\n",
    "\n",
    "#         # Pixel decoder\n",
    "#         input_dim = self.embed_dim * 2  # Combined dimension from both codebooks\n",
    "#         self.post_quant_conv = nn.Conv3d(input_dim, self.z_channels, 1)\n",
    "#         self.decoder = Generator(H)\n",
    "\n",
    "#         # Determine downsampling factor\n",
    "#         if self.num_resolutions == 5:\n",
    "#             down_factor = 16\n",
    "#         elif self.num_resolutions == 4:\n",
    "#             down_factor = 8\n",
    "#         elif self.num_resolutions == 3:\n",
    "#             down_factor = 4\n",
    "#         else:\n",
    "#             raise NotImplementedError\n",
    "\n",
    "#         # Biome prediction head for style representation learning\n",
    "#         if H.with_biome_supervision:\n",
    "#             print(\"Include feature prediction head for biome supervision\")\n",
    "#             self.feat_pred_head = FeatPredHead(input_dim=self.embed_dim, down_factor=down_factor)\n",
    "#         else:\n",
    "#             print(\"NO biome supervision\")\n",
    "\n",
    "#         if H.with_disentanglement:\n",
    "#             print(\"Disentangle Ratio: \", H.disentanglement_ratio)\n",
    "#         else:\n",
    "#             print(\"No Disentangle Regularization\")\n",
    "\n",
    "#     def compute_disentangle_loss(self, quant_struct, quant_style):\n",
    "#         # Reshape from 5D to 2D\n",
    "#         quant_struct = rearrange(quant_struct, 'b c h w d -> (b h w d) c')\n",
    "#         quant_style = rearrange(quant_style, 'b c h w d -> (b h w d) c')\n",
    "\n",
    "#         # Normalize the vectors\n",
    "#         quant_struct = F.normalize(quant_struct, p=2, dim=-1)\n",
    "#         quant_style = F.normalize(quant_style, p=2, dim=-1)\n",
    "\n",
    "#         # Compute dot product and loss\n",
    "#         dot_product = torch.sum(quant_struct * quant_style, dim=1)\n",
    "#         loss = torch.mean(dot_product ** 2) * self.disentanglement_ratio\n",
    "\n",
    "#         return loss\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         # Get both style and structure encodings\n",
    "#         h_style, h_struct = self.encoder(input)\n",
    "#         h_style = self.quant_conv_style(h_style)\n",
    "#         h_struct = self.quant_conv_struct(h_struct)\n",
    "\n",
    "#         # Quantize both paths\n",
    "#         quant_style, emb_loss_style, _ = self.quantize_style(h_style)\n",
    "#         # print(\"quant_style requires grad:\", quant_style.requires_grad)\n",
    "\n",
    "#         quant_struct, emb_loss_struct, _ = self.quantize_struct(h_struct)\n",
    "        \n",
    "\n",
    "#         # Biome feature prediction if enabled\n",
    "#         if self.with_biome_supervision:\n",
    "#             style_feat = self.feat_pred_head(quant_style)\n",
    "#             # print(\"style_feat requires grad:\", style_feat.requires_grad)\n",
    "#             style_feat.retain_grad()  # Add this line\n",
    "\n",
    "#         else:\n",
    "#             style_feat = None\n",
    "\n",
    "#         # Compute disentanglement loss if enabled\n",
    "#         if self.with_disentanglement:\n",
    "#             disentangle_loss = self.compute_disentangle_loss(quant_struct, quant_style)\n",
    "#         else:\n",
    "#             disentangle_loss = 0\n",
    "\n",
    "#         # Combine quantized representations and decode\n",
    "#         quant = torch.cat([quant_struct, quant_style], dim=1)\n",
    "#         dec = self.decoder(quant)\n",
    "\n",
    "#         return dec, emb_loss_style, emb_loss_struct, disentangle_loss, style_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HparamsBase(dict):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        try:\n",
    "            return self[attr]\n",
    "        except KeyError:\n",
    "            return None\n",
    "\n",
    "    def __setattr__(self, attr, value):\n",
    "        self[attr] = value\n",
    "\n",
    "class HparamsFQGAN(HparamsBase):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__(dataset)\n",
    "        \n",
    "        if self.dataset == 'minecraft':\n",
    "            # Existing parameters\n",
    "            self.batch_size = 8\n",
    "            self.img_size = 24\n",
    "            self.n_channels = 43\n",
    "            self.nf = 64\n",
    "            self.ndf = 64\n",
    "            self.res_blocks = 2\n",
    "            self.latent_shape = [1, 6, 6, 6]\n",
    "            \n",
    "            # New parameters for dual codebook architecture\n",
    "            self.codebook_size = 32  # Size of each codebook\n",
    "            self.emb_dim = 32  # Embedding dimension\n",
    "            self.z_channels = 32  # Bottleneck channels\n",
    "            self.ch_mult = [1, 2, 4]  # Channel multipliers for progressive downsampling\n",
    "            self.num_resolutions = len(self.ch_mult)\n",
    "            self.attn_resolutions = [6]  # Resolutions at which to apply attention\n",
    "            \n",
    "            # Loss weights and parameters\n",
    "            self.disc_type = 'conv'\n",
    "            self.disc_weight_max = 0.5  # Weight for discriminator loss\n",
    "            self.disc_weight_min = 0.0  # Weight for discriminator loss\n",
    "            self.disc_adaptive_weight = True  # Enable adaptive weighting\n",
    "            self.disc_start_step = 2500  # Step to start discriminator training\n",
    "            self.reconstruction_weight = 1.0  # Weight for reconstruction loss\n",
    "            self.codebook_weight = 1.0  # Weight for codebook loss\n",
    "            self.biome_weight = 1.0  # Weight for biome feature prediction\n",
    "            self.disentanglement_ratio = 0.5  # Weight for disentanglement loss\n",
    "            \n",
    "            # Codebook specific parameters\n",
    "            self.beta = 0.5  # Commitment loss coefficient\n",
    "            self.entropy_loss_ratio = 0.05  # For codebook entropy regularization\n",
    "            self.codebook_l2_norm = True  # Whether to L2 normalize codebook entries\n",
    "            self.codebook_show_usage = True  # Track codebook usage statistics\n",
    "            \n",
    "            # Training parameters\n",
    "            self.lr = 1e-4  # Learning rate\n",
    "            self.beta1 = 0.9  # Adam beta1\n",
    "            self.beta2 = 0.95  # Adam beta2\n",
    "            self.disc_layers = 3  # Number of discriminator layers\n",
    "            self.train_steps = 10000\n",
    "            self.start_step = 0\n",
    "            \n",
    "            self.transformer_dim = self.emb_dim  # Make transformer dim match embedding dim\n",
    "            self.num_heads = 8  # Number of attention heads\n",
    "            \n",
    "            # Feature prediction parameters\n",
    "            self.with_biome_supervision = True  # Enable biome feature prediction\n",
    "            self.with_disentanglement = True  # Enable disentanglement loss\n",
    "            \n",
    "            # Logging parameters (if not already present)\n",
    "            self.steps_per_log = 150\n",
    "            self.steps_per_checkpoint = 1000\n",
    "            self.steps_per_display_output = 500\n",
    "            self.steps_per_save_output = 500\n",
    "            self.steps_per_validation = 150\n",
    "            self.val_samples_to_save = 16\n",
    "            self.val_samples_to_display = 4\n",
    "            self.visdom_port = 8097\n",
    "            \n",
    "\n",
    "\n",
    "            self.num_biomes = 11  # Number of biome classes\n",
    "            self.biome_feat_dim = 256  # Dimension of biome features\n",
    "            self.biome_classifier_path = 'best_biome_classifier.pt'\n",
    "        else:\n",
    "            raise KeyError(f'Defaults not defined for dataset: {self.dataset}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_and_save(model, plotted_loss_terms, vq_loss, val_loader, block_converter, visualizer, vis, step, H):\n",
    "    \"\"\"Run validation and save results\"\"\"\n",
    "    model.eval()\n",
    "    val_losses = {term: [] for term in plotted_loss_terms}\n",
    "    val_reconstructions = []\n",
    "    val_originals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            if isinstance(batch, list):\n",
    "                x = batch[0]\n",
    "            else:\n",
    "                x = batch\n",
    "            x = x.cuda()\n",
    "            \n",
    "            # Forward pass\n",
    "            recons, codebook_loss_style, codebook_loss_struct, disentangle_loss, biome_feat = model(x)\n",
    "            \n",
    "            # Calculate losses\n",
    "            loss_dict = vq_loss(\n",
    "                codebook_loss_style, codebook_loss_struct,\n",
    "                x, recons, disentangle_loss, biome_feat,\n",
    "                optimizer_idx=0,\n",
    "                global_step=step\n",
    "            )\n",
    "            \n",
    "            # Store losses\n",
    "            for term in plotted_loss_terms:\n",
    "                if term in loss_dict:\n",
    "                    val_losses[term].append(loss_dict[term].item() if torch.is_tensor(loss_dict[term]) else loss_dict[term])\n",
    "            \n",
    "            # Store reconstructions and originals\n",
    "            val_reconstructions.append(recons.cpu())\n",
    "            val_originals.append(x.cpu())\n",
    "            \n",
    "            # Break only after we have enough samples\n",
    "            total_samples = sum(r.size(0) for r in val_reconstructions)\n",
    "            if total_samples >= H.val_samples_to_save:\n",
    "                break\n",
    "    \n",
    "    # Concatenate all samples\n",
    "    val_reconstructions = torch.cat(val_reconstructions, dim=0)\n",
    "    val_originals = torch.cat(val_originals, dim=0)\n",
    "    \n",
    "    # Trim to exact number of samples needed\n",
    "    val_reconstructions = val_reconstructions[:H.val_samples_to_save]\n",
    "    val_originals = val_originals[:H.val_samples_to_save]\n",
    "    \n",
    "    val_data = {\n",
    "        'reconstructions': val_reconstructions,\n",
    "        'originals': val_originals,\n",
    "        'losses': {k: np.mean(v) if v else 0 for k, v in val_losses.items()},\n",
    "        'step': step\n",
    "    }\n",
    "    \n",
    "    # Display samples\n",
    "    display_samples = min(H.val_samples_to_display, val_reconstructions.size(0))\n",
    "    orig_blocks = block_converter.convert_to_original_blocks(val_data['originals'][:display_samples])\n",
    "    recon_blocks = block_converter.convert_to_original_blocks(val_data['reconstructions'][:display_samples])\n",
    "    \n",
    "    log_dir = f\"../model_logs/{H.log_dir}/val_images\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # Display and save validation visualizations\n",
    "    display_minecraft_pyvista(\n",
    "        vis, visualizer, orig_blocks,\n",
    "        win_name='Val Original Maps',\n",
    "        title=f'Val Original Maps step {step}',\n",
    "        save_path=f\"{log_dir}/val_orig_{step}.png\"\n",
    "    )\n",
    "    display_minecraft_pyvista(\n",
    "        vis, visualizer, recon_blocks,\n",
    "        win_name='Val Reconstructed Maps',\n",
    "        title=f'Val Reconstructed Maps step {step}',\n",
    "        save_path=f\"{log_dir}/val_recon_{step}.png\"\n",
    "    )\n",
    "    \n",
    "    # print(f\"Batch size: {H.batch_size}\")\n",
    "    # print(f\"Val reconstructions shape: {val_reconstructions.shape}\")\n",
    "    # print(f\"Val originals shape: {val_originals.shape}\")\n",
    "\n",
    "    # Plot validation losses in Visdom\n",
    "    for term in plotted_loss_terms:\n",
    "        if term in val_losses and val_losses[term]:  # Only plot if we have values\n",
    "            vis.line(\n",
    "                Y=np.array([np.mean(val_losses[term])]),  # Take mean of batch losses\n",
    "                X=np.array([step]),\n",
    "                win=f'val_{term}_plot',\n",
    "                update='append',\n",
    "                opts=dict(title=f'Validation {term} Loss')\n",
    "            )\n",
    "    \n",
    "    model.train()\n",
    "    return val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file for running the training of the VQGAN\n",
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "from log_utils import log, log_stats, save_model, save_stats, save_images, save_maps, \\\n",
    "                            display_images, set_up_visdom, config_log, start_training_log, log_hparams_to_json\n",
    "import visdom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Setting up training for VQGAN on minecraft\n",
      "Using following hparams:\n",
      "> attn_resolutions: [6]\n",
      "> batch_size: 8\n",
      "> beta: 0.5\n",
      "> beta1: 0.9\n",
      "> beta2: 0.95\n",
      "> biome_classifier_path: best_biome_classifier.pt\n",
      "> biome_feat_dim: 256\n",
      "> biome_weight: 1.0\n",
      "> ch_mult: [1, 2, 4]\n",
      "> codebook_l2_norm: True\n",
      "> codebook_show_usage: True\n",
      "> codebook_size: 32\n",
      "> codebook_weight: 1.0\n",
      "> dataset: minecraft\n",
      "> disc_adaptive_weight: True\n",
      "> disc_layers: 3\n",
      "> disc_start_step: 2500\n",
      "> disc_type: conv\n",
      "> disc_weight_max: 0.5\n",
      "> disc_weight_min: 0.0\n",
      "> disentanglement_ratio: 0.5\n",
      "> emb_dim: 32\n",
      "> entropy_loss_ratio: 0.05\n",
      "> img_size: 24\n",
      "> latent_shape: [1, 6, 6, 6]\n",
      "> load_dir: FQGAN12stylecodebook_32struct_addnotconcat3\n",
      "> log_dir: FQGAN12stylecodebook_32struct_addnotconcat3\n",
      "> lr: 0.0001\n",
      "> n_channels: 43\n",
      "> ndf: 64\n",
      "> nf: 64\n",
      "> num_biomes: 11\n",
      "> num_heads: 8\n",
      "> num_resolutions: 3\n",
      "> reconstruction_weight: 1.0\n",
      "> res_blocks: 2\n",
      "> start_step: 0\n",
      "> steps_per_checkpoint: 1000\n",
      "> steps_per_display_output: 500\n",
      "> steps_per_log: 150\n",
      "> steps_per_save_output: 500\n",
      "> steps_per_validation: 150\n",
      "> train_steps: 10000\n",
      "> transformer_dim: 32\n",
      "> val_samples_to_display: 4\n",
      "> val_samples_to_save: 16\n",
      "> visdom_port: 8097\n",
      "> with_biome_supervision: True\n",
      "> with_disentanglement: True\n",
      "> z_channels: 32\n",
      "Hyperparameters logged to ../model_logs/FQGAN12stylecodebook_32struct_addnotconcat3\\hparams.json\n"
     ]
    }
   ],
   "source": [
    "H = HparamsFQGAN(dataset='minecraft')\n",
    "H.log_dir = 'FQGAN12stylecodebook_32struct_addnotconcat3'\n",
    "H.load_dir = 'FQGAN12stylecodebook_32struct_addnotconcat3'\n",
    "\n",
    "vis = visdom.Visdom(port=H.visdom_port)\n",
    "config_log(H.log_dir)\n",
    "log('---------------------------------')\n",
    "log(f'Setting up training for VQGAN on {H.dataset}')\n",
    "\n",
    "start_training_log(H)\n",
    "log_hparams_to_json(H, H.log_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.4.3'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'vtk': 'https://cdn.jsdelivr.net/npm/vtk.js@30.1.0/vtk'}, 'shim': {'vtk': {'exports': 'vtk'}}});\n      require([\"vtk\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 1;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window.vtk !== undefined) && (!(window.vtk instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.4.5/dist/bundled/abstractvtkplot/vtk.js@30.1.0/vtk.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.holoviz.org/panel/1.4.5/dist/bundled/abstractvtkplot/vtk.js@30.1.0/vtk.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.3.min.js\", \"https://cdn.holoviz.org/panel/1.4.5/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n\ttry {\n          inline_js[i].call(root, root.Bokeh);\n\t} catch(e) {\n\t  if (!reloading) {\n\t    throw e;\n\t  }\n\t}\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        })\n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='04eb3140-e378-4fd0-bc7e-664e9f1c9b0f'>\n",
       "  <div id=\"f3e67b97-bfc0-4e34-9452-43e59aded97b\" data-root-id=\"04eb3140-e378-4fd0-bc7e-664e9f1c9b0f\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"04e44acc-9245-49a8-8dc9-56cbc35a6dc6\":{\"version\":\"3.4.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"04eb3140-e378-4fd0-bc7e-664e9f1c9b0f\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"11a014ef-27a2-4bb4-9007-f3c4ed8f82db\",\"attributes\":{\"plot_id\":\"04eb3140-e378-4fd0-bc7e-664e9f1c9b0f\",\"comm_id\":\"a94c32e9a21544b0ae411fc43053391a\",\"client_comm_id\":\"828e9d94c6974d009dbcd79eafb4b461\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":null}]}]}};\n",
       "  var render_items = [{\"docid\":\"04e44acc-9245-49a8-8dc9-56cbc35a6dc6\",\"roots\":{\"04eb3140-e378-4fd0-bc7e-664e9f1c9b0f\":\"f3e67b97-bfc0-4e34-9452-43e59aded97b\"},\"root_ids\":[\"04eb3140-e378-4fd0-bc7e-664e9f1c9b0f\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined && ( root.vtk !== undefined) && ( root.vtk !== undefined))\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "04eb3140-e378-4fd0-bc7e-664e9f1c9b0f"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed data...\n",
      "Loaded 11082 chunks of size torch.Size([43, 24, 24, 24])\n",
      "Number of unique block types: 43\n",
      "Unique blocks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]\n",
      "saving validation dataset to file: ../../text2env/data/FQGAN12stylecodebook_32struct_addnotconcat3_valset.pt\n",
      "Saved validation data to ../../text2env/data/FQGAN12stylecodebook_32struct_addnotconcat3_valset.pt\n",
      "\n",
      "Dataloader details:\n",
      "Training samples: 9974\n",
      "Validation samples: 1108\n",
      "Batch size: 8\n",
      "Training batches: 1247\n",
      "Validation batches: 139\n"
     ]
    }
   ],
   "source": [
    "data_path = '../../text2env/data/minecraft_biome_newworld_10k_processed_cleaned.pt'\n",
    "mappings_path = '../../text2env/data/minecraft_biome_newworld_10k_mappings.pt'\n",
    "# visualizer = MinecraftVisualizer()\n",
    "visualizer = MinecraftVisualizerPyVista()\n",
    "train_loader, val_loader = get_minecraft_dataloaders(\n",
    "    data_path,\n",
    "    batch_size=H.batch_size,\n",
    "    num_workers=0,\n",
    "    val_split=0.1,\n",
    "    save_val_path=f'../../text2env/data/{H.log_dir}_valset.pt'\n",
    ")\n",
    "block_converter = BlockBiomeConverter.load_mappings(mappings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fq_models import FQModel, VQLossDualCodebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Include feature prediction head for biome supervision\n",
      "Disentangle Ratio:  0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TimBits\\miniconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0:\n",
      "rec_loss: 3.8228\n",
      "style_loss: 0.0242\n",
      "struct_loss: -0.0565\n",
      "biome_feat_loss: 1.0002\n",
      "disent_loss: 0.0099\n",
      "g_loss: -0.0000\n",
      "d_loss: 0.0000\n",
      "Codebook Usage - Style: 0.92%, Structure: 0.81%\n",
      "Rendering...\n",
      "Done Rendering\n",
      "\n",
      "Step 150:\n",
      "rec_loss: 0.3516\n",
      "style_loss: -0.0763\n",
      "struct_loss: -0.1200\n",
      "biome_feat_loss: 0.2883\n",
      "disent_loss: 0.0058\n",
      "g_loss: -0.0000\n",
      "d_loss: 0.0000\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 300:\n",
      "rec_loss: 0.1674\n",
      "style_loss: -0.0782\n",
      "struct_loss: -0.1248\n",
      "biome_feat_loss: 0.1646\n",
      "disent_loss: 0.0064\n",
      "g_loss: -0.0000\n",
      "d_loss: 0.0000\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 450:\n",
      "rec_loss: 0.2684\n",
      "style_loss: -0.0807\n",
      "struct_loss: -0.1249\n",
      "biome_feat_loss: 0.1817\n",
      "disent_loss: 0.0056\n",
      "g_loss: -0.0000\n",
      "d_loss: 0.0000\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "Rendering...\n",
      "Done Rendering\n",
      "\n",
      "Step 600:\n",
      "rec_loss: 0.1914\n",
      "style_loss: -0.0818\n",
      "struct_loss: -0.1300\n",
      "biome_feat_loss: 0.1385\n",
      "disent_loss: 0.0043\n",
      "g_loss: -0.0000\n",
      "d_loss: 0.0000\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 750:\n",
      "rec_loss: 0.1517\n",
      "style_loss: -0.0843\n",
      "struct_loss: -0.1296\n",
      "biome_feat_loss: 0.1140\n",
      "disent_loss: 0.0044\n",
      "g_loss: -0.0000\n",
      "d_loss: 0.0000\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 900:\n",
      "rec_loss: 0.1318\n",
      "style_loss: -0.0840\n",
      "struct_loss: -0.1297\n",
      "biome_feat_loss: 0.0750\n",
      "disent_loss: 0.0046\n",
      "g_loss: -0.0000\n",
      "d_loss: 0.0000\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "Rendering...\n",
      "Done Rendering\n",
      "Saving fqgan to fqgan_1000.th\n",
      "Saving optimizer_g to optimizer_g_1000.th\n",
      "Saving optimizer_d to optimizer_d_1000.th\n",
      "Saving stats to ../model_logs/FQGAN12stylecodebook_32struct_addnotconcat3/saved_stats/stats_1000\n",
      "\n",
      "Step 1050:\n",
      "rec_loss: 0.1717\n",
      "style_loss: -0.0785\n",
      "struct_loss: -0.1322\n",
      "biome_feat_loss: 0.0791\n",
      "disent_loss: 0.0047\n",
      "g_loss: -0.0000\n",
      "d_loss: 0.0000\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 1200:\n",
      "rec_loss: 0.2032\n",
      "style_loss: -0.0826\n",
      "struct_loss: -0.1333\n",
      "biome_feat_loss: 0.0462\n",
      "disent_loss: 0.0040\n",
      "g_loss: 0.0000\n",
      "d_loss: 0.0000\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 1350:\n",
      "rec_loss: 0.1402\n",
      "style_loss: -0.0858\n",
      "struct_loss: -0.1324\n",
      "biome_feat_loss: 0.0505\n",
      "disent_loss: 0.0045\n",
      "g_loss: 0.0000\n",
      "d_loss: 0.0000\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 1500:\n",
      "rec_loss: 0.1430\n",
      "style_loss: -0.0807\n",
      "struct_loss: -0.1294\n",
      "biome_feat_loss: 0.0770\n",
      "disent_loss: 0.0054\n",
      "g_loss: 0.0000\n",
      "d_loss: 0.0000\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "Rendering...\n",
      "Done Rendering\n",
      "\n",
      "Step 1650:\n",
      "rec_loss: 0.1241\n",
      "style_loss: -0.0847\n",
      "struct_loss: -0.1338\n",
      "biome_feat_loss: 0.0390\n",
      "disent_loss: 0.0042\n",
      "g_loss: -0.0000\n",
      "d_loss: 0.0000\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 1800:\n",
      "rec_loss: 0.1696\n",
      "style_loss: -0.0806\n",
      "struct_loss: -0.1319\n",
      "biome_feat_loss: 0.0609\n",
      "disent_loss: 0.0047\n",
      "g_loss: -0.0000\n",
      "d_loss: 0.0000\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 1950:\n",
      "rec_loss: 0.1438\n",
      "style_loss: -0.0843\n",
      "struct_loss: -0.1343\n",
      "biome_feat_loss: 0.0414\n",
      "disent_loss: 0.0042\n",
      "g_loss: 0.0000\n",
      "d_loss: 0.0000\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "Rendering...\n",
      "Done Rendering\n",
      "Saving fqgan to fqgan_2000.th\n",
      "Saving optimizer_g to optimizer_g_2000.th\n",
      "Saving optimizer_d to optimizer_d_2000.th\n",
      "Saving stats to ../model_logs/FQGAN12stylecodebook_32struct_addnotconcat3/saved_stats/stats_2000\n",
      "\n",
      "Step 2100:\n",
      "rec_loss: 0.1243\n",
      "style_loss: -0.0831\n",
      "struct_loss: -0.1342\n",
      "biome_feat_loss: 0.0699\n",
      "disent_loss: 0.0043\n",
      "g_loss: 0.0000\n",
      "d_loss: 0.0000\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 2250:\n",
      "rec_loss: 0.0861\n",
      "style_loss: -0.0924\n",
      "struct_loss: -0.1409\n",
      "biome_feat_loss: 0.0369\n",
      "disent_loss: 0.0026\n",
      "g_loss: -0.0000\n",
      "d_loss: 0.0000\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 2400:\n",
      "rec_loss: 0.1226\n",
      "style_loss: -0.0830\n",
      "struct_loss: -0.1343\n",
      "biome_feat_loss: 0.0724\n",
      "disent_loss: 0.0036\n",
      "g_loss: -0.0000\n",
      "d_loss: 0.0000\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "Rendering...\n",
      "Done Rendering\n",
      "\n",
      "Step 2550:\n",
      "rec_loss: 0.1127\n",
      "style_loss: -0.0917\n",
      "struct_loss: -0.1402\n",
      "biome_feat_loss: 0.0439\n",
      "disent_loss: 0.0029\n",
      "g_loss: -5.2298\n",
      "d_loss: -2.1220\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 2700:\n",
      "rec_loss: 0.1095\n",
      "style_loss: -0.0840\n",
      "struct_loss: -0.1349\n",
      "biome_feat_loss: 0.0343\n",
      "disent_loss: 0.0040\n",
      "g_loss: -14.2780\n",
      "d_loss: -6.6406\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 2850:\n",
      "rec_loss: 0.1193\n",
      "style_loss: -0.0903\n",
      "struct_loss: -0.1403\n",
      "biome_feat_loss: 0.0317\n",
      "disent_loss: 0.0032\n",
      "g_loss: -23.7386\n",
      "d_loss: -11.3691\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 3000:\n",
      "rec_loss: 0.0977\n",
      "style_loss: -0.0931\n",
      "struct_loss: -0.1421\n",
      "biome_feat_loss: 0.0378\n",
      "disent_loss: 0.0032\n",
      "g_loss: -33.5804\n",
      "d_loss: -16.2898\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "Rendering...\n",
      "Done Rendering\n",
      "Saving fqgan to fqgan_3000.th\n",
      "Saving optimizer_g to optimizer_g_3000.th\n",
      "Saving optimizer_d to optimizer_d_3000.th\n",
      "Saving stats to ../model_logs/FQGAN12stylecodebook_32struct_addnotconcat3/saved_stats/stats_3000\n",
      "\n",
      "Step 3150:\n",
      "rec_loss: 0.0966\n",
      "style_loss: -0.0933\n",
      "struct_loss: -0.1399\n",
      "biome_feat_loss: 0.0394\n",
      "disent_loss: 0.0031\n",
      "g_loss: -43.8028\n",
      "d_loss: -21.4005\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 3300:\n",
      "rec_loss: 0.1013\n",
      "style_loss: -0.0838\n",
      "struct_loss: -0.1382\n",
      "biome_feat_loss: 0.0244\n",
      "disent_loss: 0.0031\n",
      "g_loss: -54.4007\n",
      "d_loss: -26.6995\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 3450:\n",
      "rec_loss: 0.1348\n",
      "style_loss: -0.0844\n",
      "struct_loss: -0.1351\n",
      "biome_feat_loss: 0.0527\n",
      "disent_loss: 0.0034\n",
      "g_loss: -65.3750\n",
      "d_loss: -32.1865\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "Rendering...\n",
      "Done Rendering\n",
      "\n",
      "Step 3600:\n",
      "rec_loss: 0.0920\n",
      "style_loss: -0.0897\n",
      "struct_loss: -0.1407\n",
      "biome_feat_loss: 0.0443\n",
      "disent_loss: 0.0028\n",
      "g_loss: -76.7249\n",
      "d_loss: -37.8613\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 3750:\n",
      "rec_loss: 0.0895\n",
      "style_loss: -0.0911\n",
      "struct_loss: -0.1405\n",
      "biome_feat_loss: 0.0307\n",
      "disent_loss: 0.0028\n",
      "g_loss: -88.4494\n",
      "d_loss: -43.7237\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 3900:\n",
      "rec_loss: 0.0758\n",
      "style_loss: -0.0931\n",
      "struct_loss: -0.1416\n",
      "biome_feat_loss: 0.0289\n",
      "disent_loss: 0.0027\n",
      "g_loss: -100.5491\n",
      "d_loss: -49.7734\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "Rendering...\n",
      "Done Rendering\n",
      "Saving fqgan to fqgan_4000.th\n",
      "Saving optimizer_g to optimizer_g_4000.th\n",
      "Saving optimizer_d to optimizer_d_4000.th\n",
      "Saving stats to ../model_logs/FQGAN12stylecodebook_32struct_addnotconcat3/saved_stats/stats_4000\n",
      "\n",
      "Step 4050:\n",
      "rec_loss: 0.0541\n",
      "style_loss: -0.0943\n",
      "struct_loss: -0.1466\n",
      "biome_feat_loss: 0.0215\n",
      "disent_loss: 0.0019\n",
      "g_loss: -113.0250\n",
      "d_loss: -56.0112\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 4200:\n",
      "rec_loss: 0.0816\n",
      "style_loss: -0.0957\n",
      "struct_loss: -0.1428\n",
      "biome_feat_loss: 0.0367\n",
      "disent_loss: 0.0025\n",
      "g_loss: -125.8749\n",
      "d_loss: -62.4366\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 4350:\n",
      "rec_loss: 0.0988\n",
      "style_loss: -0.0867\n",
      "struct_loss: -0.1380\n",
      "biome_feat_loss: 0.0262\n",
      "disent_loss: 0.0030\n",
      "g_loss: -139.0976\n",
      "d_loss: -69.0476\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 4500:\n",
      "rec_loss: 0.0953\n",
      "style_loss: -0.0881\n",
      "struct_loss: -0.1399\n",
      "biome_feat_loss: 0.0239\n",
      "disent_loss: 0.0022\n",
      "g_loss: -152.6965\n",
      "d_loss: -75.8471\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "Rendering...\n",
      "Done Rendering\n",
      "\n",
      "Step 4650:\n",
      "rec_loss: 0.0740\n",
      "style_loss: -0.0940\n",
      "struct_loss: -0.1440\n",
      "biome_feat_loss: 0.0305\n",
      "disent_loss: 0.0022\n",
      "g_loss: -166.6718\n",
      "d_loss: -82.8348\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 4800:\n",
      "rec_loss: 0.1202\n",
      "style_loss: -0.0891\n",
      "struct_loss: -0.1418\n",
      "biome_feat_loss: 0.0379\n",
      "disent_loss: 0.0022\n",
      "g_loss: -181.0175\n",
      "d_loss: -90.0074\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 4950:\n",
      "rec_loss: 0.0866\n",
      "style_loss: -0.0938\n",
      "struct_loss: -0.1447\n",
      "biome_feat_loss: 0.0294\n",
      "disent_loss: 0.0020\n",
      "g_loss: -195.7388\n",
      "d_loss: -97.3676\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "Rendering...\n",
      "Done Rendering\n",
      "Saving fqgan to fqgan_5000.th\n",
      "Saving optimizer_g to optimizer_g_5000.th\n",
      "Saving optimizer_d to optimizer_d_5000.th\n",
      "Saving stats to ../model_logs/FQGAN12stylecodebook_32struct_addnotconcat3/saved_stats/stats_5000\n",
      "\n",
      "Step 5100:\n",
      "rec_loss: 0.0662\n",
      "style_loss: -0.0945\n",
      "struct_loss: -0.1432\n",
      "biome_feat_loss: 0.0263\n",
      "disent_loss: 0.0020\n",
      "g_loss: -210.8377\n",
      "d_loss: -104.9174\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 5250:\n",
      "rec_loss: 0.0919\n",
      "style_loss: -0.0919\n",
      "struct_loss: -0.1431\n",
      "biome_feat_loss: 0.0320\n",
      "disent_loss: 0.0016\n",
      "g_loss: -226.3077\n",
      "d_loss: -112.6527\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 5400:\n",
      "rec_loss: 0.0509\n",
      "style_loss: -0.0991\n",
      "struct_loss: -0.1485\n",
      "biome_feat_loss: 0.0192\n",
      "disent_loss: 0.0014\n",
      "g_loss: -242.1533\n",
      "d_loss: -120.5750\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "Rendering...\n",
      "Done Rendering\n",
      "\n",
      "Step 5550:\n",
      "rec_loss: 0.0841\n",
      "style_loss: -0.0945\n",
      "struct_loss: -0.1458\n",
      "biome_feat_loss: 0.0279\n",
      "disent_loss: 0.0015\n",
      "g_loss: -258.3739\n",
      "d_loss: -128.6848\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 5700:\n",
      "rec_loss: 0.0967\n",
      "style_loss: -0.0916\n",
      "struct_loss: -0.1429\n",
      "biome_feat_loss: 0.0286\n",
      "disent_loss: 0.0020\n",
      "g_loss: -274.9651\n",
      "d_loss: -136.9800\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 5850:\n",
      "rec_loss: 0.0741\n",
      "style_loss: -0.0948\n",
      "struct_loss: -0.1431\n",
      "biome_feat_loss: 0.0175\n",
      "disent_loss: 0.0021\n",
      "g_loss: -291.9323\n",
      "d_loss: -145.4639\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 6000:\n",
      "rec_loss: 0.0707\n",
      "style_loss: -0.0962\n",
      "struct_loss: -0.1453\n",
      "biome_feat_loss: 0.0232\n",
      "disent_loss: 0.0018\n",
      "g_loss: -309.2720\n",
      "d_loss: -154.1342\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "Rendering...\n",
      "Done Rendering\n",
      "Saving fqgan to fqgan_6000.th\n",
      "Saving optimizer_g to optimizer_g_6000.th\n",
      "Saving optimizer_d to optimizer_d_6000.th\n",
      "Saving stats to ../model_logs/FQGAN12stylecodebook_32struct_addnotconcat3/saved_stats/stats_6000\n",
      "\n",
      "Step 6150:\n",
      "rec_loss: 0.0756\n",
      "style_loss: -0.0944\n",
      "struct_loss: -0.1444\n",
      "biome_feat_loss: 0.0192\n",
      "disent_loss: 0.0020\n",
      "g_loss: -326.9905\n",
      "d_loss: -162.9933\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 6300:\n",
      "rec_loss: 0.0816\n",
      "style_loss: -0.0921\n",
      "struct_loss: -0.1434\n",
      "biome_feat_loss: 0.0330\n",
      "disent_loss: 0.0020\n",
      "g_loss: -345.0822\n",
      "d_loss: -172.0394\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 6450:\n",
      "rec_loss: 0.0797\n",
      "style_loss: -0.0970\n",
      "struct_loss: -0.1451\n",
      "biome_feat_loss: 0.0150\n",
      "disent_loss: 0.0017\n",
      "g_loss: -363.5440\n",
      "d_loss: -181.2693\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "Rendering...\n",
      "Done Rendering\n",
      "\n",
      "Step 6600:\n",
      "rec_loss: 0.0652\n",
      "style_loss: -0.0945\n",
      "struct_loss: -0.1446\n",
      "biome_feat_loss: 0.0255\n",
      "disent_loss: 0.0016\n",
      "g_loss: -382.3837\n",
      "d_loss: -190.6915\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 6750:\n",
      "rec_loss: 0.0719\n",
      "style_loss: -0.0971\n",
      "struct_loss: -0.1464\n",
      "biome_feat_loss: 0.0200\n",
      "disent_loss: 0.0015\n",
      "g_loss: -401.6019\n",
      "d_loss: -200.2974\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 6900:\n",
      "rec_loss: 0.0892\n",
      "style_loss: -0.0858\n",
      "struct_loss: -0.1386\n",
      "biome_feat_loss: 0.0314\n",
      "disent_loss: 0.0018\n",
      "g_loss: -421.1923\n",
      "d_loss: -210.0942\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "Rendering...\n",
      "Done Rendering\n",
      "Saving fqgan to fqgan_7000.th\n",
      "Saving optimizer_g to optimizer_g_7000.th\n",
      "Saving optimizer_d to optimizer_d_7000.th\n",
      "Saving stats to ../model_logs/FQGAN12stylecodebook_32struct_addnotconcat3/saved_stats/stats_7000\n",
      "\n",
      "Step 7050:\n",
      "rec_loss: 0.0616\n",
      "style_loss: -0.0935\n",
      "struct_loss: -0.1444\n",
      "biome_feat_loss: 0.0175\n",
      "disent_loss: 0.0014\n",
      "g_loss: -441.1510\n",
      "d_loss: -220.0741\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 7200:\n",
      "rec_loss: 0.0642\n",
      "style_loss: -0.0966\n",
      "struct_loss: -0.1464\n",
      "biome_feat_loss: 0.0220\n",
      "disent_loss: 0.0016\n",
      "g_loss: -461.4931\n",
      "d_loss: -230.2453\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 7350:\n",
      "rec_loss: 0.0662\n",
      "style_loss: -0.0963\n",
      "struct_loss: -0.1461\n",
      "biome_feat_loss: 0.0180\n",
      "disent_loss: 0.0015\n",
      "g_loss: -482.2075\n",
      "d_loss: -240.6014\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 7500:\n",
      "rec_loss: 0.0709\n",
      "style_loss: -0.0984\n",
      "struct_loss: -0.1472\n",
      "biome_feat_loss: 0.0236\n",
      "disent_loss: 0.0012\n",
      "g_loss: -503.2998\n",
      "d_loss: -251.1489\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "Rendering...\n",
      "Done Rendering\n",
      "\n",
      "Step 7650:\n",
      "rec_loss: 0.0570\n",
      "style_loss: -0.0977\n",
      "struct_loss: -0.1479\n",
      "biome_feat_loss: 0.0179\n",
      "disent_loss: 0.0014\n",
      "g_loss: -524.7445\n",
      "d_loss: -261.8692\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 7800:\n",
      "rec_loss: 0.0714\n",
      "style_loss: -0.0966\n",
      "struct_loss: -0.1447\n",
      "biome_feat_loss: 0.0165\n",
      "disent_loss: 0.0015\n",
      "g_loss: -546.5841\n",
      "d_loss: -272.7900\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 7950:\n",
      "rec_loss: 0.0786\n",
      "style_loss: -0.0922\n",
      "struct_loss: -0.1437\n",
      "biome_feat_loss: 0.0229\n",
      "disent_loss: 0.0013\n",
      "g_loss: -568.7830\n",
      "d_loss: -283.8904\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "Rendering...\n",
      "Done Rendering\n",
      "Saving fqgan to fqgan_8000.th\n",
      "Saving optimizer_g to optimizer_g_8000.th\n",
      "Saving optimizer_d to optimizer_d_8000.th\n",
      "Saving stats to ../model_logs/FQGAN12stylecodebook_32struct_addnotconcat3/saved_stats/stats_8000\n",
      "\n",
      "Step 8100:\n",
      "rec_loss: 0.0670\n",
      "style_loss: -0.0901\n",
      "struct_loss: -0.1451\n",
      "biome_feat_loss: 0.0203\n",
      "disent_loss: 0.0014\n",
      "g_loss: -591.3561\n",
      "d_loss: -295.1761\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 8250:\n",
      "rec_loss: 0.0776\n",
      "style_loss: -0.0933\n",
      "struct_loss: -0.1439\n",
      "biome_feat_loss: 0.0285\n",
      "disent_loss: 0.0013\n",
      "g_loss: -614.3080\n",
      "d_loss: -306.6537\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 8400:\n",
      "rec_loss: 0.0680\n",
      "style_loss: -0.0953\n",
      "struct_loss: -0.1461\n",
      "biome_feat_loss: 0.0377\n",
      "disent_loss: 0.0014\n",
      "g_loss: -637.6432\n",
      "d_loss: -318.3221\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "Rendering...\n",
      "Done Rendering\n",
      "\n",
      "Step 8550:\n",
      "rec_loss: 0.0743\n",
      "style_loss: -0.0909\n",
      "struct_loss: -0.1437\n",
      "biome_feat_loss: 0.0175\n",
      "disent_loss: 0.0014\n",
      "g_loss: -661.3459\n",
      "d_loss: -330.1699\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 8700:\n",
      "rec_loss: 0.0703\n",
      "style_loss: -0.0933\n",
      "struct_loss: -0.1427\n",
      "biome_feat_loss: 0.0223\n",
      "disent_loss: 0.0014\n",
      "g_loss: -685.4039\n",
      "d_loss: -342.2025\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 8850:\n",
      "rec_loss: 0.1017\n",
      "style_loss: -0.0897\n",
      "struct_loss: -0.1398\n",
      "biome_feat_loss: 0.0415\n",
      "disent_loss: 0.0013\n",
      "g_loss: -709.8704\n",
      "d_loss: -354.4345\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 9000:\n",
      "rec_loss: 0.0741\n",
      "style_loss: -0.0912\n",
      "struct_loss: -0.1419\n",
      "biome_feat_loss: 0.0099\n",
      "disent_loss: 0.0014\n",
      "g_loss: -734.6920\n",
      "d_loss: -366.8434\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "Rendering...\n",
      "Done Rendering\n",
      "Saving fqgan to fqgan_9000.th\n",
      "Saving optimizer_g to optimizer_g_9000.th\n",
      "Saving optimizer_d to optimizer_d_9000.th\n",
      "Saving stats to ../model_logs/FQGAN12stylecodebook_32struct_addnotconcat3/saved_stats/stats_9000\n",
      "\n",
      "Step 9150:\n",
      "rec_loss: 0.0378\n",
      "style_loss: -0.1002\n",
      "struct_loss: -0.1503\n",
      "biome_feat_loss: 0.0108\n",
      "disent_loss: 0.0009\n",
      "g_loss: -759.8722\n",
      "d_loss: -379.4334\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 9300:\n",
      "rec_loss: 0.0844\n",
      "style_loss: -0.0948\n",
      "struct_loss: -0.1484\n",
      "biome_feat_loss: 0.0130\n",
      "disent_loss: 0.0010\n",
      "g_loss: -785.4397\n",
      "d_loss: -392.2185\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 9450:\n",
      "rec_loss: 0.0785\n",
      "style_loss: -0.0909\n",
      "struct_loss: -0.1403\n",
      "biome_feat_loss: 0.0229\n",
      "disent_loss: 0.0013\n",
      "g_loss: -811.4045\n",
      "d_loss: -405.2011\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "Rendering...\n",
      "Done Rendering\n",
      "\n",
      "Step 9600:\n",
      "rec_loss: 0.0998\n",
      "style_loss: -0.0881\n",
      "struct_loss: -0.1395\n",
      "biome_feat_loss: 0.0272\n",
      "disent_loss: 0.0015\n",
      "g_loss: -837.7075\n",
      "d_loss: -418.3521\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 9750:\n",
      "rec_loss: 0.0699\n",
      "style_loss: -0.0945\n",
      "struct_loss: -0.1452\n",
      "biome_feat_loss: 0.0148\n",
      "disent_loss: 0.0012\n",
      "g_loss: -864.3999\n",
      "d_loss: -431.6980\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "\n",
      "Step 9900:\n",
      "rec_loss: 0.0469\n",
      "style_loss: -0.0998\n",
      "struct_loss: -0.1485\n",
      "biome_feat_loss: 0.0141\n",
      "disent_loss: 0.0011\n",
      "g_loss: -891.4814\n",
      "d_loss: -445.2377\n",
      "Codebook Usage - Style: 1.00%, Structure: 1.00%\n",
      "Rendering...\n",
      "Done Rendering\n",
      "Saving fqgan to fqgan_9999.th\n",
      "Saving optimizer_g to optimizer_g_9999.th\n",
      "Saving optimizer_d to optimizer_d_9999.th\n",
      "Saving stats to ../model_logs/FQGAN12stylecodebook_32struct_addnotconcat3/saved_stats/stats_9999\n"
     ]
    }
   ],
   "source": [
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n",
    "\n",
    "# Initialize model and loss\n",
    "vqgan = FQModel(H).cuda()\n",
    "vq_loss = VQLossDualCodebook(H).cuda()\n",
    "\n",
    "train_iterator = cycle(train_loader)\n",
    "if val_loader is not None:\n",
    "    val_iterator = cycle(val_loader)\n",
    "\n",
    "# Setup optimizers - one for generator/encoder, one for discriminator\n",
    "optimizer_g = torch.optim.Adam(\n",
    "    vqgan.parameters(), \n",
    "    lr=H.lr,\n",
    "    betas=(H.beta1, H.beta2)\n",
    ")\n",
    "optimizer_d = torch.optim.Adam(\n",
    "    vq_loss.discriminator.parameters(),\n",
    "    lr=H.lr,\n",
    "    betas=(H.beta1, H.beta2)\n",
    ")\n",
    "\n",
    "# Initialize loss tracking\n",
    "plotted_loss_terms = [\n",
    "    'rec_loss', 'style_loss', 'struct_loss', \n",
    "    'biome_feat_loss', 'disent_loss', 'g_loss', 'd_loss'\n",
    "]\n",
    "loss_arrays = {term: np.array([]) for term in plotted_loss_terms}\n",
    "codebook_usage = {\n",
    "    'style': np.array([]),\n",
    "    'struct': np.array([])\n",
    "}\n",
    "\n",
    "# Training loop\n",
    "for step in range(0, H.train_steps):\n",
    "    step_start_time = time.time()\n",
    "    batch = next(train_iterator)\n",
    "    \n",
    "    if isinstance(batch, list):\n",
    "        x = batch[0]\n",
    "    else:\n",
    "        x = batch\n",
    "    x = x.cuda()\n",
    "\n",
    "    \n",
    "    \n",
    "    # Forward pass through model\n",
    "    recons, codebook_loss_style, codebook_loss_struct, disentangle_loss, biome_feat = vqgan(x)\n",
    "\n",
    "    \n",
    "    # Generator/Encoder update\n",
    "    optimizer_g.zero_grad()\n",
    "    # Calculate generator losses\n",
    "    loss_dict = vq_loss(\n",
    "        codebook_loss_style, codebook_loss_struct,\n",
    "        x, recons, disentangle_loss, biome_feat,\n",
    "        optimizer_idx=0,\n",
    "        global_step=step\n",
    "    )\n",
    "    \n",
    "    loss_dict['loss'].backward()\n",
    "    torch.nn.utils.clip_grad_norm_(vqgan.parameters(), max_norm=1.0)\n",
    "    optimizer_g.step()\n",
    "\n",
    "    \n",
    "    # if step % 100 == 0:  # Every 100 steps\n",
    "    #     # Check if style features require grad\n",
    "    #     print(\"Style features require grad:\", biome_feat.requires_grad)\n",
    "        \n",
    "    #     # Check gradient magnitudes after loss.backward()\n",
    "    #     style_grads = biome_feat.grad\n",
    "    #     if style_grads is not None:\n",
    "    #         print(\"Style features grad magnitude:\", style_grads.abs().mean().item())\n",
    "    #     else:\n",
    "    #         print(\"No gradients for style features!\")\n",
    "\n",
    "    # Initialize d_loss_dict\n",
    "    d_loss_dict = {}\n",
    "\n",
    "    # Discriminator update\n",
    "    if step >= H.disc_start_step:\n",
    "        optimizer_d.zero_grad()\n",
    "        \n",
    "        # Calculate discriminator losses\n",
    "        d_loss_dict = vq_loss(\n",
    "            codebook_loss_style, codebook_loss_struct,\n",
    "            x, recons, disentangle_loss, biome_feat,\n",
    "            optimizer_idx=1,\n",
    "            global_step=step\n",
    "        )\n",
    "\n",
    "        # print(f\"Final d_loss: {d_loss_dict}\")\n",
    "        \n",
    "        d_loss_dict['d_loss'].backward()\n",
    "        torch.nn.utils.clip_grad_norm_(vq_loss.discriminator.parameters(), max_norm=1.0)\n",
    "        optimizer_d.step()\n",
    "\n",
    "    # Logging\n",
    "    if step % H.steps_per_log == 0:\n",
    "        print(f\"\\nStep {step}:\")\n",
    "        # Update loss arrays - handle both tensor and float losses\n",
    "        for term in plotted_loss_terms:\n",
    "            if term in loss_dict:\n",
    "                value = loss_dict[term].item() if torch.is_tensor(loss_dict[term]) else loss_dict[term]\n",
    "                loss_arrays[term] = np.append(loss_arrays[term], value)\n",
    "                print(f\"{term}: {value:.4f}\")\n",
    "            elif term == 'd_loss':\n",
    "                # Add 0.0 for d_loss before discriminator starts training\n",
    "                if step >= H.disc_start_step:\n",
    "                    value = d_loss_dict['d_loss'].item() if torch.is_tensor(d_loss_dict['d_loss']) else d_loss_dict['d_loss']\n",
    "                else:\n",
    "                    value = 0.0\n",
    "                loss_arrays[term] = np.append(loss_arrays[term], value)\n",
    "                print(f\"{term}: {value:.4f}\")\n",
    "\n",
    "        # Update codebook usage tracking\n",
    "        style_usage = loss_dict['codebook_usage_style'] if torch.is_tensor(loss_dict['codebook_usage_style']) else loss_dict['codebook_usage_style']\n",
    "        struct_usage = loss_dict['codebook_usage_struct'] if torch.is_tensor(loss_dict['codebook_usage_struct']) else loss_dict['codebook_usage_struct']\n",
    "        \n",
    "        codebook_usage['style'] = np.append(codebook_usage['style'], style_usage)\n",
    "        codebook_usage['struct'] = np.append(codebook_usage['struct'], struct_usage)\n",
    "        print(f\"Codebook Usage - Style: {style_usage:.2f}%, Structure: {struct_usage:.2f}%\")\n",
    "        # Plot in Visdom\n",
    "        x_axis = list(range(0, step+1, H.steps_per_log))\n",
    "        \n",
    "        # Individual loss plots\n",
    "        for term in plotted_loss_terms:\n",
    "\n",
    "            # print(f'term: {term}')\n",
    "            # print(f'value: {loss_arrays[term]}')\n",
    "            # print(f'x_axis len: {len(x_axis)}')\n",
    "            # print(f'value shape: {loss_arrays[term].shape}')\n",
    "            if len(loss_arrays[term]) > 0:\n",
    "                vis.line(\n",
    "                    loss_arrays[term],\n",
    "                    x_axis,\n",
    "                    win=f'{term}_plot',\n",
    "                    opts=dict(title=f'{term} Loss')\n",
    "                )\n",
    "\n",
    "        # Combined loss plot\n",
    "        if len(x_axis) > 1:\n",
    "            vis.line(\n",
    "                Y=np.column_stack([loss_arrays[term] for term in plotted_loss_terms if len(loss_arrays[term]) > 0]),\n",
    "                X=np.column_stack([x_axis for _ in plotted_loss_terms if len(loss_arrays[_]) > 0]),\n",
    "                win='all_losses',\n",
    "                opts=dict(title='All Losses', legend=[t for t in plotted_loss_terms if len(loss_arrays[t]) > 0])\n",
    "            )\n",
    "\n",
    "        # Codebook usage plot\n",
    "        vis.line(\n",
    "            Y=np.column_stack([codebook_usage['style'], codebook_usage['struct']]),\n",
    "            X=np.column_stack([x_axis, x_axis]),\n",
    "            win='codebook_usage',\n",
    "            opts=dict(title='Codebook Usage', legend=['Style Codebook', 'Structure Codebook'])\n",
    "        )\n",
    "\n",
    "    # Visualization of reconstructions\n",
    "    if step % H.steps_per_display_output == 0 or step == H.train_steps - 1:\n",
    "        print(\"Rendering...\")\n",
    "        # Convert indices back to original block IDs for visualization\n",
    "        orig_blocks = block_converter.convert_to_original_blocks(x)\n",
    "        recon_blocks = block_converter.convert_to_original_blocks(recons)\n",
    "        \n",
    "        if step % H.steps_per_save_output == 0:\n",
    "            log_dir = f\"../model_logs/{H.log_dir}/images\"\n",
    "            os.makedirs(log_dir, exist_ok=True)\n",
    "            # display_minecraft(vis, visualizer, orig_blocks, win_name='Original Maps', title=f'Original Maps step {step}', save_path=f\"{log_dir}/orig_{step}.png\")\n",
    "            # display_minecraft(vis, visualizer, recon_blocks, win_name='Reconstructed Maps', title=f'Reconstructed Maps step {step}', save_path=f\"{log_dir}/recon_{step}.png\")\n",
    "            display_minecraft_pyvista(vis, visualizer, orig_blocks, win_name='Original Maps', title=f'Original Maps step {step}', save_path=f\"{log_dir}/orig_{step}.png\", nrow=8)\n",
    "            display_minecraft_pyvista(vis, visualizer, recon_blocks, win_name='Reconstructed Maps', title=f'Reconstructed Maps step {step}', save_path=f\"{log_dir}/recon_{step}.png\", nrow=8)\n",
    "            # display_minecraft(vis, visualizer, orig_blocks, win_name='Original Maps', \n",
    "            #                 f'Original Maps {step}', save_path=f\"{log_dir}/orig_{step}.png\")\n",
    "            # display_minecraft(vis, visualizer, recon_blocks, 'Reconstructed Maps', \n",
    "            #                 f'FQGAN Recons {step}', save_path=f\"{log_dir}/recon_{step}.png\")\n",
    "        else:\n",
    "            # display_minecraft(vis, visualizer, orig_blocks, win_name='Original Maps', title=f'Original Maps step {step}')\n",
    "            # display_minecraft(vis, visualizer, recon_blocks, win_name='Reconstructed Maps', title=f'Reconstructed Maps step {step}')\n",
    "            display_minecraft_pyvista(vis, visualizer, orig_blocks, win_name='Original Maps', title=f'Original Maps step {step}', nrow=8)\n",
    "            display_minecraft_pyvista(vis, visualizer, recon_blocks, win_name='Reconstructed Maps', title=f'Reconstructed Maps step {step}', nrow=8)\n",
    "        print(\"Done Rendering\")\n",
    "\n",
    "    # Run validation\n",
    "    # if step % H.steps_per_validation == 0:\n",
    "    #     val_losses = validate_and_save(\n",
    "    #         vqgan, plotted_loss_terms, vq_loss, val_loader, block_converter,\n",
    "    #         visualizer, vis, step, H\n",
    "    #     )\n",
    "    #     # print(f'Validation losses: {val_losses}')\n",
    "    # Save checkpoints\n",
    "    if (step % H.steps_per_checkpoint == 0 and step > 0) or step == H.train_steps - 1:\n",
    "        save_model(vqgan, 'fqgan', step, H.log_dir)\n",
    "        save_model(optimizer_g, 'optimizer_g', step, H.log_dir)\n",
    "        save_model(optimizer_d, 'optimizer_d', step, H.log_dir)\n",
    "\n",
    "        train_stats = {\n",
    "            'losses': loss_arrays,\n",
    "            'codebook_usage': codebook_usage,\n",
    "            'steps_per_log': H.steps_per_log,\n",
    "        }\n",
    "        save_stats(H, train_stats, step)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
