{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "from visualization_utils import MinecraftVisualizerPyVista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biome and terrain visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_to_cols = {\n",
    "            0: (0.5, 0.25, 0.0),    # light brown\n",
    "            10: 'black', # bedrock\n",
    "            29: \"#006400\", # cacutus\n",
    "            38: \"#B8860B\",  # clay\n",
    "            60: \"brown\",  # dirt\n",
    "            92: \"gold\",  # gold ore\n",
    "            93: \"green\",  # grass\n",
    "            115: \"brown\",  # ladder...?\n",
    "            119: (.02, .28, .16, 0.8),  # transparent forest green (RGBA) for leaves\n",
    "            120: (.02, .28, .16, 0.8),  # leaves2\n",
    "            194: \"yellow\",  # sand\n",
    "            217: \"gray\",  # stone\n",
    "            240: (0.0, 0.0, 1.0, 0.4),  # water\n",
    "            227: (0.0, 1.0, 0.0, .3), # tall grass\n",
    "            237: (0.33, 0.7, 0.33, 0.3), # vine\n",
    "            40: \"#2F4F4F\",  # coal ore\n",
    "            62: \"#228B22\",  # double plant\n",
    "            108: \"#BEBEBE\",  # iron ore\n",
    "            131: \"saddlebrown\",  # log1\n",
    "            132: \"saddlebrown\",  #log2\n",
    "            95: \"lightgray\",  # gravel\n",
    "            243: \"wheat\",  # wheat. lmao\n",
    "            197: \"limegreen\",  # sapling\n",
    "            166: \"orange\",  #pumpkin\n",
    "            167: \"#FF8C00\",  # pumpkin stem\n",
    "            184: \"#FFA07A\",  # red flower\n",
    "            195: \"tan\",  # sandstone\n",
    "            250: \"white\",  #wool \n",
    "            251: \"gold\",   #yellow flower\n",
    "        }\n",
    "\n",
    "def draw_latent_cuboid(fig, latent_coords, size=4):\n",
    "    \"\"\"\n",
    "    Draw a transparent cuboid around the specified latent coordinates.\n",
    "    \n",
    "    Args:\n",
    "        fig: matplotlib figure to draw on\n",
    "        latent_coords: list of tuples, each containing (d,h,w) coordinates\n",
    "        size: size of each latent cell in final space (default 4 for 6->24 upscaling)\n",
    "    \"\"\"\n",
    "    def cuboid_data(o, sizes):\n",
    "        l, w, h = sizes\n",
    "        x = [[o[0], o[0] + l, o[0] + l, o[0], o[0]],  \n",
    "             [o[0], o[0] + l, o[0] + l, o[0], o[0]],  \n",
    "             [o[0], o[0] + l, o[0] + l, o[0], o[0]],  \n",
    "             [o[0], o[0] + l, o[0] + l, o[0], o[0]]]  \n",
    "        y = [[o[1], o[1], o[1] + w, o[1] + w, o[1]],  \n",
    "             [o[1], o[1], o[1] + w, o[1] + w, o[1]],  \n",
    "             [o[1], o[1], o[1], o[1], o[1]],          \n",
    "             [o[1] + w, o[1] + w, o[1] + w, o[1] + w, o[1] + w]]\n",
    "        z = [[o[2], o[2], o[2], o[2], o[2]],          \n",
    "             [o[2] + h, o[2] + h, o[2] + h, o[2] + h, o[2] + h],\n",
    "             [o[2], o[2], o[2] + h, o[2] + h, o[2]],  \n",
    "             [o[2], o[2], o[2] + h, o[2] + h, o[2]]]  \n",
    "        return np.array(x), np.array(y), np.array(z)\n",
    "\n",
    "    ax = fig.gca()\n",
    "    \n",
    "    # Convert coordinates to numpy array for easier manipulation\n",
    "    coords = np.array(latent_coords)\n",
    "    \n",
    "    # Find min and max for each dimension\n",
    "    d_min, h_min, w_min = coords.min(axis=0)\n",
    "    d_max, h_max, w_max = coords.max(axis=0)\n",
    "    \n",
    "    # Calculate origin and sizes\n",
    "    origin = np.array([abs(5 - d_max)*size, w_min*size, h_min*size])\n",
    "    sizes = (\n",
    "        abs(d_max - d_min + 1) * size,  # length\n",
    "        (w_max - w_min + 1) * size,     # width\n",
    "        (h_max - h_min + 1) * size      # height\n",
    "    )\n",
    "    \n",
    "    # Create and draw single cuboid\n",
    "    X, Y, Z = cuboid_data(origin, sizes)\n",
    "    ax.plot_surface(X, Y, Z, color='red', alpha=0.1)\n",
    "    \n",
    "    # Plot edges\n",
    "    for i in range(4):\n",
    "        ax.plot(X[i], Y[i], Z[i], color='red', linewidth=1)\n",
    "    for i in range(4):\n",
    "        ax.plot([X[0][i], X[1][i]], [Y[0][i], Y[1][i]], [Z[0][i], Z[1][i]], \n",
    "               color='red', linewidth=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def visualize_chunk(voxels, figsize=(10, 10), elev=20, azim=45, highlight_latents=None):\n",
    "    \"\"\"\n",
    "    Optimized version of the 3D visualization of a Minecraft chunk.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # Convert one-hot to block IDs if needed\n",
    "    if isinstance(voxels, torch.Tensor):\n",
    "        if voxels.dim() == 4:  # One-hot encoded [C,H,W,D]\n",
    "            voxels = voxels.detach().cpu()\n",
    "            voxels = torch.argmax(voxels, dim=0).numpy()\n",
    "        else:\n",
    "            voxels = voxels.detach().cpu().numpy()\n",
    "\n",
    "    # Apply the same transformations as original\n",
    "    voxels = voxels.transpose(2, 0, 1) # Moves axes from [D,H,W] to [W,D,H]\n",
    "    voxels = np.rot90(voxels, 1, (0, 1))  # Rotate 90 degrees around height axis\n",
    "    # print([block_id for block_id in np.unique(voxels) if block_id not in blocks_to_cols])\n",
    "    # Create figure and 3D axis\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Generate a single boolean mask for each block type\n",
    "    block_masks = {block_id: (voxels == block_id) for block_id in np.unique(voxels) if block_id in blocks_to_cols}\n",
    "    \n",
    "    # Plot all block types with their respective colors\n",
    "    for block_id, mask in block_masks.items():\n",
    "        ax.voxels(mask, facecolors=blocks_to_cols[int(block_id)])\n",
    "    \n",
    "    # Plot remaining blocks in red with black edges\n",
    "    other_vox = (voxels != 5) & (voxels != -1) & (~np.any(np.stack(list(block_masks.values())), axis=0))\n",
    "    ax.voxels(other_vox, edgecolor=\"k\", facecolor=\"red\")\n",
    "    \n",
    "    # Set default view angle\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "\n",
    "    if highlight_latents is not None:\n",
    "        fig = draw_latent_cuboid(fig, highlight_latents)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def visualize_chunk_with_biomes(voxels, biomes, figsize=(10, 10), elev=20, azim=45):\n",
    "    \"\"\"\n",
    "    3D visualization of a Minecraft chunk with biome overlay.\n",
    "    \n",
    "    Args:\n",
    "        voxels: numpy array of block IDs\n",
    "        biomes: numpy array of biome IDs (same shape as voxels)\n",
    "        figsize: tuple for figure size\n",
    "        elev, azim: viewing angle parameters\n",
    "    \"\"\"\n",
    "    # Convert tensors to numpy if needed\n",
    "    if isinstance(voxels, torch.Tensor):\n",
    "        if voxels.dim() == 4:  # One-hot encoded\n",
    "            voxels = voxels.detach().cpu()\n",
    "            voxels = torch.argmax(voxels, dim=0).numpy()\n",
    "        else:\n",
    "            voxels = voxels.detach().cpu().numpy()\n",
    "    if isinstance(biomes, torch.Tensor):\n",
    "        biomes = biomes.detach().cpu().numpy()\n",
    "\n",
    "    # Apply the same transformations to both arrays\n",
    "    voxels = voxels.transpose(2, 0, 1)\n",
    "    voxels = np.rot90(voxels, 1, (0, 1))\n",
    "    biomes = biomes.transpose(2, 0, 1)\n",
    "    biomes = np.rot90(biomes, 1, (0, 1))\n",
    "\n",
    "    # Create figure and 3D axis\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # First plot the regular blocks\n",
    "    block_masks = {block_id: (voxels == block_id) \n",
    "                  for block_id in np.unique(voxels) \n",
    "                  if block_id in blocks_to_cols}\n",
    "    \n",
    "    for block_id, mask in block_masks.items():\n",
    "        ax.voxels(mask, facecolors=blocks_to_cols[int(block_id)])\n",
    "\n",
    "    # Plot remaining blocks in red with black edges\n",
    "    other_vox = (voxels != 5) & (voxels != -1) & (~np.any(np.stack(list(block_masks.values())), axis=0))\n",
    "    ax.voxels(other_vox, edgecolor=\"k\", facecolor=\"red\")\n",
    "\n",
    "    # Create a colormap for biomes\n",
    "    unique_biomes = np.unique(biomes)\n",
    "    biome_colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_biomes)))\n",
    "    biome_color_map = dict(zip(unique_biomes, biome_colors))\n",
    "\n",
    "    # Plot biome overlay with transparency\n",
    "    for biome in unique_biomes:\n",
    "        biome_mask = (biomes == biome)\n",
    "        if biome_mask.any():  # Only plot if biome exists in chunk\n",
    "            color = biome_color_map[biome]\n",
    "            color = (*color[:3], 0.2)  # Set alpha to 0.2 for transparency\n",
    "            ax.voxels(biome_mask, facecolors=color, edgecolor=None)\n",
    "\n",
    "    # Set default view angle\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "\n",
    "    # Add a legend for biomes\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=biome_color_map[biome], \n",
    "                           alpha=0.2,\n",
    "                           label=f'Biome {biome}')\n",
    "                      for biome in unique_biomes]\n",
    "    ax.legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biome dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converters\n",
    "Save mappings from block types and biome strings to ordinal integers, so that we can retrieve the original values from what we generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockBiomeConverter:\n",
    "    def __init__(self, block_mappings=None, biome_mappings=None):\n",
    "        \"\"\"\n",
    "        Initialize with pre-computed mappings for both blocks and biomes\n",
    "        \n",
    "        Args:\n",
    "            block_mappings: dict containing 'index_to_block' and 'block_to_index'\n",
    "            biome_mappings: dict containing 'index_to_biome' and 'biome_to_index'\n",
    "        \"\"\"\n",
    "        self.index_to_block = block_mappings['index_to_block'] if block_mappings else None\n",
    "        self.block_to_index = block_mappings['block_to_index'] if block_mappings else None\n",
    "        self.index_to_biome = biome_mappings['index_to_biome'] if biome_mappings else None\n",
    "        self.biome_to_index = biome_mappings['biome_to_index'] if biome_mappings else None\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataset(cls, data_path):\n",
    "        \"\"\"Create mappings from a dataset file\"\"\"\n",
    "        data = np.load(data_path, allow_pickle=True)\n",
    "        voxels = data['voxels']\n",
    "        biomes = data['biomes']\n",
    "        \n",
    "        # Create block mappings (blocks are integers)\n",
    "        unique_blocks = np.unique(voxels)\n",
    "        block_to_index = {int(block): idx for idx, block in enumerate(unique_blocks)}\n",
    "        index_to_block = {idx: int(block) for idx, block in enumerate(unique_blocks)}\n",
    "        \n",
    "        # Create biome mappings (biomes are strings)\n",
    "        unique_biomes = np.unique(biomes)\n",
    "        biome_to_index = {str(biome): idx for idx, biome in enumerate(unique_biomes)}\n",
    "        index_to_biome = {idx: str(biome) for idx, biome in enumerate(unique_biomes)}\n",
    "        \n",
    "        block_mappings = {'index_to_block': index_to_block, 'block_to_index': block_to_index}\n",
    "        biome_mappings = {'index_to_biome': index_to_biome, 'biome_to_index': biome_to_index}\n",
    "        \n",
    "        return cls(block_mappings, biome_mappings)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_arrays(cls, voxels, biomes):\n",
    "        \"\"\"Create mappings directly from numpy arrays\"\"\"\n",
    "        # Create block mappings (blocks are integers)\n",
    "        unique_blocks = np.unique(voxels)\n",
    "        block_to_index = {int(block): idx for idx, block in enumerate(unique_blocks)}\n",
    "        index_to_block = {idx: int(block) for idx, block in enumerate(unique_blocks)}\n",
    "        \n",
    "        # Create biome mappings (biomes are strings)\n",
    "        unique_biomes = np.unique(biomes)\n",
    "        biome_to_index = {str(biome): idx for idx, biome in enumerate(unique_biomes)}\n",
    "        index_to_biome = {idx: str(biome) for idx, biome in enumerate(unique_biomes)}\n",
    "        \n",
    "        block_mappings = {'index_to_block': index_to_block, 'block_to_index': block_to_index}\n",
    "        biome_mappings = {'index_to_biome': index_to_biome, 'biome_to_index': biome_to_index}\n",
    "        \n",
    "        return cls(block_mappings, biome_mappings)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_mappings(cls, path):\n",
    "        \"\"\"Load pre-saved mappings\"\"\"\n",
    "        mappings = torch.load(path)\n",
    "        return cls(mappings['block_mappings'], mappings['biome_mappings'])\n",
    "    \n",
    "    def save_mappings(self, path):\n",
    "        \"\"\"Save mappings for later use\"\"\"\n",
    "        torch.save({\n",
    "            'block_mappings': {\n",
    "                'index_to_block': self.index_to_block,\n",
    "                'block_to_index': self.block_to_index\n",
    "            },\n",
    "            'biome_mappings': {\n",
    "                'index_to_biome': self.index_to_biome,\n",
    "                'biome_to_index': self.biome_to_index\n",
    "            }\n",
    "        }, path)\n",
    "    \n",
    "    def convert_to_original_blocks(self, data):\n",
    "        \"\"\"\n",
    "        Convert from indices back to original block IDs.\n",
    "        Handles both one-hot encoded and already-indexed data.\n",
    "        \n",
    "        Args:\n",
    "            data: torch.Tensor of either:\n",
    "                - one-hot encoded blocks [B, C, H, W, D] or [C, H, W, D]\n",
    "                - indexed blocks [B, H, W, D] or [H, W, D]\n",
    "        Returns:\n",
    "            torch.Tensor of original block IDs with shape [B, H, W, D] or [H, W, D]\n",
    "        \"\"\"\n",
    "        # If one-hot encoded (dim == 5 or first dim == num_blocks), convert to indices first\n",
    "        if len(data.shape) == 5 or (len(data.shape) == 4 and data.shape[0] == len(self.block_to_index)):\n",
    "            data = torch.argmax(data, dim=1 if len(data.shape) == 5 else 0)\n",
    "        \n",
    "        # Now convert indices to original blocks\n",
    "        if len(data.shape) == 4:  # Batch dimension present\n",
    "            return torch.tensor([[[[self.index_to_block[int(b)] \n",
    "                                for b in row]\n",
    "                                for row in layer]\n",
    "                                for layer in slice_]\n",
    "                                for slice_ in data])\n",
    "        else:  # No batch dimension\n",
    "            return torch.tensor([[[self.index_to_block[int(b)] \n",
    "                                for b in row]\n",
    "                                for row in layer]\n",
    "                                for layer in data])\n",
    "\n",
    "    def convert_to_original_biomes(self, data):\n",
    "        \"\"\"\n",
    "        Convert from indices back to original biome strings.\n",
    "        Handles both one-hot encoded and already-indexed data.\n",
    "        \n",
    "        Args:\n",
    "            data: torch.Tensor of either:\n",
    "                - one-hot encoded biomes [B, C, H, W, D] or [C, H, W, D]\n",
    "                - indexed biomes [B, H, W, D] or [H, W, D]\n",
    "        Returns:\n",
    "            numpy array of original biome strings with shape [B, H, W, D] or [H, W, D]\n",
    "        \"\"\"\n",
    "        # If one-hot encoded (dim == 5 or first dim == num_biomes), convert to indices first\n",
    "        if len(data.shape) == 5 or (len(data.shape) == 4 and data.shape[0] == len(self.biome_to_index)):\n",
    "            data = torch.argmax(data, dim=1 if len(data.shape) == 5 else 0)\n",
    "        \n",
    "        # Now convert indices to original biomes\n",
    "        if len(data.shape) == 4:  # Batch dimension present\n",
    "            return np.array([[[[self.index_to_biome[int(b)] \n",
    "                            for b in row]\n",
    "                            for row in layer]\n",
    "                            for layer in slice_]\n",
    "                            for slice_ in data])\n",
    "        else:  # No batch dimension\n",
    "            return np.array([[[self.index_to_biome[int(b)] \n",
    "                            for b in row]\n",
    "                            for row in layer]\n",
    "                            for layer in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biome Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "biome_mapping = {\n",
    "    'ocean': 'ocean',\n",
    "    'deep_ocean': 'ocean',\n",
    "    'deep_warm_ocean': 'ocean',\n",
    "\n",
    "    'desert': 'desert',\n",
    "    'desert_hills': 'desert',\n",
    "    'mutated_desert': 'desert',\n",
    "\n",
    "    'beaches': 'beaches',\n",
    "    'stone_beach': 'beaches',\n",
    "\n",
    "    'cave': 'cave',\n",
    "\n",
    "    'extreme_hills': 'extreme_hills',\n",
    "    'extreme_hills_with_trees': 'extreme_hills',\n",
    "    'mutated_extreme_hills_with_trees': 'extreme_hills',\n",
    "    'mutated_extreme_hills': 'extreme_hills',\n",
    "\n",
    "    'forest': 'forest',\n",
    "    'forest_hills': 'forest',\n",
    "    'mutated_forest': 'forest',\n",
    "    'mutated_roofed_forest': 'forest',\n",
    "    'roofed_forest': 'forest',\n",
    "\n",
    "    'birch_forest': 'birch_forest',\n",
    "    'birch_forest_hills': 'birch_forest',\n",
    "    'mutated_birch_forest': 'birch_forest',\n",
    "    'mutated_birch_forest_hills': 'birch_forest',\n",
    "\n",
    "    'plains': 'plains',\n",
    "    'mutated_plains': 'plains',\n",
    "\n",
    "    'river': 'river',\n",
    "\n",
    "    'savanna': 'savanna',\n",
    "    'savanna_rock': 'savanna',\n",
    "\n",
    "    'swampland': 'swampland',\n",
    "    'mutated_swampland': 'swampland',\n",
    "\n",
    "    'taiga': 'taiga',\n",
    "    'taiga_hills': 'taiga',\n",
    "    'mutated_taiga': 'taiga',\n",
    "    'redwood_taiga': 'taiga',\n",
    "    # ... add more mappings as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biome_mapping = {\n",
    "#     'ocean': 'ocean',\n",
    "#     'deep_ocean': 'ocean',\n",
    "#     'deep_warm_ocean': 'ocean',\n",
    "#     'desert': 'desert',\n",
    "#     'desert_hills': 'desert',\n",
    "#     'beaches': 'beaches',\n",
    "#     'stone_beach': 'beaches',\n",
    "#     'cave': 'cave',\n",
    "#     'extreme_hills': 'extreme_hills',\n",
    "#     'extreme_hills_with_trees': 'extreme_hills',\n",
    "#     'mutated_extreme_hills_with_trees': 'extreme_hills',\n",
    "#     'forest': 'forest',\n",
    "#     'forest_hills': 'forest',\n",
    "#     'plains': 'plains',\n",
    "#     'mutated_plains': 'plains',\n",
    "#     'river': 'river',\n",
    "#     'savanna': 'savanna',\n",
    "#     'savanna_rock': 'savanna',\n",
    "#     'swampland': 'swampland',\n",
    "#     'taiga': 'taiga',\n",
    "#     'taiga_hills': 'taiga',\n",
    "#     # ... add more mappings as needed\n",
    "# }\n",
    "\n",
    "class MinecraftTerrainDataset(Dataset):\n",
    "    def __init__(self, processed_chunks, processed_biomes, indices, converter):\n",
    "        \"\"\"\n",
    "        Internal constructor used by create_train_val_datasets\n",
    "        \n",
    "        Args:\n",
    "            processed_chunks: Pre-processed chunk data\n",
    "            processed_biomes: Pre-processed biome data\n",
    "            indices: Indices for this split\n",
    "            converter: BlockBiomeConverter instance\n",
    "        \"\"\"\n",
    "        self.processed_chunks = processed_chunks\n",
    "        self.processed_biomes = processed_biomes\n",
    "        self.indices = indices\n",
    "        self.converter = converter\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.indices[idx]\n",
    "        return self.processed_chunks[actual_idx], self.processed_biomes[actual_idx]\n",
    "    \n",
    "    @classmethod\n",
    "    def condense_biomes(cls, voxels, biomes, biome_mapping):\n",
    "        \"\"\"\n",
    "        Combine similar biomes according to mapping.\n",
    "        \n",
    "        Args:\n",
    "            voxels: numpy array of block IDs\n",
    "            biomes: numpy array of biome strings\n",
    "            biome_mapping: dict mapping original biome names to new consolidated names\n",
    "        \n",
    "        Returns:\n",
    "            voxels, biomes (unchanged voxels, consolidated biomes)\n",
    "        \"\"\"\n",
    "        print(\"Condensing biomes...\")\n",
    "        print(\"Original unique biomes:\", np.unique(biomes))\n",
    "        \n",
    "        # Create a copy to modify\n",
    "        new_biomes = biomes.copy()\n",
    "        \n",
    "        # Apply the mapping\n",
    "        for old_biome, new_biome in biome_mapping.items():\n",
    "            new_biomes[biomes == old_biome] = new_biome\n",
    "        \n",
    "        print(\"Condensed unique biomes:\", np.unique(new_biomes))\n",
    "        return voxels, new_biomes\n",
    "    \n",
    "    @classmethod\n",
    "    def preprocess_air_biomes(cls, voxels, biomes, air_id=5, bubble_size=2):\n",
    "        \"\"\"\n",
    "        Preprocesses biome labels by creating an \"air\" biome label for air blocks\n",
    "        beyond a bubble around the terrain.\n",
    "        \n",
    "        Args:\n",
    "            voxels: numpy array of block IDs [H, W, D]\n",
    "            biomes: numpy array of biome strings [H, W, D]\n",
    "            air_id: ID representing air blocks in voxels array\n",
    "            bubble_size: Number of blocks to extend the bubble past terrain\n",
    "        \n",
    "        Returns:\n",
    "            processed_biomes: numpy array with updated biome labels\n",
    "        \"\"\"\n",
    "        from scipy import ndimage\n",
    "        print(\"Processing air biomes...\")\n",
    "        print(\"Original unique biomes:\", np.unique(biomes))\n",
    "        \n",
    "        # Create output array\n",
    "        processed_biomes = biomes.copy()\n",
    "        \n",
    "        # Skip if this chunk contains cave biome\n",
    "        if 'cave' in np.unique(biomes):\n",
    "            return processed_biomes\n",
    "            \n",
    "        # Create binary mask of air blocks\n",
    "        air_mask = (voxels == air_id)\n",
    "        \n",
    "        # Label connected components of air\n",
    "        labeled_array, num_features = ndimage.label(air_mask)\n",
    "        \n",
    "        # Find components that touch the top of the chunk\n",
    "        top_labels = set(np.unique(labeled_array[0, :, :]))  # Labels present in top layer\n",
    "        if 0 in top_labels:  # Remove background label\n",
    "            top_labels.remove(0)\n",
    "            \n",
    "        if len(top_labels) > 0:\n",
    "            # Find the largest component that touches the top\n",
    "            component_sizes = [(label, np.sum(labeled_array == label)) \n",
    "                            for label in top_labels]\n",
    "            sky_label = max(component_sizes, key=lambda x: x[1])[0]\n",
    "            \n",
    "            # Create mask for sky component\n",
    "            sky_mask = (labeled_array == sky_label)\n",
    "            \n",
    "            # Create terrain mask (inverse of sky mask)\n",
    "            terrain_mask = ~sky_mask\n",
    "            \n",
    "            # Dilate the terrain mask to create the bubble\n",
    "            kernel = np.ones((bubble_size*2 + 1, bubble_size*2 + 1, bubble_size*2 + 1))\n",
    "            bubble = ndimage.binary_dilation(terrain_mask, structure=kernel)\n",
    "            \n",
    "            # Set biome to \"air\" for sky blocks outside the bubble\n",
    "            processed_biomes[sky_mask & ~bubble] = \"air\"\n",
    "        \n",
    "        print(\"Processed unique biomes:\", np.unique(processed_biomes))\n",
    "        return processed_biomes\n",
    "    \n",
    "    @classmethod\n",
    "    def remove_underground_chunks(cls, voxels, biomes, min_air_blocks=1152):\n",
    "        \"\"\"\n",
    "        Remove chunks that are underground (have too few air blocks),\n",
    "        except for cave biomes.\n",
    "        \n",
    "        Args:\n",
    "            voxels: numpy array of block IDs\n",
    "            biomes: numpy array of biome strings\n",
    "            min_air_blocks: minimum number of air blocks required (default 24x24x2=1152)\n",
    "        \n",
    "        Returns:\n",
    "            filtered voxels, filtered biomes\n",
    "        \"\"\"\n",
    "        print(\"Removing underground chunks...\")\n",
    "        print(f\"Initial chunks: {len(voxels)}\")\n",
    "        \n",
    "        # Count air blocks in each chunk\n",
    "        air_counts = np.sum(voxels == 5, axis=(1, 2, 3))  # 5 is air block ID\n",
    "        \n",
    "        # Create mask for valid chunks\n",
    "        valid_chunks = []\n",
    "        for i in range(len(voxels)):\n",
    "            unique_biomes = set(np.unique(biomes[i]))\n",
    "            if air_counts[i] >= min_air_blocks or 'cave' in unique_biomes:\n",
    "                valid_chunks.append(i)\n",
    "        \n",
    "        valid_chunks = np.array(valid_chunks)\n",
    "        \n",
    "        # Filter arrays\n",
    "        filtered_voxels = voxels[valid_chunks]\n",
    "        filtered_biomes = biomes[valid_chunks]\n",
    "        \n",
    "        print(f\"Remaining chunks: {len(filtered_voxels)}\")\n",
    "        return filtered_voxels, filtered_biomes\n",
    "    \n",
    "    @classmethod\n",
    "    def create_train_val_datasets(cls, data_path, train_ratio=0.8, biome_mapping=None):\n",
    "        \"\"\"\n",
    "        Creates both training and validation datasets.\n",
    "        \n",
    "        Args:\n",
    "            data_path: Path to the .npz file containing voxels and biomes\n",
    "            train_ratio: Fraction of data to use for training\n",
    "            biome_mapping: Optional dict to condense similar biomes\n",
    "        \"\"\"\n",
    "        data_path = Path(data_path)\n",
    "        mappings_path = data_path.parent / f\"{data_path.stem}_mappings3.pt\"\n",
    "        processed_data_path = data_path.parent / f\"{data_path.stem}_processed_cleaned3.pt\"\n",
    "\n",
    "        # Try to load processed data first\n",
    "        if processed_data_path.exists() and mappings_path.exists():\n",
    "            print(\"Loading pre-processed data and mappings...\")\n",
    "            processed_data = torch.load(processed_data_path)\n",
    "            processed_chunks = processed_data['chunks']\n",
    "            processed_biomes = processed_data['biomes']\n",
    "            converter = BlockBiomeConverter.load_mappings(mappings_path)\n",
    "        else:\n",
    "            # Load and clean raw data\n",
    "            print(\"Processing data for the first time...\")\n",
    "            data = np.load(data_path, allow_pickle=True)\n",
    "            # voxels = data['voxels']\n",
    "            voxels = data['chunks']\n",
    "            biomes = data['biomes']\n",
    "            \n",
    "            # Clean the data\n",
    "            if biome_mapping is not None:\n",
    "                print(\"Condensing biomes...\")\n",
    "                voxels, biomes = cls.condense_biomes(voxels, biomes, biome_mapping)\n",
    "            \n",
    "            print(\"Removing underground chunks...\")\n",
    "            voxels, biomes = cls.remove_underground_chunks(voxels, biomes)\n",
    "            \n",
    "            # print(\"Processing air biomes...\")\n",
    "            # # Process each chunk individually\n",
    "            # processed_biomes = []\n",
    "            # for i in range(len(voxels)):\n",
    "            #     if i % 100 == 0:\n",
    "            #         print(f\"Processing chunk {i}/{len(voxels)}\")\n",
    "            #     chunk_biomes = cls.preprocess_air_biomes(voxels[i], biomes[i], bubble_size=1)\n",
    "            #     processed_biomes.append(chunk_biomes)\n",
    "            # biomes = np.stack(processed_biomes)\n",
    "            # Create mappings with cleaned data\n",
    "            print(\"Creating new block/biome mappings...\")\n",
    "            converter = BlockBiomeConverter.from_arrays(voxels, biomes)\n",
    "            converter.save_mappings(mappings_path)\n",
    "            \n",
    "            # Get dimensions\n",
    "            num_blocks = len(converter.block_to_index)\n",
    "            num_biomes = len(converter.biome_to_index)\n",
    "            \n",
    "                ## Process in batches\n",
    "            num_samples = len(voxels)\n",
    "            batch_size = 100\n",
    "            num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "            \n",
    "            processed_chunks_list = []\n",
    "            processed_biomes_list = []\n",
    "            \n",
    "            print(f\"Processing {num_samples} samples in {num_batches} batches...\")\n",
    "            for i in range(num_batches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min((i + 1) * batch_size, num_samples)\n",
    "                print(f\"Processing batch {i+1}/{num_batches}\")\n",
    "                \n",
    "                # Convert batch to indices\n",
    "                batch_chunks = torch.tensor([[[[converter.block_to_index[int(b)] \n",
    "                                for b in row]\n",
    "                                for row in layer]\n",
    "                                for layer in slice_]\n",
    "                                for slice_ in voxels[start_idx:end_idx]])\n",
    "                \n",
    "                batch_biomes = torch.tensor([[[[converter.biome_to_index[str(b)] \n",
    "                                for b in row]\n",
    "                                for row in layer]\n",
    "                                for layer in slice_]\n",
    "                                for slice_ in biomes[start_idx:end_idx]])\n",
    "                \n",
    "                # One-hot encode batch\n",
    "                processed_batch_chunks = F.one_hot(\n",
    "                    batch_chunks.long(), \n",
    "                    num_classes=num_blocks\n",
    "                ).permute(0, 4, 1, 2, 3).float()\n",
    "                \n",
    "                processed_batch_biomes = F.one_hot(\n",
    "                    batch_biomes.long(),\n",
    "                    num_classes=num_biomes\n",
    "                ).permute(0, 4, 1, 2, 3).float()\n",
    "                \n",
    "                processed_chunks_list.append(processed_batch_chunks)\n",
    "                processed_biomes_list.append(processed_batch_biomes)\n",
    "                \n",
    "                # Clear some memory\n",
    "                del batch_chunks, batch_biomes\n",
    "                del processed_batch_chunks, processed_batch_biomes\n",
    "                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            \n",
    "            del voxels\n",
    "            del biomes\n",
    "            # Concatenate all batches\n",
    "            processed_chunks = torch.cat(processed_chunks_list, dim=0)\n",
    "            processed_biomes = torch.cat(processed_biomes_list, dim=0)\n",
    "                \n",
    "            # Save processed data\n",
    "            print(\"Saving processed data...\")\n",
    "            torch.save({\n",
    "                'chunks': processed_chunks,\n",
    "                'biomes': processed_biomes\n",
    "            }, processed_data_path)\n",
    "        \n",
    "        # Create train/val split\n",
    "        num_samples = len(processed_chunks)\n",
    "        indices = torch.randperm(num_samples)\n",
    "        split_idx = int(num_samples * train_ratio)\n",
    "        \n",
    "        train_indices = indices[:split_idx]\n",
    "        val_indices = indices[split_idx:]\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = cls(processed_chunks, processed_biomes, train_indices, converter)\n",
    "        val_dataset = cls(processed_chunks, processed_biomes, val_indices, converter)\n",
    "        \n",
    "        print(f\"\\nDataset details:\")\n",
    "        print(f\"Total chunks: {len(processed_chunks)}\")\n",
    "        print(f\"Number of unique block types: {len(converter.block_to_index)}\")\n",
    "        print(f\"Number of unique biome types: {len(converter.biome_to_index)}\")\n",
    "        print(f\"Training samples: {len(train_indices)}\")\n",
    "        print(f\"Validation samples: {len(val_indices)}\")\n",
    "        print(f\"Chunk shape: {processed_chunks.shape}\")\n",
    "        print(f\"Biome shape: {processed_biomes.shape}\")\n",
    "        \n",
    "        return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minecraft_dataloaders(data_path, batch_size=32, train_ratio=0.8, num_workers=0, biome_mapping=None):\n",
    "    \"\"\"\n",
    "    Creates training and validation dataloaders for Minecraft chunks.\n",
    "    \"\"\"\n",
    "    # Create both datasets at once\n",
    "    train_dataset, val_dataset = MinecraftTerrainDataset.create_train_val_datasets(\n",
    "        data_path, \n",
    "        train_ratio=train_ratio,\n",
    "        biome_mapping=biome_mapping\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Store converter in the loaders\n",
    "    train_loader.converter = train_dataset.converter\n",
    "    val_loader.converter = val_dataset.converter\n",
    "    \n",
    "    print(f\"\\nDataloader details:\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Training batches: {len(train_loader)}\")\n",
    "    print(f\"Validation batches: {len(val_loader)}\")\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biome classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(in_channels):\n",
    "    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)   #   divides the channels into 32 groups, and normalizes each group. More effective for smaller batch size than batch norm\n",
    "\n",
    "@torch.jit.script\n",
    "def swish(x):\n",
    "    return x*torch.sigmoid(x)   #  swish activation function, compiled using torch.jit.script. Smooth, non-linear activation function, works better than ReLu in some cases. swish (x) = x * sigmoid(x)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=None):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = in_channels if out_channels is None else out_channels\n",
    "        self.norm1 = normalize(in_channels)\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.norm2 = normalize(out_channels)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_out = nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        x = x_in\n",
    "        x = self.norm1(x)\n",
    "        x = swish(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm2(x)\n",
    "        x = swish(x)\n",
    "        x = self.conv2(x)\n",
    "        if self.in_channels != self.out_channels:\n",
    "            x_in = self.conv_out(x_in)\n",
    "\n",
    "        return x + x_in\n",
    "    \n",
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.norm = normalize(in_channels)\n",
    "        # Convert all 2D convolutions to 3D\n",
    "        self.q = torch.nn.Conv3d(\n",
    "            in_channels,\n",
    "            in_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "        self.k = torch.nn.Conv3d(\n",
    "            in_channels,\n",
    "            in_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "        self.v = torch.nn.Conv3d(\n",
    "            in_channels,\n",
    "            in_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "        self.proj_out = torch.nn.Conv3d(\n",
    "            in_channels,\n",
    "            in_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_ = x\n",
    "        h_ = self.norm(h_)\n",
    "        q = self.q(h_)\n",
    "        k = self.k(h_)\n",
    "        v = self.v(h_)\n",
    "\n",
    "        # compute attention\n",
    "        b, c, h, w, d = q.shape\n",
    "        q = q.reshape(b, c, h*w*d)    # Flatten all spatial dimensions\n",
    "        q = q.permute(0, 2, 1)        # b, hwd, c\n",
    "        k = k.reshape(b, c, h*w*d)    # b, c, hwd\n",
    "        w_ = torch.bmm(q, k)          # b, hwd, hwd    \n",
    "        w_ = w_ * (int(c)**(-0.5))    # Scale dot products\n",
    "        w_ = F.softmax(w_, dim=2)     # Softmax over spatial positions\n",
    "\n",
    "        # attend to values\n",
    "        v = v.reshape(b, c, h*w*d)\n",
    "        w_ = w_.permute(0, 2, 1)      # b, hwd, hwd (first hwd of k, second of q)\n",
    "        h_ = torch.bmm(v, w_)         # b, c, hwd\n",
    "        h_ = h_.reshape(b, c, h, w, d) # Restore spatial structure\n",
    "\n",
    "        h_ = self.proj_out(h_)\n",
    "\n",
    "        return x + h_\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiomeClassifier(nn.Module):\n",
    "    def __init__(self, num_block_types, num_biomes, feature_dim=256):\n",
    "        super(BiomeClassifier, self).__init__()\n",
    "        \n",
    "        # Initial embedding layer for one-hot encoded blocks\n",
    "        self.block_proj = nn.Conv3d(num_block_types, 64, kernel_size=1)\n",
    "        \n",
    "        # Encoder layers that will downsample to 6x6x6 spatial dimensions\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Layer 1: 24x24x24 -> 12x12x12\n",
    "            nn.Conv3d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Layer 2: 12x12x12 -> 6x6x6\n",
    "            nn.Conv3d(128, feature_dim, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(feature_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Self-attention block at 6x6x6 resolution\n",
    "        self.attention = AttnBlock(feature_dim)\n",
    "        \n",
    "        # Additional convolution after attention for feature extraction\n",
    "        self.feature_conv = nn.Conv3d(feature_dim, feature_dim, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Biome prediction head\n",
    "        self.biome_head = nn.Conv3d(feature_dim, num_biomes, kernel_size=1)\n",
    "        \n",
    "        # Upsampling layer to get back to original resolution\n",
    "        self.upsample = nn.Upsample(size=(24, 24, 24), mode='trilinear', align_corners=False)\n",
    "        \n",
    "    def get_intermediate_features(self, x):\n",
    "        \"\"\"Get the features after attention for representation learning\"\"\"\n",
    "        x = self.block_proj(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.attention(x)\n",
    "        return self.feature_conv(x)\n",
    "        \n",
    "    def forward(self, x, return_features=False):\n",
    "        # x shape: (batch_size, num_blocks, 24, 24, 24)\n",
    "        \n",
    "        # Initial projection and encoding\n",
    "        x = self.block_proj(x)\n",
    "        x = self.encoder(x)  # (batch_size, feature_dim, 6, 6, 6)\n",
    "        \n",
    "        # Apply self-attention\n",
    "        x = self.attention(x)\n",
    "        \n",
    "        # Get features for representation learning\n",
    "        features = self.feature_conv(x)\n",
    "        \n",
    "        # Predict biomes and upsample\n",
    "        biome_logits = self.biome_head(features)  # (batch_size, num_biomes, 6, 6, 6)\n",
    "        biome_logits = self.upsample(biome_logits)  # (batch_size, num_biomes, 24, 24, 24)\n",
    "        \n",
    "        if return_features:\n",
    "            return biome_logits, features\n",
    "        return biome_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine raw npz files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data_utils import combine_chunk_files\n",
    "# input_dir = \"../../text2env/data/Voxels/voxels\"\n",
    "# output_file = \"../../text2env/data/24_newdataset.npz\"\n",
    "# result = combine_chunk_files(input_dir, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for the first time...\n",
      "Condensing biomes...\n",
      "Condensing biomes...\n",
      "Original unique biomes: ['beaches' 'birch_forest' 'birch_forest_hills' 'cave' 'deep_ocean'\n",
      " 'desert' 'desert_hills' 'extreme_hills' 'extreme_hills_with_trees'\n",
      " 'forest' 'forest_hills' 'mutated_birch_forest'\n",
      " 'mutated_birch_forest_hills' 'mutated_desert' 'mutated_extreme_hills'\n",
      " 'mutated_extreme_hills_with_trees' 'mutated_forest' 'mutated_plains'\n",
      " 'mutated_roofed_forest' 'mutated_swampland' 'mutated_taiga' 'ocean'\n",
      " 'plains' 'river' 'roofed_forest' 'savanna' 'savanna_rock' 'stone_beach'\n",
      " 'swampland' 'taiga' 'taiga_hills']\n",
      "Condensed unique biomes: ['beaches' 'birch_forest' 'cave' 'desert' 'extreme_hills' 'forest'\n",
      " 'mutated_extreme_hills' 'ocean' 'plains' 'river' 'savanna' 'swampland'\n",
      " 'taiga']\n",
      "Removing underground chunks...\n",
      "Removing underground chunks...\n",
      "Initial chunks: 11120\n",
      "Remaining chunks: 11119\n",
      "Creating new block/biome mappings...\n",
      "Processing 11119 samples in 112 batches...\n",
      "Processing batch 1/112\n",
      "Processing batch 2/112\n",
      "Processing batch 3/112\n",
      "Processing batch 4/112\n",
      "Processing batch 5/112\n",
      "Processing batch 6/112\n",
      "Processing batch 7/112\n",
      "Processing batch 8/112\n",
      "Processing batch 9/112\n",
      "Processing batch 10/112\n",
      "Processing batch 11/112\n",
      "Processing batch 12/112\n",
      "Processing batch 13/112\n",
      "Processing batch 14/112\n",
      "Processing batch 15/112\n",
      "Processing batch 16/112\n",
      "Processing batch 17/112\n",
      "Processing batch 18/112\n",
      "Processing batch 19/112\n",
      "Processing batch 20/112\n",
      "Processing batch 21/112\n",
      "Processing batch 22/112\n",
      "Processing batch 23/112\n",
      "Processing batch 24/112\n",
      "Processing batch 25/112\n",
      "Processing batch 26/112\n",
      "Processing batch 27/112\n",
      "Processing batch 28/112\n",
      "Processing batch 29/112\n",
      "Processing batch 30/112\n",
      "Processing batch 31/112\n",
      "Processing batch 32/112\n",
      "Processing batch 33/112\n",
      "Processing batch 34/112\n",
      "Processing batch 35/112\n",
      "Processing batch 36/112\n",
      "Processing batch 37/112\n",
      "Processing batch 38/112\n",
      "Processing batch 39/112\n",
      "Processing batch 40/112\n",
      "Processing batch 41/112\n",
      "Processing batch 42/112\n",
      "Processing batch 43/112\n",
      "Processing batch 44/112\n",
      "Processing batch 45/112\n",
      "Processing batch 46/112\n",
      "Processing batch 47/112\n",
      "Processing batch 48/112\n",
      "Processing batch 49/112\n",
      "Processing batch 50/112\n",
      "Processing batch 51/112\n",
      "Processing batch 52/112\n",
      "Processing batch 53/112\n",
      "Processing batch 54/112\n",
      "Processing batch 55/112\n",
      "Processing batch 56/112\n",
      "Processing batch 57/112\n",
      "Processing batch 58/112\n",
      "Processing batch 59/112\n",
      "Processing batch 60/112\n",
      "Processing batch 61/112\n",
      "Processing batch 62/112\n",
      "Processing batch 63/112\n",
      "Processing batch 64/112\n",
      "Processing batch 65/112\n",
      "Processing batch 66/112\n",
      "Processing batch 67/112\n",
      "Processing batch 68/112\n",
      "Processing batch 69/112\n",
      "Processing batch 70/112\n",
      "Processing batch 71/112\n",
      "Processing batch 72/112\n",
      "Processing batch 73/112\n",
      "Processing batch 74/112\n",
      "Processing batch 75/112\n",
      "Processing batch 76/112\n",
      "Processing batch 77/112\n",
      "Processing batch 78/112\n",
      "Processing batch 79/112\n",
      "Processing batch 80/112\n",
      "Processing batch 81/112\n",
      "Processing batch 82/112\n",
      "Processing batch 83/112\n",
      "Processing batch 84/112\n",
      "Processing batch 85/112\n",
      "Processing batch 86/112\n",
      "Processing batch 87/112\n",
      "Processing batch 88/112\n",
      "Processing batch 89/112\n",
      "Processing batch 90/112\n",
      "Processing batch 91/112\n",
      "Processing batch 92/112\n",
      "Processing batch 93/112\n",
      "Processing batch 94/112\n",
      "Processing batch 95/112\n",
      "Processing batch 96/112\n",
      "Processing batch 97/112\n",
      "Processing batch 98/112\n",
      "Processing batch 99/112\n",
      "Processing batch 100/112\n",
      "Processing batch 101/112\n",
      "Processing batch 102/112\n",
      "Processing batch 103/112\n",
      "Processing batch 104/112\n",
      "Processing batch 105/112\n",
      "Processing batch 106/112\n",
      "Processing batch 107/112\n",
      "Processing batch 108/112\n",
      "Processing batch 109/112\n",
      "Processing batch 110/112\n",
      "Processing batch 111/112\n",
      "Processing batch 112/112\n",
      "Saving processed data...\n",
      "\n",
      "Dataset details:\n",
      "Total chunks: 11119\n",
      "Number of unique block types: 42\n",
      "Number of unique biome types: 13\n",
      "Training samples: 8895\n",
      "Validation samples: 2224\n",
      "Chunk shape: torch.Size([11119, 42, 24, 24, 24])\n",
      "Biome shape: torch.Size([11119, 13, 24, 24, 24])\n",
      "\n",
      "Dataloader details:\n",
      "Batch size: 32\n",
      "Training batches: 278\n",
      "Validation batches: 70\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_loader, val_loader = get_minecraft_dataloaders(\n",
    "    data_path='../../text2env/data/24_newdataset.npz',\n",
    "    batch_size=32,\n",
    "    num_workers=0,\n",
    "    biome_mapping=biome_mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current biome types:\n",
      "0: beaches\n",
      "1: cave\n",
      "2: desert\n",
      "3: extreme_hills\n",
      "4: forest\n",
      "5: ocean\n",
      "6: plains\n",
      "7: river\n",
      "8: savanna\n",
      "9: taiga\n"
     ]
    }
   ],
   "source": [
    "# # Load the processed data\n",
    "# processed_data_path = Path('../../text2env/data/minecraft_biome_newjava_2500_processed_cleaned.pt')\n",
    "# data = torch.load(processed_data_path)\n",
    "# processed_biomes = data['biomes']\n",
    "\n",
    "# # Load the mappings to get biome names\n",
    "# mappings_path = Path('../../text2env/data/minecraft_biome_newjava_2500_mappings.pt')\n",
    "# converter = BlockBiomeConverter.load_mappings(mappings_path)\n",
    "\n",
    "# # Print all unique biomes\n",
    "# print(\"Current biome types:\")\n",
    "# for idx, biome in converter.index_to_biome.items():\n",
    "#     print(f\"{idx}: {biome}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/125: 100%|| 278/278 [00:16<00:00, 16.76it/s, train_loss=0.813]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125:\n",
      "  Train Loss: 0.8125\n",
      "  Val Loss: 2.8253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/125: 100%|| 278/278 [00:16<00:00, 16.89it/s, train_loss=0.574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/125:\n",
      "  Train Loss: 0.5740\n",
      "  Val Loss: 2.8011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/125: 100%|| 278/278 [00:16<00:00, 17.31it/s, train_loss=0.472]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/125:\n",
      "  Train Loss: 0.4721\n",
      "  Val Loss: 2.8334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/125: 100%|| 278/278 [00:17<00:00, 16.32it/s, train_loss=0.43] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/125:\n",
      "  Train Loss: 0.4295\n",
      "  Val Loss: 2.8581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/125: 100%|| 278/278 [00:16<00:00, 16.35it/s, train_loss=0.4]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/125:\n",
      "  Train Loss: 0.3997\n",
      "  Val Loss: 2.8520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/125: 100%|| 278/278 [00:17<00:00, 15.84it/s, train_loss=0.378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/125:\n",
      "  Train Loss: 0.3778\n",
      "  Val Loss: 2.8300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/125: 100%|| 278/278 [00:17<00:00, 15.63it/s, train_loss=0.37] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/125:\n",
      "  Train Loss: 0.3697\n",
      "  Val Loss: 2.8241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/125: 100%|| 278/278 [00:17<00:00, 15.56it/s, train_loss=0.346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/125:\n",
      "  Train Loss: 0.3464\n",
      "  Val Loss: 2.8632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/125: 100%|| 278/278 [00:17<00:00, 16.15it/s, train_loss=0.388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/125:\n",
      "  Train Loss: 0.3879\n",
      "  Val Loss: 2.8490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/125: 100%|| 278/278 [00:17<00:00, 15.59it/s, train_loss=0.306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/125:\n",
      "  Train Loss: 0.3065\n",
      "  Val Loss: 2.9173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/125: 100%|| 278/278 [00:18<00:00, 15.36it/s, train_loss=0.29] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/125:\n",
      "  Train Loss: 0.2899\n",
      "  Val Loss: 2.8915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/125: 100%|| 278/278 [00:17<00:00, 15.76it/s, train_loss=0.291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/125:\n",
      "  Train Loss: 0.2909\n",
      "  Val Loss: 2.8909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/125: 100%|| 278/278 [00:18<00:00, 15.21it/s, train_loss=0.259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/125:\n",
      "  Train Loss: 0.2594\n",
      "  Val Loss: 2.9301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/125: 100%|| 278/278 [00:17<00:00, 15.49it/s, train_loss=0.245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/125:\n",
      "  Train Loss: 0.2450\n",
      "  Val Loss: 2.9306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/125: 100%|| 278/278 [00:16<00:00, 16.41it/s, train_loss=0.218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/125:\n",
      "  Train Loss: 0.2180\n",
      "  Val Loss: 2.9876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/125: 100%|| 278/278 [00:17<00:00, 16.24it/s, train_loss=0.215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/125:\n",
      "  Train Loss: 0.2147\n",
      "  Val Loss: 2.9934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/125: 100%|| 278/278 [00:17<00:00, 15.50it/s, train_loss=0.425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/125:\n",
      "  Train Loss: 0.4255\n",
      "  Val Loss: 2.9379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/125: 100%|| 278/278 [00:17<00:00, 15.94it/s, train_loss=0.207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/125:\n",
      "  Train Loss: 0.2067\n",
      "  Val Loss: 2.9724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/125: 100%|| 278/278 [00:17<00:00, 15.75it/s, train_loss=0.195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/125:\n",
      "  Train Loss: 0.1949\n",
      "  Val Loss: 3.0028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/125: 100%|| 278/278 [00:18<00:00, 15.12it/s, train_loss=0.185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/125:\n",
      "  Train Loss: 0.1851\n",
      "  Val Loss: 2.9969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/125: 100%|| 278/278 [00:17<00:00, 15.52it/s, train_loss=0.186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/125:\n",
      "  Train Loss: 0.1857\n",
      "  Val Loss: 2.9846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/125: 100%|| 278/278 [00:17<00:00, 15.49it/s, train_loss=0.183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/125:\n",
      "  Train Loss: 0.1827\n",
      "  Val Loss: 2.9920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/125: 100%|| 278/278 [00:17<00:00, 15.92it/s, train_loss=0.155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/125:\n",
      "  Train Loss: 0.1553\n",
      "  Val Loss: 3.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/125: 100%|| 278/278 [00:17<00:00, 16.09it/s, train_loss=0.142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/125:\n",
      "  Train Loss: 0.1417\n",
      "  Val Loss: 3.0502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/125: 100%|| 278/278 [00:17<00:00, 16.28it/s, train_loss=0.139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/125:\n",
      "  Train Loss: 0.1391\n",
      "  Val Loss: 3.0282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/125: 100%|| 278/278 [00:17<00:00, 15.88it/s, train_loss=0.14] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/125:\n",
      "  Train Loss: 0.1402\n",
      "  Val Loss: 3.0845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/125: 100%|| 278/278 [00:17<00:00, 16.04it/s, train_loss=0.227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/125:\n",
      "  Train Loss: 0.2266\n",
      "  Val Loss: 3.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/125: 100%|| 278/278 [00:17<00:00, 16.33it/s, train_loss=0.163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/125:\n",
      "  Train Loss: 0.1630\n",
      "  Val Loss: 3.0525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/125: 100%|| 278/278 [00:17<00:00, 15.83it/s, train_loss=0.139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/125:\n",
      "  Train Loss: 0.1392\n",
      "  Val Loss: 3.0350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/125: 100%|| 278/278 [00:18<00:00, 15.09it/s, train_loss=0.132]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/125:\n",
      "  Train Loss: 0.1325\n",
      "  Val Loss: 3.0634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/125: 100%|| 278/278 [00:17<00:00, 15.61it/s, train_loss=0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/125:\n",
      "  Train Loss: 0.1246\n",
      "  Val Loss: 3.0380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/125: 100%|| 278/278 [00:17<00:00, 15.61it/s, train_loss=0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/125:\n",
      "  Train Loss: 0.1253\n",
      "  Val Loss: 3.0627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/125: 100%|| 278/278 [00:17<00:00, 15.62it/s, train_loss=0.124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/125:\n",
      "  Train Loss: 0.1242\n",
      "  Val Loss: 3.0736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/125: 100%|| 278/278 [00:17<00:00, 16.09it/s, train_loss=0.128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/125:\n",
      "  Train Loss: 0.1279\n",
      "  Val Loss: 3.0748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/125: 100%|| 278/278 [00:18<00:00, 15.29it/s, train_loss=0.124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/125:\n",
      "  Train Loss: 0.1236\n",
      "  Val Loss: 3.0803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/125: 100%|| 278/278 [00:17<00:00, 15.84it/s, train_loss=0.153]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/125:\n",
      "  Train Loss: 0.1529\n",
      "  Val Loss: 3.1406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/125: 100%|| 278/278 [00:17<00:00, 15.50it/s, train_loss=0.128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/125:\n",
      "  Train Loss: 0.1277\n",
      "  Val Loss: 3.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/125: 100%|| 278/278 [00:17<00:00, 16.35it/s, train_loss=0.119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/125:\n",
      "  Train Loss: 0.1194\n",
      "  Val Loss: 3.0925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/125: 100%|| 278/278 [00:16<00:00, 16.46it/s, train_loss=0.219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/125:\n",
      "  Train Loss: 0.2190\n",
      "  Val Loss: 3.1054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/125: 100%|| 278/278 [00:18<00:00, 15.26it/s, train_loss=0.147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/125:\n",
      "  Train Loss: 0.1472\n",
      "  Val Loss: 3.1266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/125: 100%|| 278/278 [00:18<00:00, 14.91it/s, train_loss=0.12] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/125:\n",
      "  Train Loss: 0.1195\n",
      "  Val Loss: 3.1585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/125: 100%|| 278/278 [00:18<00:00, 14.75it/s, train_loss=0.107]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/125:\n",
      "  Train Loss: 0.1067\n",
      "  Val Loss: 3.1447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/125: 100%|| 278/278 [00:18<00:00, 15.30it/s, train_loss=0.105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/125:\n",
      "  Train Loss: 0.1049\n",
      "  Val Loss: 3.1709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/125: 100%|| 278/278 [00:17<00:00, 15.48it/s, train_loss=0.102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/125:\n",
      "  Train Loss: 0.1016\n",
      "  Val Loss: 3.1620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/125: 100%|| 278/278 [00:18<00:00, 15.41it/s, train_loss=0.0984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/125:\n",
      "  Train Loss: 0.0984\n",
      "  Val Loss: 3.1396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/125: 100%|| 278/278 [00:17<00:00, 15.55it/s, train_loss=0.101] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/125:\n",
      "  Train Loss: 0.1005\n",
      "  Val Loss: 3.1750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/125: 100%|| 278/278 [00:18<00:00, 15.12it/s, train_loss=0.104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/125:\n",
      "  Train Loss: 0.1043\n",
      "  Val Loss: 3.1886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/125: 100%|| 278/278 [00:18<00:00, 14.64it/s, train_loss=0.1]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/125:\n",
      "  Train Loss: 0.1001\n",
      "  Val Loss: 3.1887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/125: 100%|| 278/278 [00:18<00:00, 14.87it/s, train_loss=0.102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/125:\n",
      "  Train Loss: 0.1022\n",
      "  Val Loss: 3.1572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/125: 100%|| 278/278 [00:17<00:00, 16.35it/s, train_loss=0.1]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/125:\n",
      "  Train Loss: 0.1005\n",
      "  Val Loss: 3.1755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/125: 100%|| 278/278 [00:17<00:00, 16.35it/s, train_loss=0.0962]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/125:\n",
      "  Train Loss: 0.0962\n",
      "  Val Loss: 3.2158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/125: 100%|| 278/278 [00:17<00:00, 15.97it/s, train_loss=0.222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/125:\n",
      "  Train Loss: 0.2222\n",
      "  Val Loss: 3.3892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/125: 100%|| 278/278 [00:16<00:00, 16.39it/s, train_loss=0.163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/125:\n",
      "  Train Loss: 0.1632\n",
      "  Val Loss: 3.1636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/125: 100%|| 278/278 [00:17<00:00, 16.30it/s, train_loss=0.108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/125:\n",
      "  Train Loss: 0.1075\n",
      "  Val Loss: 3.2136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/125: 100%|| 278/278 [00:17<00:00, 16.13it/s, train_loss=0.0914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/125:\n",
      "  Train Loss: 0.0914\n",
      "  Val Loss: 3.2313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/125: 100%|| 278/278 [00:17<00:00, 16.11it/s, train_loss=0.0858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/125:\n",
      "  Train Loss: 0.0858\n",
      "  Val Loss: 3.2525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/125: 100%|| 278/278 [00:17<00:00, 16.08it/s, train_loss=0.0843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/125:\n",
      "  Train Loss: 0.0843\n",
      "  Val Loss: 3.2549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/125: 100%|| 278/278 [00:17<00:00, 16.30it/s, train_loss=0.081] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/125:\n",
      "  Train Loss: 0.0810\n",
      "  Val Loss: 3.2479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/125: 100%|| 278/278 [00:17<00:00, 15.88it/s, train_loss=0.081] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/125:\n",
      "  Train Loss: 0.0810\n",
      "  Val Loss: 3.2375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/125: 100%|| 278/278 [00:17<00:00, 16.25it/s, train_loss=0.0816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/125:\n",
      "  Train Loss: 0.0816\n",
      "  Val Loss: 3.2468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/125: 100%|| 278/278 [00:18<00:00, 15.07it/s, train_loss=0.0837]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/125:\n",
      "  Train Loss: 0.0837\n",
      "  Val Loss: 3.2385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/125: 100%|| 278/278 [00:18<00:00, 15.00it/s, train_loss=0.0851]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/125:\n",
      "  Train Loss: 0.0851\n",
      "  Val Loss: 3.2227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/125: 100%|| 278/278 [00:18<00:00, 15.17it/s, train_loss=0.083] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/125:\n",
      "  Train Loss: 0.0830\n",
      "  Val Loss: 3.2456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/125: 100%|| 278/278 [00:18<00:00, 15.34it/s, train_loss=0.0827]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/125:\n",
      "  Train Loss: 0.0827\n",
      "  Val Loss: 3.2338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/125: 100%|| 278/278 [00:18<00:00, 14.84it/s, train_loss=0.0811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/125:\n",
      "  Train Loss: 0.0811\n",
      "  Val Loss: 3.2479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/125: 100%|| 278/278 [00:18<00:00, 15.18it/s, train_loss=0.0815]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/125:\n",
      "  Train Loss: 0.0815\n",
      "  Val Loss: 3.2512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/125: 100%|| 278/278 [00:18<00:00, 15.31it/s, train_loss=0.0934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/125:\n",
      "  Train Loss: 0.0934\n",
      "  Val Loss: 3.2947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/125: 100%|| 278/278 [00:18<00:00, 15.35it/s, train_loss=0.21] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/125:\n",
      "  Train Loss: 0.2102\n",
      "  Val Loss: 3.4128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/125: 100%|| 278/278 [00:18<00:00, 15.08it/s, train_loss=0.106]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/125:\n",
      "  Train Loss: 0.1062\n",
      "  Val Loss: 3.3045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/125: 100%|| 278/278 [00:17<00:00, 15.51it/s, train_loss=0.0855]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/125:\n",
      "  Train Loss: 0.0855\n",
      "  Val Loss: 3.3248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/125: 100%|| 278/278 [00:18<00:00, 15.07it/s, train_loss=0.082] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/125:\n",
      "  Train Loss: 0.0820\n",
      "  Val Loss: 3.3062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/125: 100%|| 278/278 [00:18<00:00, 14.85it/s, train_loss=0.0813]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/125:\n",
      "  Train Loss: 0.0813\n",
      "  Val Loss: 3.3210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/125: 100%|| 278/278 [00:18<00:00, 15.37it/s, train_loss=0.0731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/125:\n",
      "  Train Loss: 0.0731\n",
      "  Val Loss: 3.3172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/125: 100%|| 278/278 [00:18<00:00, 15.17it/s, train_loss=0.07]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/125:\n",
      "  Train Loss: 0.0700\n",
      "  Val Loss: 3.3396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/125: 100%|| 278/278 [00:18<00:00, 14.75it/s, train_loss=0.0752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/125:\n",
      "  Train Loss: 0.0752\n",
      "  Val Loss: 3.3284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/125: 100%|| 278/278 [00:18<00:00, 15.16it/s, train_loss=0.371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/125:\n",
      "  Train Loss: 0.3710\n",
      "  Val Loss: 3.7334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/125: 100%|| 278/278 [00:18<00:00, 15.43it/s, train_loss=0.122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/125:\n",
      "  Train Loss: 0.1219\n",
      "  Val Loss: 3.5063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78/125: 100%|| 278/278 [00:18<00:00, 14.83it/s, train_loss=0.0861]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/125:\n",
      "  Train Loss: 0.0861\n",
      "  Val Loss: 3.5344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/125: 100%|| 278/278 [00:18<00:00, 15.24it/s, train_loss=0.0774]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/125:\n",
      "  Train Loss: 0.0774\n",
      "  Val Loss: 3.5421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80/125: 100%|| 278/278 [00:18<00:00, 15.28it/s, train_loss=0.0733]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/125:\n",
      "  Train Loss: 0.0733\n",
      "  Val Loss: 3.5317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/125: 100%|| 278/278 [00:19<00:00, 14.44it/s, train_loss=0.0712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/125:\n",
      "  Train Loss: 0.0712\n",
      "  Val Loss: 3.6108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82/125: 100%|| 278/278 [00:18<00:00, 14.84it/s, train_loss=0.0786]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/125:\n",
      "  Train Loss: 0.0786\n",
      "  Val Loss: 3.4772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83/125: 100%|| 278/278 [00:18<00:00, 15.17it/s, train_loss=0.0724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/125:\n",
      "  Train Loss: 0.0724\n",
      "  Val Loss: 3.4969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84/125: 100%|| 278/278 [00:18<00:00, 15.36it/s, train_loss=0.0704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/125:\n",
      "  Train Loss: 0.0704\n",
      "  Val Loss: 3.4982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85/125: 100%|| 278/278 [00:18<00:00, 14.91it/s, train_loss=0.0677]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/125:\n",
      "  Train Loss: 0.0677\n",
      "  Val Loss: 3.5013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86/125: 100%|| 278/278 [00:18<00:00, 14.94it/s, train_loss=0.0677]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/125:\n",
      "  Train Loss: 0.0677\n",
      "  Val Loss: 3.4892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87/125: 100%|| 278/278 [00:18<00:00, 15.15it/s, train_loss=0.0698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/125:\n",
      "  Train Loss: 0.0698\n",
      "  Val Loss: 3.4872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88/125: 100%|| 278/278 [00:18<00:00, 15.38it/s, train_loss=0.0692]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/125:\n",
      "  Train Loss: 0.0692\n",
      "  Val Loss: 3.5712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89/125: 100%|| 278/278 [00:18<00:00, 14.71it/s, train_loss=0.0852]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/125:\n",
      "  Train Loss: 0.0852\n",
      "  Val Loss: 3.8680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90/125: 100%|| 278/278 [00:19<00:00, 14.18it/s, train_loss=0.101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/125:\n",
      "  Train Loss: 0.1015\n",
      "  Val Loss: 3.5088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91/125: 100%|| 278/278 [00:18<00:00, 15.15it/s, train_loss=0.0981]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/125:\n",
      "  Train Loss: 0.0981\n",
      "  Val Loss: 3.4406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92/125: 100%|| 278/278 [00:18<00:00, 15.30it/s, train_loss=0.0861]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/125:\n",
      "  Train Loss: 0.0861\n",
      "  Val Loss: 3.4431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93/125: 100%|| 278/278 [00:18<00:00, 15.16it/s, train_loss=0.0753]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/125:\n",
      "  Train Loss: 0.0753\n",
      "  Val Loss: 3.4577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94/125: 100%|| 278/278 [00:18<00:00, 14.91it/s, train_loss=0.0672]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/125:\n",
      "  Train Loss: 0.0672\n",
      "  Val Loss: 3.3967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95/125: 100%|| 278/278 [00:18<00:00, 15.25it/s, train_loss=0.0646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/125:\n",
      "  Train Loss: 0.0646\n",
      "  Val Loss: 3.4263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96/125: 100%|| 278/278 [00:18<00:00, 15.30it/s, train_loss=0.0647]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/125:\n",
      "  Train Loss: 0.0647\n",
      "  Val Loss: 3.4249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97/125: 100%|| 278/278 [00:18<00:00, 15.29it/s, train_loss=0.0664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/125:\n",
      "  Train Loss: 0.0664\n",
      "  Val Loss: 3.3932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98/125: 100%|| 278/278 [00:18<00:00, 14.81it/s, train_loss=0.0672]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/125:\n",
      "  Train Loss: 0.0672\n",
      "  Val Loss: 3.3708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99/125: 100%|| 278/278 [00:18<00:00, 15.00it/s, train_loss=0.0688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/125:\n",
      "  Train Loss: 0.0688\n",
      "  Val Loss: 3.3950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100/125: 100%|| 278/278 [00:17<00:00, 15.49it/s, train_loss=0.0694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/125:\n",
      "  Train Loss: 0.0694\n",
      "  Val Loss: 3.3841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 101/125: 100%|| 278/278 [00:17<00:00, 16.34it/s, train_loss=0.0696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/125:\n",
      "  Train Loss: 0.0696\n",
      "  Val Loss: 3.3583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 102/125: 100%|| 278/278 [00:17<00:00, 15.75it/s, train_loss=0.072] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/125:\n",
      "  Train Loss: 0.0720\n",
      "  Val Loss: 3.3951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 103/125: 100%|| 278/278 [00:17<00:00, 16.01it/s, train_loss=0.0753]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/125:\n",
      "  Train Loss: 0.0753\n",
      "  Val Loss: 3.3765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 104/125: 100%|| 278/278 [00:17<00:00, 15.64it/s, train_loss=0.0776]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/125:\n",
      "  Train Loss: 0.0776\n",
      "  Val Loss: 3.3700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 105/125: 100%|| 278/278 [00:17<00:00, 16.04it/s, train_loss=0.0715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/125:\n",
      "  Train Loss: 0.0715\n",
      "  Val Loss: 3.3907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 106/125: 100%|| 278/278 [00:17<00:00, 16.16it/s, train_loss=0.0737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/125:\n",
      "  Train Loss: 0.0737\n",
      "  Val Loss: 3.3496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 107/125: 100%|| 278/278 [00:17<00:00, 16.26it/s, train_loss=0.0672]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/125:\n",
      "  Train Loss: 0.0672\n",
      "  Val Loss: 3.3446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 108/125: 100%|| 278/278 [00:16<00:00, 16.37it/s, train_loss=0.0679]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/125:\n",
      "  Train Loss: 0.0679\n",
      "  Val Loss: 3.3514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 109/125: 100%|| 278/278 [00:17<00:00, 15.86it/s, train_loss=0.0637]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/125:\n",
      "  Train Loss: 0.0637\n",
      "  Val Loss: 3.3629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 110/125: 100%|| 278/278 [00:16<00:00, 16.46it/s, train_loss=0.0647]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/125:\n",
      "  Train Loss: 0.0647\n",
      "  Val Loss: 3.3633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 111/125: 100%|| 278/278 [00:18<00:00, 15.36it/s, train_loss=0.0655]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/125:\n",
      "  Train Loss: 0.0655\n",
      "  Val Loss: 3.3554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 112/125: 100%|| 278/278 [00:17<00:00, 15.71it/s, train_loss=0.0655]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/125:\n",
      "  Train Loss: 0.0655\n",
      "  Val Loss: 3.3601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 113/125: 100%|| 278/278 [00:17<00:00, 15.55it/s, train_loss=0.0651]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/125:\n",
      "  Train Loss: 0.0651\n",
      "  Val Loss: 3.3587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 114/125: 100%|| 278/278 [00:18<00:00, 15.02it/s, train_loss=0.17] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/125:\n",
      "  Train Loss: 0.1697\n",
      "  Val Loss: 3.5544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 115/125: 100%|| 278/278 [00:17<00:00, 15.67it/s, train_loss=0.111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/125:\n",
      "  Train Loss: 0.1111\n",
      "  Val Loss: 3.3820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 116/125: 100%|| 278/278 [00:17<00:00, 16.33it/s, train_loss=0.0751]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/125:\n",
      "  Train Loss: 0.0751\n",
      "  Val Loss: 3.4012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 117/125: 100%|| 278/278 [00:17<00:00, 16.23it/s, train_loss=0.064] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/125:\n",
      "  Train Loss: 0.0640\n",
      "  Val Loss: 3.4125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 118/125: 100%|| 278/278 [00:16<00:00, 16.39it/s, train_loss=0.0611]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/125:\n",
      "  Train Loss: 0.0611\n",
      "  Val Loss: 3.4004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 119/125: 100%|| 278/278 [00:17<00:00, 16.01it/s, train_loss=0.0576]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/125:\n",
      "  Train Loss: 0.0576\n",
      "  Val Loss: 3.4346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 120/125: 100%|| 278/278 [00:17<00:00, 15.62it/s, train_loss=0.0564]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/125:\n",
      "  Train Loss: 0.0564\n",
      "  Val Loss: 3.4247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 121/125: 100%|| 278/278 [00:17<00:00, 16.18it/s, train_loss=0.0565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/125:\n",
      "  Train Loss: 0.0565\n",
      "  Val Loss: 3.4636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 122/125: 100%|| 278/278 [00:17<00:00, 16.15it/s, train_loss=0.0581]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/125:\n",
      "  Train Loss: 0.0581\n",
      "  Val Loss: 3.4416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 123/125: 100%|| 278/278 [00:17<00:00, 16.22it/s, train_loss=0.0638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/125:\n",
      "  Train Loss: 0.0638\n",
      "  Val Loss: 3.4403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 124/125: 100%|| 278/278 [00:17<00:00, 15.86it/s, train_loss=0.0616]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/125:\n",
      "  Train Loss: 0.0616\n",
      "  Val Loss: 3.4118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 125/125: 100%|| 278/278 [00:18<00:00, 15.35it/s, train_loss=0.0626]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/125:\n",
      "  Train Loss: 0.0626\n",
      "  Val Loss: 3.4063\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and move to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BiomeClassifier(\n",
    "    num_block_types=len(train_loader.converter.block_to_index),\n",
    "    num_biomes=len(train_loader.converter.biome_to_index)\n",
    ").to(device)\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 125\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "    for terrain, biomes in progress_bar:\n",
    "        terrain = terrain.to(device)\n",
    "        biomes = biomes.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(terrain)  # (batch_size, num_biomes, 24, 24, 24)\n",
    "        \n",
    "        # Convert target from one-hot to indices\n",
    "        target = biomes.argmax(dim=1)  # (batch_size, 24, 24, 24)\n",
    "        \n",
    "        # Compute loss - no reshaping needed!\n",
    "        loss = criterion(logits, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({'train_loss': train_loss / train_batches})\n",
    "    \n",
    "    avg_train_loss = train_loss / train_batches\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for terrain, biomes in val_loader:\n",
    "            terrain = terrain.to(device)\n",
    "            biomes = biomes.to(device)\n",
    "            \n",
    "            logits = model(terrain)\n",
    "            \n",
    "            B, C, H, W, D = logits.shape\n",
    "            loss = criterion(\n",
    "                logits.reshape(B * H * W * D, C),\n",
    "                biomes.argmax(dim=1).reshape(B * H * W * D)\n",
    "            )\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_batches += 1\n",
    "    \n",
    "    avg_val_loss = val_loss / val_batches\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "    print(f'  Train Loss: {avg_train_loss:.4f}')\n",
    "    print(f'  Val Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'best_biome_classifier_airprocessed.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiomeClassifier(\n",
       "  (block_proj): Conv3d(43, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv3d(64, 128, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv3d(128, 256, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (4): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (attention): AttnBlock(\n",
       "    (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "    (q): Conv3d(256, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (k): Conv3d(256, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (v): Conv3d(256, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (proj_out): Conv3d(256, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  )\n",
       "  (feature_conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (biome_head): Conv3d(256, 14, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  (upsample): Upsample(size=(24, 24, 24), mode='trilinear')\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BiomeClassifier(\n",
    "    num_block_types=len(train_loader.converter.block_to_index),\n",
    "    num_biomes=len(train_loader.converter.biome_to_index)\n",
    ").to('cuda')\n",
    "        \n",
    "        # Load pretrained weights for biome classifier\n",
    "model.load_state_dict(torch.load('best_biome_classifier_airprocessed.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.4.3'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'vtk': 'https://cdn.jsdelivr.net/npm/vtk.js@30.1.0/vtk'}, 'shim': {'vtk': {'exports': 'vtk'}}});\n      require([\"vtk\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 1;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window.vtk !== undefined) && (!(window.vtk instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.4.5/dist/bundled/abstractvtkplot/vtk.js@30.1.0/vtk.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.holoviz.org/panel/1.4.5/dist/bundled/abstractvtkplot/vtk.js@30.1.0/vtk.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.3.min.js\", \"https://cdn.holoviz.org/panel/1.4.5/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n\ttry {\n          inline_js[i].call(root, root.Bokeh);\n\t} catch(e) {\n\t  if (!reloading) {\n\t    throw e;\n\t  }\n\t}\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        })\n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='8033bbb5-7b94-4860-8fac-127c42423130'>\n",
       "  <div id=\"fe3d41bf-9353-4a8e-8265-1b2ac0c3c6bb\" data-root-id=\"8033bbb5-7b94-4860-8fac-127c42423130\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"b658cef6-5691-4b9a-89de-dbafd85f80f7\":{\"version\":\"3.4.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"8033bbb5-7b94-4860-8fac-127c42423130\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"32892c20-acaa-4390-87c0-ffa4def8dced\",\"attributes\":{\"plot_id\":\"8033bbb5-7b94-4860-8fac-127c42423130\",\"comm_id\":\"2f0ae254c96a4b7f9f3732ac19fbd6f9\",\"client_comm_id\":\"5afc60187ab240ee84f2d5528c3d198f\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":null}]}]}};\n",
       "  var render_items = [{\"docid\":\"b658cef6-5691-4b9a-89de-dbafd85f80f7\",\"roots\":{\"8033bbb5-7b94-4860-8fac-127c42423130\":\"fe3d41bf-9353-4a8e-8265-1b2ac0c3c6bb\"},\"root_ids\":[\"8033bbb5-7b94-4860-8fac-127c42423130\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined && ( root.vtk !== undefined) && ( root.vtk !== undefined) && ( root.vtk !== undefined) && ( root.vtk !== undefined))\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "8033bbb5-7b94-4860-8fac-127c42423130"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7300b3904d604269865d83b9d47ef3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:58071/index.html?ui=P_0x1943050a520_0&reconnect=auto\" class=\"pyvis"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50efff4b275b4f4ba2367c93a34ce280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:58071/index.html?ui=P_0x194307266a0_1&reconnect=auto\" class=\"pyvis"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab969fc45884b7ba14f2c52a0da5dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:58071/index.html?ui=P_0x19c8cc9eb20_2&reconnect=auto\" class=\"pyvis"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7681ef4ec747eda27847834990b10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:58071/index.html?ui=P_0x19ca02d6ac0_3&reconnect=auto\" class=\"pyvis"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c672a2cab1024693abf2e0e37f8896e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:58071/index.html?ui=P_0x19cab9e5cd0_4&reconnect=auto\" class=\"pyvis"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89bf372ed754c56b2de7c1296dbe33e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:58071/index.html?ui=P_0x19cabdbcaf0_5&reconnect=auto\" class=\"pyvis"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c999a643336451b8ad4ad7d14f59ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:58071/index.html?ui=P_0x19cabe84cd0_6&reconnect=auto\" class=\"pyvis"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20789a07a2044379a1d8fd562da9fcb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:58071/index.html?ui=P_0x19cb2f4c1f0_7&reconnect=auto\" class=\"pyvis"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a009729dbe49578d88674da77657c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:58071/index.html?ui=P_0x19cb33fccd0_8&reconnect=auto\" class=\"pyvis"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dfa784aefeb4c2bad0e1d40e9f14212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:58071/index.html?ui=P_0x19cc4de98e0_9&reconnect=auto\" class=\"pyvis"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get 5 random samples\n",
    "visualizer = MinecraftVisualizerPyVista()\n",
    "num_samples = 5\n",
    "all_batches = list(val_loader)\n",
    "device = 'cuda'\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # Get a random batch\n",
    "    random_idx = np.random.randint(0, len(all_batches))\n",
    "    terrain_batch, biome_batch = all_batches[random_idx]\n",
    "    \n",
    "    # Move to device for prediction\n",
    "    terrain_batch = terrain_batch.to(device)\n",
    "    \n",
    "    # Get model predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_biomes = model(terrain_batch)\n",
    "    \n",
    "    # Take first sample from batch\n",
    "    terrain = terrain_batch[0]\n",
    "    true_biomes = biome_batch[0]\n",
    "    pred_biomes = pred_biomes[0]\n",
    "    \n",
    "    # Convert everything back to original format\n",
    "    original_terrain = val_loader.converter.convert_to_original_blocks(terrain)\n",
    "    original_true_biomes = val_loader.converter.convert_to_original_biomes(true_biomes)\n",
    "    original_pred_biomes = val_loader.converter.convert_to_original_biomes(pred_biomes)\n",
    "\n",
    "    \n",
    "    plotter = visualizer.visualize_chunk_with_biomes(original_terrain, original_true_biomes)\n",
    "    plotter.show()\n",
    "\n",
    "    plotter = visualizer.visualize_chunk_with_biomes(original_terrain, original_pred_biomes)\n",
    "    plotter.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.4.3'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'vtk': 'https://cdn.jsdelivr.net/npm/vtk.js@30.1.0/vtk'}, 'shim': {'vtk': {'exports': 'vtk'}}});\n      require([\"vtk\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 1;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window.vtk !== undefined) && (!(window.vtk instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.4.5/dist/bundled/abstractvtkplot/vtk.js@30.1.0/vtk.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.holoviz.org/panel/1.4.5/dist/bundled/abstractvtkplot/vtk.js@30.1.0/vtk.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.3.min.js\", \"https://cdn.holoviz.org/panel/1.4.5/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n\ttry {\n          inline_js[i].call(root, root.Bokeh);\n\t} catch(e) {\n\t  if (!reloading) {\n\t    throw e;\n\t  }\n\t}\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        })\n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='46b730bf-63d5-4027-86f2-569af4b7ab5e'>\n",
       "  <div id=\"d933e42d-c448-4061-8942-550896c49b26\" data-root-id=\"46b730bf-63d5-4027-86f2-569af4b7ab5e\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"99ead0ca-45ee-4518-a672-e0abd1a60c3e\":{\"version\":\"3.4.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"46b730bf-63d5-4027-86f2-569af4b7ab5e\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"8d3d9f77-bf9e-4aca-844d-8fdf469d5840\",\"attributes\":{\"plot_id\":\"46b730bf-63d5-4027-86f2-569af4b7ab5e\",\"comm_id\":\"90b44b9016a84ff581a1869b33fc6778\",\"client_comm_id\":\"ee49582a45434d97bb52b8f569fc845b\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":null}]}]}};\n",
       "  var render_items = [{\"docid\":\"99ead0ca-45ee-4518-a672-e0abd1a60c3e\",\"roots\":{\"46b730bf-63d5-4027-86f2-569af4b7ab5e\":\"d933e42d-c448-4061-8942-550896c49b26\"},\"root_ids\":[\"46b730bf-63d5-4027-86f2-569af4b7ab5e\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined && ( root.vtk !== undefined) && ( root.vtk !== undefined))\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "46b730bf-63d5-4027-86f2-569af4b7ab5e"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9848654c96be40b8ac3a1232c2139474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:58557/index.html?ui=P_0x1bb7f17ffd0_0&reconnect=auto\" class=\"pyvis"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259e822f8f7746f3bba226a8ea423e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:58557/index.html?ui=P_0x1bb83ed0670_1&reconnect=auto\" class=\"pyvis"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ddafc1d5284ddda46240fd22eadc41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:58557/index.html?ui=P_0x1bb93333940_2&reconnect=auto\" class=\"pyvis"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3cf5124dc043728727436a43f597ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:58557/index.html?ui=P_0x1bb94d63ee0_3&reconnect=auto\" class=\"pyvis"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70fc334f59c4e2ca6dfdeb6e65757a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:58557/index.html?ui=P_0x1bb94eb21f0_4&reconnect=auto\" class=\"pyvis"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9402a97c45a146bf9c4ac1baa4d60af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:58557/index.html?ui=P_0x1bb9558a490_5&reconnect=auto\" class=\"pyvis"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2526fd2548e3473684ef27758ef3b3c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:58557/index.html?ui=P_0x1bba7e65a30_6&reconnect=auto\" class=\"pyvis"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f0cc68923546b88c9327b398a62064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:58557/index.html?ui=P_0x1bba7f47910_7&reconnect=auto\" class=\"pyvis"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b20676e5c148c296ee16172b2c838b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:58557/index.html?ui=P_0x1bbae3944f0_8&reconnect=auto\" class=\"pyvis"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb954a802ffa409b8784a22215dce031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:58557/index.html?ui=P_0x1bbae6e7fd0_9&reconnect=auto\" class=\"pyvis"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get first batch\n",
    "terrain_batch, biome_batch = next(iter(train_loader))\n",
    "visualizer = MinecraftVisualizerPyVista()\n",
    "num_samples = 5\n",
    "device = 'cuda'\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # Take sample i from batch and move to device\n",
    "    terrain = terrain_batch[i:i+1].to(device)  # Keep batch dimension\n",
    "    biomes = biome_batch[i]\n",
    "    \n",
    "    # Get model predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_biomes = model(terrain)[0]  # Remove batch dimension\n",
    "    \n",
    "    # Convert everything back to original format\n",
    "    original_terrain = train_loader.dataset.converter.convert_to_original_blocks(terrain_batch[i])\n",
    "    original_true_biomes = train_loader.dataset.converter.convert_to_original_biomes(biomes)\n",
    "    original_pred_biomes = train_loader.dataset.converter.convert_to_original_biomes(pred_biomes)\n",
    "    \n",
    "    # Visualize true and predicted\n",
    "    print(f\"\\nSample {i+1}\")\n",
    "    plotter = visualizer.visualize_chunk_with_biomes(original_terrain, original_true_biomes)\n",
    "    plotter.show()\n",
    "    \n",
    "    plotter = visualizer.visualize_chunk_with_biomes(original_terrain, original_pred_biomes)\n",
    "    plotter.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
