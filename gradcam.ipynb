{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from sampler_utils import retrieve_autoencoder_components_state_dicts, latent_ids_to_onehot3d, get_latent_loaders\n",
    "from models3d import VQAutoEncoder, Generator\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, fixed\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from PIL import Image\n",
    "import torch.distributions as dists\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from models3d import BiomeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "#matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockBiomeConverter:\n",
    "    def __init__(self, block_mappings=None, biome_mappings=None):\n",
    "        \"\"\"\n",
    "        Initialize with pre-computed mappings for both blocks and biomes\n",
    "        \n",
    "        Args:\n",
    "            block_mappings: dict containing 'index_to_block' and 'block_to_index'\n",
    "            biome_mappings: dict containing 'index_to_biome' and 'biome_to_index'\n",
    "        \"\"\"\n",
    "        self.index_to_block = block_mappings['index_to_block'] if block_mappings else None\n",
    "        self.block_to_index = block_mappings['block_to_index'] if block_mappings else None\n",
    "        self.index_to_biome = biome_mappings['index_to_biome'] if biome_mappings else None\n",
    "        self.biome_to_index = biome_mappings['biome_to_index'] if biome_mappings else None\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataset(cls, data_path):\n",
    "        \"\"\"Create mappings from a dataset file\"\"\"\n",
    "        data = np.load(data_path, allow_pickle=True)\n",
    "        voxels = data['voxels']\n",
    "        biomes = data['biomes']\n",
    "        \n",
    "        # Create block mappings (blocks are integers)\n",
    "        unique_blocks = np.unique(voxels)\n",
    "        block_to_index = {int(block): idx for idx, block in enumerate(unique_blocks)}\n",
    "        index_to_block = {idx: int(block) for idx, block in enumerate(unique_blocks)}\n",
    "        \n",
    "        # Create biome mappings (biomes are strings)\n",
    "        unique_biomes = np.unique(biomes)\n",
    "        biome_to_index = {str(biome): idx for idx, biome in enumerate(unique_biomes)}\n",
    "        index_to_biome = {idx: str(biome) for idx, biome in enumerate(unique_biomes)}\n",
    "        \n",
    "        block_mappings = {'index_to_block': index_to_block, 'block_to_index': block_to_index}\n",
    "        biome_mappings = {'index_to_biome': index_to_biome, 'biome_to_index': biome_to_index}\n",
    "        \n",
    "        return cls(block_mappings, biome_mappings)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_arrays(cls, voxels, biomes):\n",
    "        \"\"\"Create mappings directly from numpy arrays\"\"\"\n",
    "        # Create block mappings (blocks are integers)\n",
    "        unique_blocks = np.unique(voxels)\n",
    "        block_to_index = {int(block): idx for idx, block in enumerate(unique_blocks)}\n",
    "        index_to_block = {idx: int(block) for idx, block in enumerate(unique_blocks)}\n",
    "        \n",
    "        # Create biome mappings (biomes are strings)\n",
    "        unique_biomes = np.unique(biomes)\n",
    "        biome_to_index = {str(biome): idx for idx, biome in enumerate(unique_biomes)}\n",
    "        index_to_biome = {idx: str(biome) for idx, biome in enumerate(unique_biomes)}\n",
    "        \n",
    "        block_mappings = {'index_to_block': index_to_block, 'block_to_index': block_to_index}\n",
    "        biome_mappings = {'index_to_biome': index_to_biome, 'biome_to_index': biome_to_index}\n",
    "        \n",
    "        return cls(block_mappings, biome_mappings)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_mappings(cls, path):\n",
    "        \"\"\"Load pre-saved mappings\"\"\"\n",
    "        mappings = torch.load(path)\n",
    "        return cls(mappings['block_mappings'], mappings['biome_mappings'])\n",
    "    \n",
    "    def save_mappings(self, path):\n",
    "        \"\"\"Save mappings for later use\"\"\"\n",
    "        torch.save({\n",
    "            'block_mappings': {\n",
    "                'index_to_block': self.index_to_block,\n",
    "                'block_to_index': self.block_to_index\n",
    "            },\n",
    "            'biome_mappings': {\n",
    "                'index_to_biome': self.index_to_biome,\n",
    "                'biome_to_index': self.biome_to_index\n",
    "            }\n",
    "        }, path)\n",
    "    \n",
    "    def convert_to_original_blocks(self, data):\n",
    "        \"\"\"\n",
    "        Convert from indices back to original block IDs.\n",
    "        Handles both one-hot encoded and already-indexed data.\n",
    "        \n",
    "        Args:\n",
    "            data: torch.Tensor of either:\n",
    "                - one-hot encoded blocks [B, C, H, W, D] or [C, H, W, D]\n",
    "                - indexed blocks [B, H, W, D] or [H, W, D]\n",
    "        Returns:\n",
    "            torch.Tensor of original block IDs with shape [B, H, W, D] or [H, W, D]\n",
    "        \"\"\"\n",
    "        # If one-hot encoded (dim == 5 or first dim == num_blocks), convert to indices first\n",
    "        if len(data.shape) == 5 or (len(data.shape) == 4 and data.shape[0] == len(self.block_to_index)):\n",
    "            data = torch.argmax(data, dim=1 if len(data.shape) == 5 else 0)\n",
    "        \n",
    "        # Now convert indices to original blocks\n",
    "        if len(data.shape) == 4:  # Batch dimension present\n",
    "            return torch.tensor([[[[self.index_to_block[int(b)] \n",
    "                                for b in row]\n",
    "                                for row in layer]\n",
    "                                for layer in slice_]\n",
    "                                for slice_ in data])\n",
    "        else:  # No batch dimension\n",
    "            return torch.tensor([[[self.index_to_block[int(b)] \n",
    "                                for b in row]\n",
    "                                for row in layer]\n",
    "                                for layer in data])\n",
    "\n",
    "    def convert_to_original_biomes(self, data):\n",
    "        \"\"\"\n",
    "        Convert from indices back to original biome strings.\n",
    "        Handles both one-hot encoded and already-indexed data.\n",
    "        \n",
    "        Args:\n",
    "            data: torch.Tensor of either:\n",
    "                - one-hot encoded biomes [B, C, H, W, D] or [C, H, W, D]\n",
    "                - indexed biomes [B, H, W, D] or [H, W, D]\n",
    "        Returns:\n",
    "            numpy array of original biome strings with shape [B, H, W, D] or [H, W, D]\n",
    "        \"\"\"\n",
    "        # If one-hot encoded (dim == 5 or first dim == num_biomes), convert to indices first\n",
    "        if len(data.shape) == 5 or (len(data.shape) == 4 and data.shape[0] == len(self.biome_to_index)):\n",
    "            data = torch.argmax(data, dim=1 if len(data.shape) == 5 else 0)\n",
    "        \n",
    "        # Now convert indices to original biomes\n",
    "        if len(data.shape) == 4:  # Batch dimension present\n",
    "            return np.array([[[[self.index_to_biome[int(b)] \n",
    "                            for b in row]\n",
    "                            for row in layer]\n",
    "                            for layer in slice_]\n",
    "                            for slice_ in data])\n",
    "        else:  # No batch dimension\n",
    "            return np.array([[[self.index_to_biome[int(b)] \n",
    "                            for b in row]\n",
    "                            for row in layer]\n",
    "                            for layer in data])\n",
    "        \n",
    "    def get_air_block_index(self):\n",
    "        \"\"\"\n",
    "        Find the one-hot index corresponding to the air block (ID 5).\n",
    "        Returns:\n",
    "            int: The index where air blocks are encoded in one-hot format\n",
    "        \"\"\"\n",
    "        # Find the index that maps to block ID 5 (air) in our index_to_block mapping\n",
    "        for idx, block_id in self.index_to_block.items():\n",
    "            if block_id == 5:  # Air block ID\n",
    "                return idx\n",
    "        raise ValueError(\"Air block (ID 5) not found in block mappings!\")\n",
    "    \n",
    "    def get_blockid_indices(self, block_ids):\n",
    "        \"\"\"\n",
    "        Find the one-hot index corresponding to the air block (ID 5).\n",
    "        Returns:\n",
    "            int: The index where air blocks are encoded in one-hot format\n",
    "        \"\"\"\n",
    "        # Find the index that maps to block ID 5 (air) in our index_to_block mapping\n",
    "        idxs = []\n",
    "        for idx, block_id in self.index_to_block.items():\n",
    "            if block_id in block_ids:  # Air block ID\n",
    "                idxs.append(idx)\n",
    "        if len(idxs) == 0:\n",
    "            raise ValueError(\"Air block (ID 5) not found in block mappings!\")\n",
    "        return idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minecraft Chunks Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class MinecraftDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        data_path = Path(data_path)\n",
    "\n",
    "        # Try to load processed data first\n",
    "        # assert processed_data_path.exists() and mappings_path.exists()\n",
    "\n",
    "        print(\"Loading pre-processed data...\")\n",
    "        processed_data = torch.load(data_path)\n",
    "        # Only keep the chunks, discard biome data\n",
    "        self.processed_chunks = processed_data['chunks']\n",
    "        # self.processed_biomes = processed_data['biomes']\n",
    "        # Delete the biomes to free memory\n",
    "        # del processed_data['biomes']\n",
    "        # del processed_data['chunks']\n",
    "        del processed_data\n",
    "        \n",
    "        \n",
    "        print(f\"Loaded {len(self.processed_chunks)} chunks of size {self.processed_chunks.shape[1:]}\")\n",
    "        print(f\"Number of unique block types: {self.processed_chunks.shape[1]}\")\n",
    "        print(f'Unique blocks: {torch.unique(torch.argmax(self.processed_chunks, dim=1)).tolist()}')\n",
    "        #print(f\"Loaded {len(self.processed_biomes)} chunks of size {self.processed_biomes.shape[1:]}\")\n",
    "        #print(f\"Number of unique biome types: {self.processed_biomes.shape[1]}\")\n",
    "        #print(f'Unique biomes: {torch.unique(torch.argmax(self.processed_biomes, dim=1)).tolist()}')\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return self.processed_chunks[idx]\n",
    "        return self.processed_chunks[idx] #, self.processed_biomes[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        # return len(self.processed_chunks), len(self.processed_biomes)\n",
    "        return len(self.processed_chunks)\n",
    "\n",
    "def get_minecraft_dataloaders(data_path, batch_size=32, val_split=0.1, num_workers=4):\n",
    "    \"\"\"\n",
    "    Creates training and validation dataloaders for Minecraft chunks.\n",
    "    \"\"\"\n",
    "    # Create dataset\n",
    "    dataset = MinecraftDataset(data_path)\n",
    "    \n",
    "    # Split into train and validation sets\n",
    "    val_size = int(val_split * len(dataset))\n",
    "    train_size = len(dataset) - val_size\n",
    "    \n",
    "    # Use a fixed seed for reproducibility\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "    \n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, \n",
    "        [train_size, val_size],\n",
    "        generator=generator\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders with memory pinning\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    \n",
    "    print(f\"\\nDataloader details:\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Training batches: {len(train_loader)}\")\n",
    "    print(f\"Validation batches: {len(val_loader)}\")\n",
    "    \n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "Loads hyperparameters and model from checkpoint, gives easy function to encode a structure into latent codes, as well as decode those latents back into the reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FQGAN Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from log_utils import log, load_stats, load_model\n",
    "import copy\n",
    "from fq_models import FQModel, HparamsFQGAN\n",
    "\n",
    "\n",
    "# Loads hparams from hparams.json file in saved model directory\n",
    "def load_hparams_from_json(log_dir):\n",
    "    import json\n",
    "    import os\n",
    "    json_path = os.path.join(log_dir, 'hparams.json')\n",
    "    \n",
    "    if not os.path.exists(json_path):\n",
    "        raise FileNotFoundError(f\"No hparams.json file found in {log_dir}\")\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        hparams = json.load(f)\n",
    "\n",
    "    return hparams\n",
    "\n",
    "# turns loaded hparams json into propery hyperparams object\n",
    "def dict_to_vcqgan_hparams(hparams_dict, dataset=None):\n",
    "    # Determine which hyperparameter class to use based on the dataset\n",
    "    if dataset == None:\n",
    "        dataset = hparams_dict.get('dataset', 'MNIST')  # Default to MNIST if not specified\n",
    "    \n",
    "    vq_hyper = HparamsFQGAN(dataset)\n",
    "    # Set attributes from the dictionary\n",
    "    for key, value in hparams_dict.items():\n",
    "        setattr(vq_hyper, key, value)\n",
    "    \n",
    "    return vq_hyper\n",
    "\n",
    "# Loads fqgan model weights from a given checkpoint file\n",
    "def load_fqgan_from_checkpoint(H, fqgan):\n",
    "    fqgan = load_model(fqgan, \"fqgan\", H.load_step, H.load_dir).cuda()\n",
    "    fqgan.eval()\n",
    "    return fqgan\n",
    "\n",
    "# Takes a chunk or batch of chunks from the dataset, returns the encoded style and structure indices matrices\n",
    "def encode_and_quantize(fqgan, terrain_chunks, device='cuda'):\n",
    "    \"\"\"Memory-efficient encoding function\"\"\"\n",
    "    fqgan.eval()\n",
    "    with torch.no_grad():\n",
    "        # Move input to device\n",
    "        terrain_chunks = terrain_chunks.to(device)\n",
    "        \n",
    "        # Get encodings\n",
    "        h_style, h_struct = fqgan.encoder(terrain_chunks)\n",
    "        \n",
    "        # Process style path\n",
    "        h_style = fqgan.quant_conv_style(h_style)\n",
    "        quant_style, _, style_stats = fqgan.quantize_style(h_style)\n",
    "        style_indices = style_stats[2]  # Get indices from tuple\n",
    "        style_indices = style_indices.view(\n",
    "            (h_style.size()[0], h_style.size()[2], h_style.size()[3], h_style.size()[4])\n",
    "        )\n",
    "        \n",
    "        # Clear intermediate tensors\n",
    "        del h_style, quant_style, style_stats\n",
    "        \n",
    "        # Process structure path\n",
    "        h_struct = fqgan.quant_conv_struct(h_struct)\n",
    "        quant_struct, _, struct_stats = fqgan.quantize_struct(h_struct)\n",
    "        struct_indices = struct_stats[2]  # Get indices from tuple\n",
    "        struct_indices = struct_indices.view(\n",
    "            (h_struct.size()[0], h_struct.size()[2], h_struct.size()[3], h_struct.size()[4])\n",
    "        )\n",
    "        \n",
    "        # Clear intermediate tensors\n",
    "        del h_struct, quant_struct, struct_stats\n",
    "        \n",
    "        # Move indices to CPU to save GPU memory\n",
    "        style_indices = style_indices.cpu()\n",
    "        struct_indices = struct_indices.cpu()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return style_indices, struct_indices\n",
    "\n",
    "# Takes style and structure indices, returns the reconstructed map\n",
    "def decode_from_indices(style_indices, struct_indices, fqgan, device='cuda', two_stage=False):\n",
    "    \"\"\"Memory-efficient decoding function\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Move indices to device only when needed\n",
    "        style_indices = style_indices.to(device)\n",
    "        struct_indices = struct_indices.to(device)\n",
    "        \n",
    "        # Get quantized vectors\n",
    "        quant_style = fqgan.quantize_style.get_codebook_entry(\n",
    "            style_indices.view(-1),\n",
    "            shape=[1, fqgan.embed_dim, *style_indices.shape[1:]]\n",
    "        )\n",
    "        quant_struct = fqgan.quantize_struct.get_codebook_entry(\n",
    "            struct_indices.view(-1),\n",
    "            shape=[1, fqgan.embed_dim, *struct_indices.shape[1:]]\n",
    "        )\n",
    "        \n",
    "        # Clear indices from GPU\n",
    "        del style_indices, struct_indices\n",
    "        \n",
    "        # Combine and decode\n",
    "        quant = torch.cat([quant_struct, quant_style], dim=1)\n",
    "        # quant = quant_style + quant_struct\n",
    "        del quant_style, quant_struct\n",
    "        \n",
    "        if two_stage:\n",
    "            decoded, binary_decoded = fqgan.decoder(quant)\n",
    "        else:\n",
    "            decoded = fqgan.decoder(quant)\n",
    "        \n",
    "        del quant\n",
    "        \n",
    "        # Convert to block IDs if one-hot encoded\n",
    "        if decoded.shape[1] > 1:\n",
    "            decoded = torch.argmax(decoded, dim=1)\n",
    "        \n",
    "        # Move result to CPU and clear GPU memory\n",
    "        result = decoded.squeeze(0).cpu()\n",
    "        if two_stage:\n",
    "            binary_result = binary_decoded.squeeze(0).cpu()\n",
    "            del decoded\n",
    "            del binary_decoded\n",
    "            torch.cuda.empty_cache()\n",
    "            return result, binary_result\n",
    "        \n",
    "        del decoded\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load FQ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using padding mode: zeros\n",
      "Using EMA quantizer\n",
      "Using TwoStageGenerator\n",
      "NO biome supervision\n",
      "Disentangle Ratio:  0.5\n",
      "Loading fqgan_10000.th\n",
      "loaded from: FQGAN_2stagedecoder_nobiomemodel_EMA_newdata_logweighted3_40both\n"
     ]
    }
   ],
   "source": [
    "# Important if you reload the model, can run into memory issues. There's a memory leak somewhere I haven't been able to fix\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_path = \"../model_logs/FQGAN_2stagedecoder_nobiomemodel_EMA_newdata_logweighted3_40both\"\n",
    "load_step = 10000\n",
    "\n",
    "fqgan_hparams =  dict_to_vcqgan_hparams(load_hparams_from_json(f\"{model_path}\"), 'minecraft')\n",
    "fqgan_hparams.load_step = load_step\n",
    "fqgan_hparams.padding_mode = 'zeros'\n",
    "fqgan = FQModel(fqgan_hparams)\n",
    "fqgan = load_fqgan_from_checkpoint(fqgan_hparams, fqgan)\n",
    "print(f'loaded from: {fqgan_hparams.log_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed data...\n",
      "Loaded 11119 chunks of size torch.Size([42, 24, 24, 24])\n",
      "Number of unique block types: 42\n",
      "Unique blocks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "\n",
      "Dataloader details:\n",
      "Training samples: 10008\n",
      "Validation samples: 1111\n",
      "Batch size: 4\n",
      "Training batches: 2502\n",
      "Validation batches: 278\n"
     ]
    }
   ],
   "source": [
    "# loads a preprocesed dataset, which already has a mappings file created and everything one-hot encoded nicely. For more memory efficient, could try just loading the validation set file\n",
    "# data_path = '../../text2env/data/minecraft_biome_newworld_10k_processed_cleaned.pt'\n",
    "data_path = '../../text2env/data/24_newdataset_processed_cleaned3.pt'\n",
    "train_loader, val_loader = get_minecraft_dataloaders(\n",
    "    data_path,\n",
    "    batch_size=4,\n",
    "    num_workers=0, # Must be 0 if you're on windows, otherwise it errors\n",
    "    val_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mappings_path = '../../text2env/data/minecraft_biome_newworld_10k_mappings.pt'\n",
    "mappings_path = '../../text2env/data/24_newdataset_mappings3.pt'\n",
    "block_converter = BlockBiomeConverter.load_mappings(mappings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.4.3'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'vtk': 'https://cdn.jsdelivr.net/npm/vtk.js@30.1.0/vtk'}, 'shim': {'vtk': {'exports': 'vtk'}}});\n      require([\"vtk\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 1;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window.vtk !== undefined) && (!(window.vtk instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.4.5/dist/bundled/abstractvtkplot/vtk.js@30.1.0/vtk.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.holoviz.org/panel/1.4.5/dist/bundled/abstractvtkplot/vtk.js@30.1.0/vtk.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.3.min.js\", \"https://cdn.holoviz.org/panel/1.4.5/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n\ttry {\n          inline_js[i].call(root, root.Bokeh);\n\t} catch(e) {\n\t  if (!reloading) {\n\t    throw e;\n\t  }\n\t}\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        })\n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='e5961816-5b78-45e6-aa16-d965a0e20764'>\n",
       "  <div id=\"b1e65ca9-f75f-4937-98f0-396683e1cb62\" data-root-id=\"e5961816-5b78-45e6-aa16-d965a0e20764\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"cd9751ce-ae5a-4c82-9a00-91241a3ffa09\":{\"version\":\"3.4.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"e5961816-5b78-45e6-aa16-d965a0e20764\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"2e091db4-008e-4d5e-b3b2-6a4264fc86ae\",\"attributes\":{\"plot_id\":\"e5961816-5b78-45e6-aa16-d965a0e20764\",\"comm_id\":\"4f70b2fb70a9475fb76b482822802fce\",\"client_comm_id\":\"366c9a36bbae4ca2aafdc4064eb91c53\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":null}]}]}};\n",
       "  var render_items = [{\"docid\":\"cd9751ce-ae5a-4c82-9a00-91241a3ffa09\",\"roots\":{\"e5961816-5b78-45e6-aa16-d965a0e20764\":\"b1e65ca9-f75f-4937-98f0-396683e1cb62\"},\"root_ids\":[\"e5961816-5b78-45e6-aa16-d965a0e20764\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined && ( root.vtk !== undefined) && ( root.vtk !== undefined))\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "e5961816-5b78-45e6-aa16-d965a0e20764"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an instnance of our visualizer\n",
    "from visualization_utils import MinecraftVisualizerPyVista\n",
    "visualizer = MinecraftVisualizerPyVista()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM\n",
    "# from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "# from pytorch_grad_cam.utils.image import show_cam_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "sample = batch[0].unsqueeze(0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(sample, 'gradcam_emptywater.pt')\n",
    "sample = torch.load('gradcam_emptywater.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    style_indices, struct_indices = encode_and_quantize(fqgan, sample)\n",
    "reconstructed, binary_reconstructed = decode_from_indices(style_indices, struct_indices, fqgan, two_stage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18ec32ecf7248e48f0fd2a0436a07f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:53416/index.html?ui=P_0x2079de27fa0_19&reconnect=auto\" class=\"pyvi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "converted_orig = block_converter.convert_to_original_blocks(sample.squeeze())\n",
    "plotter = visualizer.visualize_chunk(converted_orig, interactive=True, show_axis=False)\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cec107b76cb4ddb96c701a2a3a9dc46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:53416/index.html?ui=P_0x2078da67820_20&reconnect=auto\" class=\"pyvi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "binary_reconstructed = (binary_reconstructed>0.5).float()\n",
    "binary_reconstructed = block_converter.convert_to_original_blocks(binary_reconstructed).squeeze()\n",
    "reconstructed = block_converter.convert_to_original_blocks(reconstructed)\n",
    "fig = visualizer.visualize_chunk(reconstructed, interactive=True, show_axis=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layers = [fqgan.encoder.conv_out_struct]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = visualizer.visualize_both_codes_with_blocks(\n",
    "    style_indices,\n",
    "    struct_indices,\n",
    "    reconstructed\n",
    ")\n",
    "plotter.show(interactive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  2,  3,  4,  5,  7,  8, 10, 16, 21, 22, 24, 25, 29, 31, 32, 33,\n",
       "        34, 35, 37, 38, 39], dtype=int64),\n",
       " array([ 4,  1, 27,  4,  1,  2,  4, 15, 32,  3, 14,  5, 13,  2,  1,  1,  1,\n",
       "        30,  7,  3, 27, 19], dtype=int64))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(struct_indices, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# o3 implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderWrapper(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Returns pre-quantisation latents z (B,C,D,H,W) so CAM can back-prop\n",
    "    through the encoder only.  Feel free to add EMA swap logic if needed.\n",
    "    \"\"\"\n",
    "    def __init__(self, fqgan):\n",
    "        super().__init__()\n",
    "        self.enc = fqgan.encoder  # adapt to your attr names\n",
    "    def forward(self, x):\n",
    "        z = self.enc(x)           # no quantisation here!\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeTarget:\n",
    "    def __init__(self, code_idx, pos, codebook):\n",
    "        self.k  = code_idx        # integer index in the codebook\n",
    "        self.pos = pos            # (d,h,w) tuple in latent grid\n",
    "        self.E = codebook         # nn.Embedding weight or tensor\n",
    "\n",
    "    def __call__(self, z_pre_q):\n",
    "        d,h,w = self.pos\n",
    "        zvec  = z_pre_q[0, :, d, h, w]      # (C,)\n",
    "        return -((zvec - self.E[self.k]) ** 2).sum()  # maximise similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam.grad_cam import GradCAM\n",
    "class GradCAM3D(GradCAM):\n",
    "    def get_cam_weights(self, input_tensor, target_layers, targets):\n",
    "        grads = self.activations_and_grads.gradients[-1]  # B,C,D,H,W\n",
    "        return grads.mean(dim=(2,3,4), keepdim=True)      # ⟨D,H,W⟩ avg\n",
    "    def get_loss(self, output, targets):\n",
    "        return sum([t(output) for t in targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_cam_weights() takes 4 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m struct_embedding \u001b[38;5;241m=\u001b[39m fqgan\u001b[38;5;241m.\u001b[39mquantize_style\u001b[38;5;241m.\u001b[39membedding\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m GradCAM3D(model\u001b[38;5;241m=\u001b[39mEncoderWrapper(fqgan), target_layers\u001b[38;5;241m=\u001b[39m[fqgan\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mconv_out_struct]) \u001b[38;5;28;01mas\u001b[39;00m cam:\n\u001b[1;32m----> 5\u001b[0m     saliency \u001b[38;5;241m=\u001b[39m \u001b[43mcam\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mCodeTarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstruct_embedding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# saliency is (D,H,W) – align to original 24³ with interpolate\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     saliency \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(saliency[\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;28;01mNone\u001b[39;00m], size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m24\u001b[39m,\u001b[38;5;241m24\u001b[39m,\u001b[38;5;241m24\u001b[39m),\n\u001b[0;32m      9\u001b[0m                              mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\TimBits\\miniconda3\\envs\\py39\\lib\\site-packages\\pytorch_grad_cam\\base_cam.py:209\u001b[0m, in \u001b[0;36mBaseCAM.__call__\u001b[1;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aug_smooth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_augmentation_smoothing(input_tensor, targets, eigen_smooth)\n\u001b[1;32m--> 209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TimBits\\miniconda3\\envs\\py39\\lib\\site-packages\\pytorch_grad_cam\\base_cam.py:129\u001b[0m, in \u001b[0;36mBaseCAM.forward\u001b[1;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__htcore\u001b[38;5;241m.\u001b[39mmark_step()\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# In most of the saliency attribution papers, the saliency is\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# computed with a single target layer.\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Commonly it is the last convolutional layer.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# use all conv layers for example, all Batchnorm layers,\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# or something else.\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m cam_per_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_cam_per_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_multi_layers(cam_per_layer)\n",
      "File \u001b[1;32mc:\\Users\\TimBits\\miniconda3\\envs\\py39\\lib\\site-packages\\pytorch_grad_cam\\base_cam.py:164\u001b[0m, in \u001b[0;36mBaseCAM.compute_cam_per_layer\u001b[1;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(grads_list):\n\u001b[0;32m    162\u001b[0m     layer_grads \u001b[38;5;241m=\u001b[39m grads_list[i]\n\u001b[1;32m--> 164\u001b[0m cam \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cam_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_activations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m cam \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(cam, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    166\u001b[0m scaled \u001b[38;5;241m=\u001b[39m scale_cam_image(cam, target_size)\n",
      "File \u001b[1;32mc:\\Users\\TimBits\\miniconda3\\envs\\py39\\lib\\site-packages\\pytorch_grad_cam\\base_cam.py:75\u001b[0m, in \u001b[0;36mBaseCAM.get_cam_image\u001b[1;34m(self, input_tensor, target_layer, targets, activations, grads, eigen_smooth)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_cam_image\u001b[39m(\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     68\u001b[0m     input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m     eigen_smooth: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     74\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m---> 75\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cam_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(activations, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     77\u001b[0m         activations \u001b[38;5;241m=\u001b[39m activations\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[1;31mTypeError\u001b[0m: get_cam_weights() takes 4 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "k = 19\n",
    "pos = (0, 2, 0)\n",
    "struct_embedding = fqgan.quantize_style.embedding\n",
    "with GradCAM3D(model=EncoderWrapper(fqgan), target_layers=[fqgan.encoder.conv_out_struct]) as cam:\n",
    "    saliency = cam(input_tensor=sample, targets=[CodeTarget(k, pos, struct_embedding)])[0]\n",
    "    # saliency is (D,H,W) – align to original 24³ with interpolate\n",
    "    saliency = F.interpolate(saliency[None,None], size=(24,24,24),\n",
    "                             mode=\"trilinear\", align_corners=False)[0,0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# o3, recreate rl paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn.functional as F\n",
    "from pytorch_grad_cam import GradCAM\n",
    "# from pytorch_grad_cam.utils.model_targets import BaseTarget\n",
    "\n",
    "# ---- 0.  thin wrapper so GradCAM sees just the decoder ----\n",
    "class DecoderWrapper(torch.nn.Module):\n",
    "    def __init__(self, fqgan):                # your full model\n",
    "        super().__init__()\n",
    "        self.dec = fqgan.decoder              # reference to decoder\n",
    "    def forward(self, masked_codes):          # (B,C,D,H,W)\n",
    "        return self.dec(masked_codes)\n",
    "\n",
    "# ---- 1. custom Grad-CAM target identical to paper’s scalar ----\n",
    "class DecoderL2Target():\n",
    "    def __call__(self, decoder_feature):\n",
    "        # score = sum of squares over everything\n",
    "        return (decoder_feature**2).sum()\n",
    "\n",
    "# ---- 2. run one explanation for code k on one chunk ----\n",
    "def explain_struct_code(fqgan, chunk, k, position_indices=None):\n",
    "    fqgan.eval()\n",
    "    with torch.no_grad():\n",
    "        h_style, h_struct  = fqgan.encoder(chunk)          # (1,C,D,H,W)\n",
    "        h_struct = fqgan.quant_conv_struct(h_struct)\n",
    "        quant_struct, _, struct_stats = fqgan.quantize_struct(h_struct)\n",
    "        struct_indices = struct_stats[2]  # Get indices from tuple\n",
    "        struct_indices = struct_indices.view(\n",
    "            (h_struct.size()[0], h_struct.size()[2], h_struct.size()[3], h_struct.size()[4])\n",
    "        )\n",
    "\n",
    "        # Process style path\n",
    "        h_style = fqgan.quant_conv_style(h_style)\n",
    "        quant_style, _, style_stats = fqgan.quantize_style(h_style)\n",
    "        style_indices = style_stats[2]  # Get indices from tuple\n",
    "        style_indices = style_indices.view(\n",
    "            (h_style.size()[0], h_style.size()[2], h_style.size()[3], h_style.size()[4])\n",
    "        )\n",
    "        print(f'h_struct size: {h_struct.shape}')\n",
    "        print(f'quant_struct size: {quant_struct.shape}')\n",
    "        print(f'struct_indices size: {struct_indices.shape}')\n",
    "\n",
    "    # --- NEW: mask logic can target a single position or k-codes ---\n",
    "    if position_indices is not None:\n",
    "        # position_indices is a (d, h, w) tuple\n",
    "        d_idx, h_idx, w_idx = position_indices\n",
    "        mask = torch.zeros_like(struct_indices, dtype=torch.float32)\n",
    "        # assume batch dim first; here batch==1 typically\n",
    "        mask[:, d_idx, h_idx, w_idx] = 1.0\n",
    "    else:\n",
    "        mask = (struct_indices == k).float()\n",
    "    z_q_masked = quant_struct * mask.unsqueeze(1)           # keep dims (C,D,H,W)\n",
    "\n",
    "    quant = torch.cat([z_q_masked, quant_style], dim=1)\n",
    "\n",
    "    print(f'z_q_masked size: {z_q_masked.shape}')\n",
    "    print(f'z_q_masked[None] size: {z_q_masked.shape}')\n",
    "    print(f'quant shape: {quant.shape}')\n",
    "\n",
    "    # --- Grad-CAM on the decoder's *first* conv ---\n",
    "    cam = GradCAM(model=DecoderWrapper(fqgan),\n",
    "                  target_layers=[fqgan.decoder.struct_conv_in])\n",
    "    heat = cam(quant, targets=[DecoderL2Target()])[0]  # (d',h',w')\n",
    "\n",
    "    print(f'heat shape: {torch.from_numpy(heat[None, None]).shape}')\n",
    "    # print(f'heat type: {heat}')\n",
    "    heat = F.interpolate(torch.from_numpy(heat[None, None]), size=chunk.shape[-3:],\n",
    "                         mode=\"trilinear\", align_corners=False)[0,0]\n",
    "    return heat, mask    # heat-map + where code k actually occurred\n",
    "\n",
    "# ---- 2. run one explanation for code k on one chunk ----\n",
    "def explain_style_code(fqgan, chunk, k, position_indices=None):\n",
    "    fqgan.eval()\n",
    "    with torch.no_grad():\n",
    "        h_style, h_struct  = fqgan.encoder(chunk)          # (1,C,D,H,W)\n",
    "        h_struct = fqgan.quant_conv_struct(h_struct)\n",
    "        quant_struct, _, struct_stats = fqgan.quantize_struct(h_struct)\n",
    "        struct_indices = struct_stats[2]  # Get indices from tuple\n",
    "        struct_indices = struct_indices.view(\n",
    "            (h_struct.size()[0], h_struct.size()[2], h_struct.size()[3], h_struct.size()[4])\n",
    "        )\n",
    "\n",
    "        # Process style path\n",
    "        h_style = fqgan.quant_conv_style(h_style)\n",
    "        quant_style, _, style_stats = fqgan.quantize_style(h_style)\n",
    "        style_indices = style_stats[2]  # Get indices from tuple\n",
    "        style_indices = style_indices.view(\n",
    "            (h_style.size()[0], h_style.size()[2], h_style.size()[3], h_style.size()[4])\n",
    "        )\n",
    "        print(f'h_style size: {h_style.shape}')\n",
    "        print(f'quant_style size: {quant_style.shape}')\n",
    "        print(f'struct_indices size: {style_indices.shape}')\n",
    "\n",
    "    # --- NEW: mask logic can target a single position or k-codes ---\n",
    "    if position_indices is not None:\n",
    "        d_idx, h_idx, w_idx = position_indices\n",
    "        mask = torch.zeros_like(style_indices, dtype=torch.float32)\n",
    "        mask[:, d_idx, h_idx, w_idx] = 1.0\n",
    "    else:\n",
    "        mask = (style_indices == k).float()\n",
    "    z_q_masked = quant_style * mask.unsqueeze(1)           # keep dims (C,D,H,W)\n",
    "\n",
    "    quant = torch.cat([quant_struct, z_q_masked], dim=1)\n",
    "\n",
    "    print(f'z_q_masked size: {z_q_masked.shape}')\n",
    "    print(f'z_q_masked[None] size: {z_q_masked.shape}')\n",
    "    print(f'quant shape: {quant.shape}')\n",
    "\n",
    "    # --- Grad-CAM on the decoder's *first* conv ---\n",
    "    cam = GradCAM(model=DecoderWrapper(fqgan),\n",
    "                  target_layers=[fqgan.decoder.initial_conv])\n",
    "    heat = cam(quant, targets=[DecoderL2Target()])[0]  # (d',h',w')\n",
    "\n",
    "    print(f'heat shape: {torch.from_numpy(heat[None, None]).shape}')\n",
    "    # print(f'heat type: {heat}')\n",
    "    heat = F.interpolate(torch.from_numpy(heat[None, None]), size=chunk.shape[-3:],\n",
    "                         mode=\"trilinear\", align_corners=False)[0,0]\n",
    "    return heat, mask    # heat-map + where code k actually occurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  2,  3,  7, 10, 14, 19, 22, 23, 29, 32, 34, 35, 37, 38, 39],\n",
       "       dtype=int64),\n",
       " array([ 5,  3, 30, 16, 18,  1,  4, 32,  5, 10, 13, 28,  6, 19, 24,  2],\n",
       "       dtype=int64))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(struct_indices, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  7,  8,  9, 10, 12, 13, 14, 16, 17, 18, 19,\n",
       "        20, 21, 23, 24, 25, 26, 27, 29, 30, 31, 32, 34, 36, 37, 38, 39],\n",
       "       dtype=int64),\n",
       " array([ 1,  9,  1, 16,  2,  9, 21,  2, 13,  8,  2,  4,  5,  1,  5,  1, 13,\n",
       "         3,  4,  7,  6,  9,  9, 13,  1,  5, 24,  5,  6,  4,  4,  2,  1],\n",
       "       dtype=int64))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(style_indices, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_struct size: torch.Size([1, 32, 6, 6, 6])\n",
      "quant_struct size: torch.Size([1, 32, 6, 6, 6])\n",
      "struct_indices size: torch.Size([1, 6, 6, 6])\n",
      "z_q_masked size: torch.Size([1, 32, 6, 6, 6])\n",
      "z_q_masked[None] size: torch.Size([1, 32, 6, 6, 6])\n",
      "quant shape: torch.Size([1, 64, 6, 6, 6])\n",
      "heat shape: torch.Size([1, 1, 6, 6, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.], dtype=float32)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heat, mask = explain_struct_code(fqgan, sample, 33)\n",
    "np.unique(heat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_style size: torch.Size([1, 32, 6, 6, 6])\n",
      "quant_style size: torch.Size([1, 32, 6, 6, 6])\n",
      "struct_indices size: torch.Size([1, 6, 6, 6])\n",
      "z_q_masked size: torch.Size([1, 32, 6, 6, 6])\n",
      "z_q_masked[None] size: torch.Size([1, 32, 6, 6, 6])\n",
      "quant shape: torch.Size([1, 64, 6, 6, 6])\n",
      "heat shape: torch.Size([1, 1, 6, 6, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.0000000e+00, 1.9840769e-28, 5.9522309e-28, ..., 9.2149466e-01,\n",
       "       9.3437725e-01, 9.4256920e-01], dtype=float32)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_heat, _ = explain_style_code(fqgan, sample, 18)\n",
    "np.unique(style_heat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "def visualize_heatmap(heatmap, plotter=None, interactive=False, show_axis=True):\n",
    "    \"\"\"\n",
    "    Visualize a 3D heatmap in a 24×24×24 grid. \n",
    "    Every “voxel” is drawn black, with its opacity equal to the heatmap value (0→transparent, 1→opaque).\n",
    "    \"\"\"\n",
    "    # Convert to numpy if needed\n",
    "    if isinstance(heatmap, torch.Tensor):\n",
    "        heatmap = heatmap.detach().cpu().numpy()\n",
    "    # If there's an extra leading channel dimension, drop it\n",
    "    if heatmap.ndim == 4 and heatmap.shape[0] == 1:\n",
    "        heatmap = heatmap[0]\n",
    "\n",
    "    # Re‐orient to match original plotting conventions\n",
    "    # (depth, height, width) → (x, y, z) ordering, then rotate\n",
    "    heatmap = heatmap.transpose(2, 0, 1)\n",
    "    heatmap = np.rot90(heatmap, 1, (0, 1))\n",
    "\n",
    "    # Build a uniform vtkImageData volume\n",
    "    grid = pv.ImageData()\n",
    "    grid.dimensions = np.array(heatmap.shape) + 1\n",
    "    grid.cell_data[\"values\"] = heatmap.flatten(order=\"F\")\n",
    "\n",
    "    # Create or reuse a PyVista plotter\n",
    "    if plotter is None:\n",
    "        plotter = pv.Plotter(notebook=True) if interactive else pv.Plotter(off_screen=True)\n",
    "\n",
    "    # Volume‐render: all voxels black, opacity from 0→1\n",
    "    plotter.add_volume(\n",
    "        grid,\n",
    "        # scalars=\"values\",\n",
    "        # color=\"black\",\n",
    "        opacity=\"linear\",  # linearly map min→0, max→1\n",
    "        shade=False        # disable additional shading\n",
    "    )\n",
    "\n",
    "    # Optionally show axes/bounds\n",
    "    if show_axis:\n",
    "        plotter.show_bounds(\n",
    "            grid='back',\n",
    "            location='back',\n",
    "            font_size=8,\n",
    "            bold=False,\n",
    "            font_family='arial',\n",
    "            use_2d=False,\n",
    "            bounds=[0, 24, 0, 24, 0, 24],\n",
    "            axes_ranges=[0, 24, 0, 24, 0, 24],\n",
    "            padding=0.0,\n",
    "            n_xlabels=2,\n",
    "            n_ylabels=2,\n",
    "            n_zlabels=2\n",
    "        )\n",
    "\n",
    "    # Standard camera setup\n",
    "    plotter.camera_position = 'iso'\n",
    "    plotter.camera.zoom(1)\n",
    "\n",
    "    return plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_struct size: torch.Size([1, 32, 6, 6, 6])\n",
      "quant_struct size: torch.Size([1, 32, 6, 6, 6])\n",
      "struct_indices size: torch.Size([1, 6, 6, 6])\n",
      "z_q_masked size: torch.Size([1, 32, 6, 6, 6])\n",
      "z_q_masked[None] size: torch.Size([1, 32, 6, 6, 6])\n",
      "quant shape: torch.Size([1, 64, 6, 6, 6])\n",
      "heat shape: torch.Size([1, 1, 6, 6, 6])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bdd4aaadb5c46649a610739eccaecd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:53416/index.html?ui=P_0x2080c241fd0_27&reconnect=auto\" class=\"pyvi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "heat, mask = explain_struct_code(fqgan, sample, 3)\n",
    "np.unique(heat)\n",
    "\n",
    "plotter = visualize_heatmap(heat, interactive=True)\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs = torch.where(style_indices == 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 1, 1, 1, 2, 2, 3, 4, 4, 4, 4, 5]),\n",
       " tensor([1, 1, 1, 2, 5, 1, 4, 2, 2, 2, 2, 3, 2]),\n",
       " tensor([4, 5, 5, 4, 1, 5, 2, 4, 3, 4, 5, 1, 4]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_indices[0, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_style size: torch.Size([1, 32, 6, 6, 6])\n",
      "quant_style size: torch.Size([1, 32, 6, 6, 6])\n",
      "struct_indices size: torch.Size([1, 6, 6, 6])\n",
      "z_q_masked size: torch.Size([1, 32, 6, 6, 6])\n",
      "z_q_masked[None] size: torch.Size([1, 32, 6, 6, 6])\n",
      "quant shape: torch.Size([1, 64, 6, 6, 6])\n",
      "heat shape: torch.Size([1, 1, 6, 6, 6])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df42aedf1884f9ea66790af05ad2321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:53416/index.html?ui=P_0x20798922be0_18&reconnect=auto\" class=\"pyvi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "style_heat, _ = explain_style_code(fqgan, sample, 27, (2, 1, 5))\n",
    "np.unique(style_heat)\n",
    "\n",
    "plotter = visualize_heatmap(style_heat, interactive=True)\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = visualizer.visualize_both_codes_with_blocks(\n",
    "    style_indices,\n",
    "    struct_indices,\n",
    "    reconstructed\n",
    ")\n",
    "plotter.show(interactive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get encodings\n",
    "        h_style, h_struct = fqgan.encoder(terrain_chunks)\n",
    "        \n",
    "        # Process style path\n",
    "        h_style = fqgan.quant_conv_style(h_style)\n",
    "        quant_style, _, style_stats = fqgan.quantize_style(h_style)\n",
    "        style_indices = style_stats[2]  # Get indices from tuple\n",
    "        style_indices = style_indices.view(\n",
    "            (h_style.size()[0], h_style.size()[2], h_style.size()[3], h_style.size()[4])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a chunk or batch of chunks from the dataset, returns the encoded style and structure indices matrices\n",
    "def encode_and_quantize(fqgan, terrain_chunks, device='cuda'):\n",
    "    \"\"\"Memory-efficient encoding function\"\"\"\n",
    "    fqgan.eval()\n",
    "    with torch.no_grad():\n",
    "        # Move input to device\n",
    "        terrain_chunks = terrain_chunks.to(device)\n",
    "        \n",
    "        # Get encodings\n",
    "        h_style, h_struct = fqgan.encoder(terrain_chunks)\n",
    "        \n",
    "        # Process style path\n",
    "        h_style = fqgan.quant_conv_style(h_style)\n",
    "        quant_style, _, style_stats = fqgan.quantize_style(h_style)\n",
    "        style_indices = style_stats[2]  # Get indices from tuple\n",
    "        style_indices = style_indices.view(\n",
    "            (h_style.size()[0], h_style.size()[2], h_style.size()[3], h_style.size()[4])\n",
    "        )\n",
    "        \n",
    "        # Clear intermediate tensors\n",
    "        del h_style, quant_style, style_stats\n",
    "        \n",
    "        # Process structure path\n",
    "        h_struct = fqgan.quant_conv_struct(h_struct)\n",
    "        quant_struct, _, struct_stats = fqgan.quantize_struct(h_struct)\n",
    "        struct_indices = struct_stats[2]  # Get indices from tuple\n",
    "        struct_indices = struct_indices.view(\n",
    "            (h_struct.size()[0], h_struct.size()[2], h_struct.size()[3], h_struct.size()[4])\n",
    "        )\n",
    "        \n",
    "        # Clear intermediate tensors\n",
    "        del h_struct, quant_struct, struct_stats\n",
    "        \n",
    "        # Move indices to CPU to save GPU memory\n",
    "        style_indices = style_indices.cpu()\n",
    "        struct_indices = struct_indices.cpu()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return style_indices, struct_indices\n",
    "\n",
    "# Takes style and structure indices, returns the reconstructed map\n",
    "def decode_from_indices(style_indices, struct_indices, fqgan, device='cuda', two_stage=False):\n",
    "    \"\"\"Memory-efficient decoding function\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Move indices to device only when needed\n",
    "        style_indices = style_indices.to(device)\n",
    "        struct_indices = struct_indices.to(device)\n",
    "        \n",
    "        # Get quantized vectors\n",
    "        quant_style = fqgan.quantize_style.get_codebook_entry(\n",
    "            style_indices.view(-1),\n",
    "            shape=[1, fqgan.embed_dim, *style_indices.shape[1:]]\n",
    "        )\n",
    "        quant_struct = fqgan.quantize_struct.get_codebook_entry(\n",
    "            struct_indices.view(-1),\n",
    "            shape=[1, fqgan.embed_dim, *struct_indices.shape[1:]]\n",
    "        )\n",
    "        \n",
    "        # Clear indices from GPU\n",
    "        del style_indices, struct_indices\n",
    "        \n",
    "        # Combine and decode\n",
    "        quant = torch.cat([quant_struct, quant_style], dim=1)\n",
    "        # quant = quant_style + quant_struct\n",
    "        del quant_style, quant_struct\n",
    "        \n",
    "        if two_stage:\n",
    "            decoded, binary_decoded = fqgan.decoder(quant)\n",
    "        else:\n",
    "            decoded = fqgan.decoder(quant)\n",
    "        \n",
    "        del quant\n",
    "        \n",
    "        # Convert to block IDs if one-hot encoded\n",
    "        if decoded.shape[1] > 1:\n",
    "            decoded = torch.argmax(decoded, dim=1)\n",
    "        \n",
    "        # Move result to CPU and clear GPU memory\n",
    "        result = decoded.squeeze(0).cpu()\n",
    "        if two_stage:\n",
    "            binary_result = binary_decoded.squeeze(0).cpu()\n",
    "            del decoded\n",
    "            del binary_decoded\n",
    "            torch.cuda.empty_cache()\n",
    "            return result, binary_result\n",
    "        \n",
    "        del decoded\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## docs ass example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(pretrained=True)\n",
    "target_layers = [model.layer4[-1]]\n",
    "input_tensor = # Create an input tensor image for your model..\n",
    "# Note: input_tensor can be a batch tensor with several images!\n",
    "\n",
    "# We have to specify the target we want to generate the CAM for.\n",
    "targets = [ClassifierOutputTarget(281)]\n",
    "\n",
    "# Construct the CAM object once, and then re-use it on many images.\n",
    "with GradCAM(model=model, target_layers=target_layers) as cam:\n",
    "  # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
    "  grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "  # In this example grayscale_cam has only one image in the batch:\n",
    "  grayscale_cam = grayscale_cam[0, :]\n",
    "  visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "  # You can also get the model outputs without having to redo inference\n",
    "  model_outputs = cam.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chatgpt ass response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_volume = ...  # shape (C, 24, 24, 24), one-hot or multi-channel\n",
    "# Hook to capture the last conv layer output and gradient\n",
    "feature_maps = None\n",
    "grads = None\n",
    "def forward_hook(module, inp, out):\n",
    "    nonlocal feature_maps\n",
    "    feature_maps = out.detach()\n",
    "def backward_hook(module, grad_in, grad_out):\n",
    "    # grad_out is a tuple (because output could be tuple), take first if needed\n",
    "    nonlocal grads\n",
    "    grads = grad_out[0].detach()\n",
    "\n",
    "layer = model.encoder.last_conv  # for example\n",
    "handle_f = layer.register_forward_hook(forward_hook)\n",
    "handle_b = layer.register_backward_hook(backward_hook)\n",
    "\n",
    "# Forward pass through encoder & quantizer\n",
    "z = model.encoder(input_volume.unsqueeze(0))        # shape (1, latent_C, D_lat, H_lat, W_lat)\n",
    "quantized, code_indices = model.quantize(z)         # quantized is shape (1, latent_dim, D_lat, H_lat, W_lat)\n",
    "# Suppose we want to analyze code at position (d0,h0,w0) in the latent\n",
    "d0,h0,w0 =  ...  # indices of the latent position of interest\n",
    "code_k = code_indices[0, d0, h0, w0].item()         # the index of the codebook selected\n",
    "\n",
    "# Define target score = negative distance between z and its selected code vector\n",
    "z_vec = z[0,:, d0, h0, w0]                          # the pre-quantization vector at that position\n",
    "codebook_vec = model.codebook.embedding[code_k]     # the corresponding codebook embedding vector\n",
    "score = -torch.norm(z_vec - codebook_vec, p=2)**2\n",
    "\n",
    "# Backpropagate to get gradients at last conv layer\n",
    "model.zero_grad()\n",
    "score.backward()\n",
    "\n",
    "# Compute weights: average grad over spatial dims for each channel\n",
    "# grads shape: (1, channels, D_lat_feat, H_lat_feat, W_lat_feat)\n",
    "weights = grads.mean(dim=[2,3,4], keepdim=True)  # average gradient over depth, height, width\n",
    "\n",
    "# Weight the captured feature maps\n",
    "# feature_maps shape: (1, channels, D_lat_feat, H_lat_feat, W_lat_feat)\n",
    "cam = (weights * feature_maps).sum(dim=1)         # sum over channels -> shape (1, D_lat_feat, H_lat_feat, W_lat_feat)\n",
    "cam = torch.relu(cam)                             # apply ReLU\n",
    "cam = torch.nn.functional.interpolate(cam, size=(24,24,24), mode='trilinear', align_corners=False)\n",
    "saliency_volume = cam.squeeze(0).cpu().numpy()    # now a 24x24x24 numpy array of importance values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
