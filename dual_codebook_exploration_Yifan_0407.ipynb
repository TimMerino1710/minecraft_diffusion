{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from sampler_utils import retrieve_autoencoder_components_state_dicts, latent_ids_to_onehot3d, get_latent_loaders\n",
    "from models3d import VQAutoEncoder, Generator\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, fixed\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from PIL import Image\n",
    "import torch.distributions as dists\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from models3d import BiomeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "#matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockBiomeConverter:\n",
    "    def __init__(self, block_mappings=None, biome_mappings=None):\n",
    "        \"\"\"\n",
    "        Initialize with pre-computed mappings for both blocks and biomes\n",
    "        \n",
    "        Args:\n",
    "            block_mappings: dict containing 'index_to_block' and 'block_to_index'\n",
    "            biome_mappings: dict containing 'index_to_biome' and 'biome_to_index'\n",
    "        \"\"\"\n",
    "        self.index_to_block = block_mappings['index_to_block'] if block_mappings else None\n",
    "        self.block_to_index = block_mappings['block_to_index'] if block_mappings else None\n",
    "        self.index_to_biome = biome_mappings['index_to_biome'] if biome_mappings else None\n",
    "        self.biome_to_index = biome_mappings['biome_to_index'] if biome_mappings else None\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataset(cls, data_path):\n",
    "        \"\"\"Create mappings from a dataset file\"\"\"\n",
    "        data = np.load(data_path, allow_pickle=True)\n",
    "        voxels = data['voxels']\n",
    "        biomes = data['biomes']\n",
    "        \n",
    "        # Create block mappings (blocks are integers)\n",
    "        unique_blocks = np.unique(voxels)\n",
    "        block_to_index = {int(block): idx for idx, block in enumerate(unique_blocks)}\n",
    "        index_to_block = {idx: int(block) for idx, block in enumerate(unique_blocks)}\n",
    "        \n",
    "        # Create biome mappings (biomes are strings)\n",
    "        unique_biomes = np.unique(biomes)\n",
    "        biome_to_index = {str(biome): idx for idx, biome in enumerate(unique_biomes)}\n",
    "        index_to_biome = {idx: str(biome) for idx, biome in enumerate(unique_biomes)}\n",
    "        \n",
    "        block_mappings = {'index_to_block': index_to_block, 'block_to_index': block_to_index}\n",
    "        biome_mappings = {'index_to_biome': index_to_biome, 'biome_to_index': biome_to_index}\n",
    "        \n",
    "        return cls(block_mappings, biome_mappings)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_arrays(cls, voxels, biomes):\n",
    "        \"\"\"Create mappings directly from numpy arrays\"\"\"\n",
    "        # Create block mappings (blocks are integers)\n",
    "        unique_blocks = np.unique(voxels)\n",
    "        block_to_index = {int(block): idx for idx, block in enumerate(unique_blocks)}\n",
    "        index_to_block = {idx: int(block) for idx, block in enumerate(unique_blocks)}\n",
    "        \n",
    "        # Create biome mappings (biomes are strings)\n",
    "        unique_biomes = np.unique(biomes)\n",
    "        biome_to_index = {str(biome): idx for idx, biome in enumerate(unique_biomes)}\n",
    "        index_to_biome = {idx: str(biome) for idx, biome in enumerate(unique_biomes)}\n",
    "        \n",
    "        block_mappings = {'index_to_block': index_to_block, 'block_to_index': block_to_index}\n",
    "        biome_mappings = {'index_to_biome': index_to_biome, 'biome_to_index': biome_to_index}\n",
    "        \n",
    "        return cls(block_mappings, biome_mappings)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_mappings(cls, path):\n",
    "        \"\"\"Load pre-saved mappings\"\"\"\n",
    "        mappings = torch.load(path)\n",
    "        return cls(mappings['block_mappings'], mappings['biome_mappings'])\n",
    "    \n",
    "    def save_mappings(self, path):\n",
    "        \"\"\"Save mappings for later use\"\"\"\n",
    "        torch.save({\n",
    "            'block_mappings': {\n",
    "                'index_to_block': self.index_to_block,\n",
    "                'block_to_index': self.block_to_index\n",
    "            },\n",
    "            'biome_mappings': {\n",
    "                'index_to_biome': self.index_to_biome,\n",
    "                'biome_to_index': self.biome_to_index\n",
    "            }\n",
    "        }, path)\n",
    "    \n",
    "    def convert_to_original_blocks(self, data):\n",
    "        \"\"\"\n",
    "        Convert from indices back to original block IDs.\n",
    "        Handles both one-hot encoded and already-indexed data.\n",
    "        \n",
    "        Args:\n",
    "            data: torch.Tensor of either:\n",
    "                - one-hot encoded blocks [B, C, H, W, D] or [C, H, W, D]\n",
    "                - indexed blocks [B, H, W, D] or [H, W, D]\n",
    "        Returns:\n",
    "            torch.Tensor of original block IDs with shape [B, H, W, D] or [H, W, D]\n",
    "        \"\"\"\n",
    "        # If one-hot encoded (dim == 5 or first dim == num_blocks), convert to indices first\n",
    "        if len(data.shape) == 5 or (len(data.shape) == 4 and data.shape[0] == len(self.block_to_index)):\n",
    "            data = torch.argmax(data, dim=1 if len(data.shape) == 5 else 0)\n",
    "        \n",
    "        # Now convert indices to original blocks\n",
    "        if len(data.shape) == 4:  # Batch dimension present\n",
    "            return torch.tensor([[[[self.index_to_block[int(b)] \n",
    "                                for b in row]\n",
    "                                for row in layer]\n",
    "                                for layer in slice_]\n",
    "                                for slice_ in data])\n",
    "        else:  # No batch dimension\n",
    "            return torch.tensor([[[self.index_to_block[int(b)] \n",
    "                                for b in row]\n",
    "                                for row in layer]\n",
    "                                for layer in data])\n",
    "\n",
    "    def convert_to_original_biomes(self, data):\n",
    "        \"\"\"\n",
    "        Convert from indices back to original biome strings.\n",
    "        Handles both one-hot encoded and already-indexed data.\n",
    "        \n",
    "        Args:\n",
    "            data: torch.Tensor of either:\n",
    "                - one-hot encoded biomes [B, C, H, W, D] or [C, H, W, D]\n",
    "                - indexed biomes [B, H, W, D] or [H, W, D]\n",
    "        Returns:\n",
    "            numpy array of original biome strings with shape [B, H, W, D] or [H, W, D]\n",
    "        \"\"\"\n",
    "        # If one-hot encoded (dim == 5 or first dim == num_biomes), convert to indices first\n",
    "        if len(data.shape) == 5 or (len(data.shape) == 4 and data.shape[0] == len(self.biome_to_index)):\n",
    "            data = torch.argmax(data, dim=1 if len(data.shape) == 5 else 0)\n",
    "        \n",
    "        # Now convert indices to original biomes\n",
    "        if len(data.shape) == 4:  # Batch dimension present\n",
    "            return np.array([[[[self.index_to_biome[int(b)] \n",
    "                            for b in row]\n",
    "                            for row in layer]\n",
    "                            for layer in slice_]\n",
    "                            for slice_ in data])\n",
    "        else:  # No batch dimension\n",
    "            return np.array([[[self.index_to_biome[int(b)] \n",
    "                            for b in row]\n",
    "                            for row in layer]\n",
    "                            for layer in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minecraft Chunks Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class MinecraftDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        data_path = Path(data_path)\n",
    "\n",
    "        # Try to load processed data first\n",
    "        # assert processed_data_path.exists() and mappings_path.exists()\n",
    "\n",
    "        print(\"Loading pre-processed data...\")\n",
    "        processed_data = torch.load(data_path)\n",
    "        # Only keep the chunks, discard biome data\n",
    "        self.processed_chunks = processed_data['chunks']\n",
    "        # self.processed_biomes = processed_data['biomes']\n",
    "        # Delete the biomes to free memory\n",
    "        # del processed_data['biomes']\n",
    "        # del processed_data['chunks']\n",
    "        del processed_data\n",
    "        \n",
    "        \n",
    "        print(f\"Loaded {len(self.processed_chunks)} chunks of size {self.processed_chunks.shape[1:]}\")\n",
    "        print(f\"Number of unique block types: {self.processed_chunks.shape[1]}\")\n",
    "        print(f'Unique blocks: {torch.unique(torch.argmax(self.processed_chunks, dim=1)).tolist()}')\n",
    "        #print(f\"Loaded {len(self.processed_biomes)} chunks of size {self.processed_biomes.shape[1:]}\")\n",
    "        #print(f\"Number of unique biome types: {self.processed_biomes.shape[1]}\")\n",
    "        #print(f'Unique biomes: {torch.unique(torch.argmax(self.processed_biomes, dim=1)).tolist()}')\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return self.processed_chunks[idx]\n",
    "        return self.processed_chunks[idx] #, self.processed_biomes[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        # return len(self.processed_chunks), len(self.processed_biomes)\n",
    "        return len(self.processed_chunks)\n",
    "\n",
    "def get_minecraft_dataloaders(data_path, batch_size=32, val_split=0.1, num_workers=4):\n",
    "    \"\"\"\n",
    "    Creates training and validation dataloaders for Minecraft chunks.\n",
    "    \"\"\"\n",
    "    # Create dataset\n",
    "    dataset = MinecraftDataset(data_path)\n",
    "    \n",
    "    # Split into train and validation sets\n",
    "    val_size = int(val_split * len(dataset))\n",
    "    train_size = len(dataset) - val_size\n",
    "    \n",
    "    # Use a fixed seed for reproducibility\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "    \n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, \n",
    "        [train_size, val_size],\n",
    "        generator=generator\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders with memory pinning\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    \n",
    "    print(f\"\\nDataloader details:\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Training batches: {len(train_loader)}\")\n",
    "    print(f\"Validation batches: {len(val_loader)}\")\n",
    "    \n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "Loads hyperparameters and model from checkpoint, gives easy function to encode a structure into latent codes, as well as decode those latents back into the reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VQGAN Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from log_utils import log, load_stats, load_model\n",
    "import copy\n",
    "from hyperparams import HparamsVQGAN\n",
    "\n",
    "# Loads hparams from hparams.json file in saved model directory\n",
    "def vq_load_hparams_from_json(log_dir):\n",
    "    import json\n",
    "    import os\n",
    "    json_path = os.path.join(log_dir, 'hparams.json')\n",
    "    \n",
    "    if not os.path.exists(json_path):\n",
    "        raise FileNotFoundError(f\"No hparams.json file found in {log_dir}\")\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        hparams = json.load(f)\n",
    "\n",
    "    return hparams\n",
    "\n",
    "# turns loaded hparams json into propery hyperparams object\n",
    "def vq_dict_to_vcqgan_hparams(hparams_dict, dataset=None):\n",
    "    # Determine which hyperparameter class to use based on the dataset\n",
    "    if dataset == None:\n",
    "        dataset = hparams_dict.get('dataset', 'MNIST')  # Default to MNIST if not specified\n",
    "    \n",
    "    vq_hyper = HparamsVQGAN(dataset)\n",
    "    # Set attributes from the dictionary\n",
    "    for key, value in hparams_dict.items():\n",
    "        setattr(vq_hyper, key, value)\n",
    "    \n",
    "    return vq_hyper\n",
    "\n",
    "def load_model_vq(model, model_load_name, step, log_dir, strict=False):\n",
    "    checkpoint_path = os.path.join(\"../model_logs\", log_dir, \"saved_models\", f\"{model_load_name}_{step}.th\")\n",
    "    print(f\"\\nAttempting to load checkpoint from: {checkpoint_path}\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found at: {checkpoint_path}\")\n",
    "        \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "    # Create new state dict with modified keys\n",
    "    new_state_dict = {}\n",
    "    for key, value in checkpoint.items():\n",
    "        if key.startswith('ae.'):\n",
    "            # Remove the 'ae.' prefix\n",
    "            new_key = key[3:]  # Skip 'ae.'\n",
    "            new_state_dict[new_key] = value\n",
    "    \n",
    "    # Try loading state dict\n",
    "    try:\n",
    "        model.load_state_dict(new_state_dict, strict=strict)\n",
    "        \n",
    "        print(\"\\nLoad successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError loading state dict: {e}\")\n",
    "        \n",
    "    return model\n",
    "def vq_load_vqgan_from_checkpoint(H, vqgan):\n",
    "    vqgan = load_model_vq(vqgan, \"vqgan\", H.load_step, H.load_dir).cuda()\n",
    "    vqgan.eval()\n",
    "    return vqgan\n",
    "\n",
    "\n",
    "def vq_encode_and_quantize(vqgan, terrain_chunks, device='cuda'):\n",
    "    \"\"\"Memory-efficient encoding function for single codebook VQGAN\"\"\"\n",
    "    vqgan.eval()\n",
    "    with torch.no_grad():\n",
    "        # Move input to device and ensure it's one-hot encoded\n",
    "        terrain_chunks = terrain_chunks.to(device)\n",
    "        \n",
    "        # Get encodings through encoder\n",
    "        latents = vqgan.encoder(terrain_chunks)\n",
    "        \n",
    "        # Get quantized indices from VQ layer\n",
    "        _, _, quant_stats = vqgan.quantize(latents)\n",
    "        indices = quant_stats[\"min_encoding_indices\"]\n",
    "        \n",
    "        # Move to CPU and clear GPU memory\n",
    "        indices = indices.cpu()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return indices\n",
    "\n",
    "def vq_decode_from_indices(indices, vqgan, device='cuda'):\n",
    "    \"\"\"Reconstructs from latent indices\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Move indices to device\n",
    "        indices = indices.to(device)\n",
    "        \n",
    "        # Get original shape for reshaping\n",
    "        batch_size = indices.size(0)\n",
    "        \n",
    "        # Convert indices to quantized embeddings using the VQ layer's get_codebook_entry\n",
    "        shape = (batch_size, 6, 6, 6, vqgan.embed_dim)  # Shape for 3D data\n",
    "        quant = vqgan.quantize.get_codebook_entry(indices.reshape(-1), shape)\n",
    "        \n",
    "        # Generate output through decoder\n",
    "        decoded = vqgan.generator(quant)\n",
    "        \n",
    "        # Convert to block IDs if one-hot encoded\n",
    "        if decoded.shape[1] > 1:\n",
    "            decoded = torch.argmax(decoded, dim=1)\n",
    "        \n",
    "        # Move result to CPU and clear GPU memory\n",
    "        decoded = decoded.squeeze(0).cpu()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FQGAN helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from log_utils import log, load_stats, load_model\n",
    "import copy\n",
    "from fq_models import FQModel, HparamsFQGAN\n",
    "\n",
    "\n",
    "# Loads hparams from hparams.json file in saved model directory\n",
    "def load_hparams_from_json(log_dir):\n",
    "    import json\n",
    "    import os\n",
    "    json_path = os.path.join(log_dir, 'hparams.json')\n",
    "    \n",
    "    if not os.path.exists(json_path):\n",
    "        raise FileNotFoundError(f\"No hparams.json file found in {log_dir}\")\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        hparams = json.load(f)\n",
    "\n",
    "    return hparams\n",
    "\n",
    "# turns loaded hparams json into propery hyperparams object\n",
    "def dict_to_vcqgan_hparams(hparams_dict, dataset=None):\n",
    "    # Determine which hyperparameter class to use based on the dataset\n",
    "    if dataset == None:\n",
    "        dataset = hparams_dict.get('dataset', 'MNIST')  # Default to MNIST if not specified\n",
    "    \n",
    "    vq_hyper = HparamsFQGAN(dataset)\n",
    "    # Set attributes from the dictionary\n",
    "    for key, value in hparams_dict.items():\n",
    "        setattr(vq_hyper, key, value)\n",
    "    \n",
    "    return vq_hyper\n",
    "\n",
    "# Loads fqgan model weights from a given checkpoint file\n",
    "def load_fqgan_from_checkpoint(H, fqgan):\n",
    "    fqgan = load_model(fqgan, \"fqgan\", H.load_step, H.load_dir).cuda()\n",
    "    fqgan.eval()\n",
    "    return fqgan\n",
    "\n",
    "# Takes a chunk or batch of chunks from the dataset, returns the encoded style and structure indices matrices\n",
    "def encode_and_quantize(fqgan, terrain_chunks, device='cuda'):\n",
    "    \"\"\"Memory-efficient encoding function\"\"\"\n",
    "    fqgan.eval()\n",
    "    with torch.no_grad():\n",
    "        # Move input to device\n",
    "        terrain_chunks = terrain_chunks.to(device)\n",
    "        \n",
    "        # Get encodings\n",
    "        h_style, h_struct = fqgan.encoder(terrain_chunks)\n",
    "        \n",
    "        # Process style path\n",
    "        h_style = fqgan.quant_conv_style(h_style)\n",
    "        quant_style, _, style_stats = fqgan.quantize_style(h_style)\n",
    "        style_indices = style_stats[2]  # Get indices from tuple\n",
    "        style_indices = style_indices.view(\n",
    "            (h_style.size()[0], h_style.size()[2], h_style.size()[3], h_style.size()[4])\n",
    "        )\n",
    "        \n",
    "        # Clear intermediate tensors\n",
    "        del h_style, quant_style, style_stats\n",
    "        \n",
    "        # Process structure path\n",
    "        h_struct = fqgan.quant_conv_struct(h_struct)\n",
    "        quant_struct, _, struct_stats = fqgan.quantize_struct(h_struct)\n",
    "        struct_indices = struct_stats[2]  # Get indices from tuple\n",
    "        struct_indices = struct_indices.view(\n",
    "            (h_struct.size()[0], h_struct.size()[2], h_struct.size()[3], h_struct.size()[4])\n",
    "        )\n",
    "        \n",
    "        # Clear intermediate tensors\n",
    "        del h_struct, quant_struct, struct_stats\n",
    "        \n",
    "        # Move indices to CPU to save GPU memory\n",
    "        style_indices = style_indices.cpu()\n",
    "        struct_indices = struct_indices.cpu()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return style_indices, struct_indices\n",
    "\n",
    "# Takes style and structure indices, returns the reconstructed map\n",
    "def decode_from_indices(style_indices, struct_indices, fqgan, device='cuda', two_stage=False):\n",
    "    \"\"\"Memory-efficient decoding function\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Move indices to device only when needed\n",
    "        style_indices = style_indices.to(device)\n",
    "        struct_indices = struct_indices.to(device)\n",
    "        \n",
    "        # Get quantized vectors\n",
    "        quant_style = fqgan.quantize_style.get_codebook_entry(\n",
    "            style_indices.view(-1),\n",
    "            shape=[1, fqgan.embed_dim, *style_indices.shape[1:]]\n",
    "        )\n",
    "        quant_struct = fqgan.quantize_struct.get_codebook_entry(\n",
    "            struct_indices.view(-1),\n",
    "            shape=[1, fqgan.embed_dim, *struct_indices.shape[1:]]\n",
    "        )\n",
    "        \n",
    "        # Clear indices from GPU\n",
    "        del style_indices, struct_indices\n",
    "        \n",
    "        # Combine and decode\n",
    "        quant = torch.cat([quant_struct, quant_style], dim=1)\n",
    "        # quant = quant_style + quant_struct\n",
    "        del quant_style, quant_struct\n",
    "        \n",
    "        if two_stage:\n",
    "            decoded, binary_decoded = fqgan.decoder(quant)\n",
    "        else:\n",
    "            decoded = fqgan.decoder(quant)\n",
    "        \n",
    "        del quant\n",
    "        \n",
    "        # Convert to block IDs if one-hot encoded\n",
    "        if decoded.shape[1] > 1:\n",
    "            decoded = torch.argmax(decoded, dim=1)\n",
    "        \n",
    "        # Move result to CPU and clear GPU memory\n",
    "        result = decoded.squeeze(0).cpu()\n",
    "        if two_stage:\n",
    "            binary_result = binary_decoded.squeeze(0).cpu()\n",
    "            del decoded\n",
    "            del binary_decoded\n",
    "            torch.cuda.empty_cache()\n",
    "            return result, binary_result\n",
    "        \n",
    "        del decoded\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load FQ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important if you reload the model, can run into memory issues. There's a memory leak somewhere I haven't been able to fix\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with directory for whatever model you want\n",
    "# model_path = './FQGAN_2stagedecoder_nobiomemodel_16bothcbook_EMA3'\n",
    "model_path = \"./FQGAN_2stagedecoder_nobiomemodel_16bothcbook_EMA3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO biome supervision\n",
      "Disentangle Ratio:  0.5\n",
      "Loading fqgan_10000.th\n",
      "loaded from: FQGAN_2stagedecoder_nobiomemodel_16bothcbook_EMA3\n"
     ]
    }
   ],
   "source": [
    "# I'm manually setting the load step here, if it errors out take a look at what the actual .th file number is\n",
    "fqgan_hparams =  dict_to_vcqgan_hparams(load_hparams_from_json(f\"{model_path}\"), 'minecraft')\n",
    "fqgan_hparams.load_step = 10000\n",
    "fqgan = FQModel(fqgan_hparams)\n",
    "fqgan = load_fqgan_from_checkpoint(fqgan_hparams, fqgan)\n",
    "print(f'loaded from: {fqgan_hparams.log_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load VQ Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from models3d import VQAutoEncoder\n",
    "\n",
    "# # vqmodel_path = 'saved_models/minecraft39ch_ce_3'\n",
    "#vqgan_hparams =  vq_dict_to_vcqgan_hparams(vq_load_hparams_from_json(f\"../model_logs/minecraft39ch_ce_3_fqdataset\"), 'minecraft')\n",
    "#vqgan_hparams.load_step = 10000\n",
    "\n",
    "#vqgan = VQAutoEncoder(vqgan_hparams)\n",
    "#vqgan = vq_load_vqgan_from_checkpoint(vqgan_hparams, vqgan)\n",
    "#print(f'loaded from: {vqgan_hparams.log_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resolution: 24, num_resolutions: 3, num_res_blocks: 2, attn_resolutions: [6], in_channels: 256, out_channels: 39, block_in_ch: 256, curr_res: 6\n",
      "\n",
      "Attempting to load checkpoint from: ../model_logs/minecraft39ch_ce_3/saved_models/vqgan_95000.th\n",
      "\n",
      "Load successful!\n",
      "loaded from: minecraft39ch_ce_3\n"
     ]
    }
   ],
   "source": [
    "from models3d import VQAutoEncoder\n",
    "\n",
    "# vqmodel_path = 'saved_models/minecraft39ch_ce_3'\n",
    "#vqgan_hparams =  vq_dict_to_vcqgan_hparams(vq_load_hparams_from_json(f\"../model_logs/minecraft39ch_ce_3_fqdataset\"), 'minecraft')\n",
    "vqgan_hparams =  vq_dict_to_vcqgan_hparams(vq_load_hparams_from_json(f\"../model_logs/minecraft39ch_ce_3\"), 'minecraft')\n",
    "#vqgan_hparams.load_step = 10000\n",
    "vqgan_hparams.load_step = 95000\n",
    "\n",
    "vqgan = VQAutoEncoder(vqgan_hparams)\n",
    "vqgan = vq_load_vqgan_from_checkpoint(vqgan_hparams, vqgan)\n",
    "print(f'loaded from: {vqgan_hparams.log_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1289/1460270348.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  processed_data = torch.load(data_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11082 chunks of size torch.Size([43, 24, 24, 24])\n",
      "Number of unique block types: 43\n",
      "Unique blocks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]\n",
      "\n",
      "Dataloader details:\n",
      "Training samples: 9974\n",
      "Validation samples: 1108\n",
      "Batch size: 4\n",
      "Training batches: 2494\n",
      "Validation batches: 277\n"
     ]
    }
   ],
   "source": [
    "# loads a preprocesed dataset, which already has a mappings file created and everything one-hot encoded nicely. For more memory efficient, could try just loading the validation set file\n",
    "# data_path = '../../text2env/data/minecraft_biome_newworld_10k_processed_cleaned.pt'\n",
    "data_path = './minecraft_biome_newworld_10k_processed_cleaned.pt'\n",
    "train_loader, val_loader = get_minecraft_dataloaders(\n",
    "    data_path,\n",
    "    batch_size=4,\n",
    "    num_workers=0, # Must be 0 if you're on windows, otherwise it errors\n",
    "    val_split=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1289/545832235.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mappings = torch.load(path)\n"
     ]
    }
   ],
   "source": [
    "# mappings_path = '../../text2env/data/minecraft_biome_newworld_10k_mappings.pt'\n",
    "mappings_path = './minecraft_biome_newworld_10k_mappings.pt'\n",
    "\n",
    "block_converter = BlockBiomeConverter.load_mappings(mappings_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"esms-options\">{\"shimMode\": true}</script><style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "  const py_version = '3.6.3'.replace('rc', '-rc.').replace('.dev', '-dev.');\n",
       "  const reloading = false;\n",
       "  const Bokeh = root.Bokeh;\n",
       "\n",
       "  // Set a timeout for this load but only if we are not already initializing\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || (force || !root._bokeh_is_initializing)) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks;\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "    if (js_modules == null) js_modules = [];\n",
       "    if (js_exports == null) js_exports = {};\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      // Don't load bokeh if it is still initializing\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    } else if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n",
       "      // There is nothing to load\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "    window._bokeh_on_load = on_load\n",
       "\n",
       "    function on_error(e) {\n",
       "      const src_el = e.srcElement\n",
       "      console.error(\"failed to load \" + (src_el.href || src_el.src));\n",
       "    }\n",
       "\n",
       "    const skip = [];\n",
       "    if (window.requirejs) {\n",
       "      window.requirejs.config({'packages': {}, 'paths': {'vtk': 'https://cdn.jsdelivr.net/npm/vtk.js@30.1.0/vtk'}, 'shim': {'vtk': {'exports': 'vtk'}}});\n",
       "      require([\"vtk\"], function() {\n",
       "        on_load()\n",
       "      })\n",
       "      root._bokeh_is_loading = css_urls.length + 1;\n",
       "    } else {\n",
       "      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n",
       "    }\n",
       "\n",
       "    const existing_stylesheets = []\n",
       "    const links = document.getElementsByTagName('link')\n",
       "    for (let i = 0; i < links.length; i++) {\n",
       "      const link = links[i]\n",
       "      if (link.href != null) {\n",
       "        existing_stylesheets.push(link.href)\n",
       "      }\n",
       "    }\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const escaped = encodeURI(url)\n",
       "      if (existing_stylesheets.indexOf(escaped) !== -1) {\n",
       "        on_load()\n",
       "        continue;\n",
       "      }\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }    if (((window.vtk !== undefined) && (!(window.vtk instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://cdn.holoviz.org/panel/1.6.1/dist/bundled/abstractvtkplot/vtk.js@30.1.0/vtk.js'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(encodeURI(urls[i]))\n",
       "      }\n",
       "    }    var existing_scripts = []\n",
       "    const scripts = document.getElementsByTagName('script')\n",
       "    for (let i = 0; i < scripts.length; i++) {\n",
       "      var script = scripts[i]\n",
       "      if (script.src != null) {\n",
       "        existing_scripts.push(script.src)\n",
       "      }\n",
       "    }\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const escaped = encodeURI(url)\n",
       "      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n",
       "        if (!window.requirejs) {\n",
       "          on_load();\n",
       "        }\n",
       "        continue;\n",
       "      }\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    for (let i = 0; i < js_modules.length; i++) {\n",
       "      const url = js_modules[i];\n",
       "      const escaped = encodeURI(url)\n",
       "      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n",
       "        if (!window.requirejs) {\n",
       "          on_load();\n",
       "        }\n",
       "        continue;\n",
       "      }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      element.type = \"module\";\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    for (const name in js_exports) {\n",
       "      const url = js_exports[name];\n",
       "      const escaped = encodeURI(url)\n",
       "      if (skip.indexOf(escaped) >= 0 || root[name] != null) {\n",
       "        if (!window.requirejs) {\n",
       "          on_load();\n",
       "        }\n",
       "        continue;\n",
       "      }\n",
       "      var element = document.createElement('script');\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.type = \"module\";\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      element.textContent = `\n",
       "      import ${name} from \"${url}\"\n",
       "      window.${name} = ${name}\n",
       "      window._bokeh_on_load()\n",
       "      `\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    if (!js_urls.length && !js_modules.length) {\n",
       "      on_load()\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.holoviz.org/panel/1.6.1/dist/bundled/reactiveesm/es-module-shims@^1.10.0/dist/es-module-shims.min.js\", \"https://cdn.holoviz.org/panel/1.6.1/dist/bundled/abstractvtkplot/vtk.js@30.1.0/vtk.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.3.min.js\", \"https://cdn.holoviz.org/panel/1.6.1/dist/panel.min.js\"];\n",
       "  const js_modules = [];\n",
       "  const js_exports = {};\n",
       "  const css_urls = [];\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (let i = 0; i < inline_js.length; i++) {\n",
       "        try {\n",
       "          inline_js[i].call(root, root.Bokeh);\n",
       "        } catch(e) {\n",
       "          if (!reloading) {\n",
       "            throw e;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      // Cache old bokeh versions\n",
       "      if (Bokeh != undefined && !reloading) {\n",
       "        var NewBokeh = root.Bokeh;\n",
       "        if (Bokeh.versions === undefined) {\n",
       "          Bokeh.versions = new Map();\n",
       "        }\n",
       "        if (NewBokeh.version !== Bokeh.version) {\n",
       "          Bokeh.versions.set(NewBokeh.version, NewBokeh)\n",
       "        }\n",
       "        root.Bokeh = Bokeh;\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    }\n",
       "    root._bokeh_is_initializing = false\n",
       "  }\n",
       "\n",
       "  function load_or_wait() {\n",
       "    // Implement a backoff loop that tries to ensure we do not load multiple\n",
       "    // versions of Bokeh and its dependencies at the same time.\n",
       "    // In recent versions we use the root._bokeh_is_initializing flag\n",
       "    // to determine whether there is an ongoing attempt to initialize\n",
       "    // bokeh, however for backward compatibility we also try to ensure\n",
       "    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n",
       "    // before older versions are fully initialized.\n",
       "    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n",
       "      // If the timeout and bokeh was not successfully loaded we reset\n",
       "      // everything and try loading again\n",
       "      root._bokeh_timeout = Date.now() + 5000;\n",
       "      root._bokeh_is_initializing = false;\n",
       "      root._bokeh_onload_callbacks = undefined;\n",
       "      root._bokeh_is_loading = 0\n",
       "      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n",
       "      load_or_wait();\n",
       "    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n",
       "      setTimeout(load_or_wait, 100);\n",
       "    } else {\n",
       "      root._bokeh_is_initializing = true\n",
       "      root._bokeh_onload_callbacks = []\n",
       "      const bokeh_loaded = root.Bokeh != null && (root.Bokeh.version === py_version || (root.Bokeh.versions !== undefined && root.Bokeh.versions.has(py_version)));\n",
       "      if (!reloading && !bokeh_loaded) {\n",
       "        if (root.Bokeh) {\n",
       "          root.Bokeh = undefined;\n",
       "        }\n",
       "        console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "      }\n",
       "      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n",
       "        console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "        run_inline_js();\n",
       "      });\n",
       "    }\n",
       "  }\n",
       "  // Give older versions of the autoload script a head-start to ensure\n",
       "  // they initialize before we start loading newer version.\n",
       "  setTimeout(load_or_wait, 100)\n",
       "}(window));"
      ],
      "application/vnd.holoviews_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n  const py_version = '3.6.3'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  const reloading = false;\n  const Bokeh = root.Bokeh;\n\n  // Set a timeout for this load but only if we are not already initializing\n  if (typeof (root._bokeh_timeout) === \"undefined\" || (force || !root._bokeh_is_initializing)) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      // Don't load bokeh if it is still initializing\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    } else if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      // There is nothing to load\n      run_callbacks();\n      return null;\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error(e) {\n      const src_el = e.srcElement\n      console.error(\"failed to load \" + (src_el.href || src_el.src));\n    }\n\n    const skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'vtk': 'https://cdn.jsdelivr.net/npm/vtk.js@30.1.0/vtk'}, 'shim': {'vtk': {'exports': 'vtk'}}});\n      require([\"vtk\"], function() {\n        on_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 1;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    const existing_stylesheets = []\n    const links = document.getElementsByTagName('link')\n    for (let i = 0; i < links.length; i++) {\n      const link = links[i]\n      if (link.href != null) {\n        existing_stylesheets.push(link.href)\n      }\n    }\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const escaped = encodeURI(url)\n      if (existing_stylesheets.indexOf(escaped) !== -1) {\n        on_load()\n        continue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window.vtk !== undefined) && (!(window.vtk instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.6.1/dist/bundled/abstractvtkplot/vtk.js@30.1.0/vtk.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(encodeURI(urls[i]))\n      }\n    }    var existing_scripts = []\n    const scripts = document.getElementsByTagName('script')\n    for (let i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n        existing_scripts.push(script.src)\n      }\n    }\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (let i = 0; i < js_modules.length; i++) {\n      const url = js_modules[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      const url = js_exports[name];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) >= 0 || root[name] != null) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.holoviz.org/panel/1.6.1/dist/bundled/reactiveesm/es-module-shims@^1.10.0/dist/es-module-shims.min.js\", \"https://cdn.holoviz.org/panel/1.6.1/dist/bundled/abstractvtkplot/vtk.js@30.1.0/vtk.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.3.min.js\", \"https://cdn.holoviz.org/panel/1.6.1/dist/panel.min.js\"];\n  const js_modules = [];\n  const js_exports = {};\n  const css_urls = [];\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (let i = 0; i < inline_js.length; i++) {\n        try {\n          inline_js[i].call(root, root.Bokeh);\n        } catch(e) {\n          if (!reloading) {\n            throw e;\n          }\n        }\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n        var NewBokeh = root.Bokeh;\n        if (Bokeh.versions === undefined) {\n          Bokeh.versions = new Map();\n        }\n        if (NewBokeh.version !== Bokeh.version) {\n          Bokeh.versions.set(NewBokeh.version, NewBokeh)\n        }\n        root.Bokeh = Bokeh;\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      // If the timeout and bokeh was not successfully loaded we reset\n      // everything and try loading again\n      root._bokeh_timeout = Date.now() + 5000;\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      root._bokeh_is_loading = 0\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      const bokeh_loaded = root.Bokeh != null && (root.Bokeh.version === py_version || (root.Bokeh.versions !== undefined && root.Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n        if (root.Bokeh) {\n          root.Bokeh = undefined;\n        }\n        console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n        console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n        run_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "if ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n",
       "  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n",
       "}\n",
       "\n",
       "\n",
       "    function JupyterCommManager() {\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n",
       "      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        comm_manager.register_target(comm_id, function(comm) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        });\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        });\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n",
       "          var messages = comm.messages[Symbol.asyncIterator]();\n",
       "          function processIteratorResult(result) {\n",
       "            var message = result.value;\n",
       "            console.log(message)\n",
       "            var content = {data: message.data, comm_id};\n",
       "            var buffers = []\n",
       "            for (var buffer of message.buffers || []) {\n",
       "              buffers.push(new DataView(buffer))\n",
       "            }\n",
       "            var metadata = message.metadata || {};\n",
       "            var msg = {content, buffers, metadata}\n",
       "            msg_handler(msg);\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "          return messages.next().then(processIteratorResult);\n",
       "        })\n",
       "      }\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n",
       "      if (comm_id in window.PyViz.comms) {\n",
       "        return window.PyViz.comms[comm_id];\n",
       "      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n",
       "        if (msg_handler) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        }\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n",
       "        comm.open();\n",
       "        if (msg_handler) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        }\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        var comm_promise = google.colab.kernel.comms.open(comm_id)\n",
       "        comm_promise.then((comm) => {\n",
       "          window.PyViz.comms[comm_id] = comm;\n",
       "          if (msg_handler) {\n",
       "            var messages = comm.messages[Symbol.asyncIterator]();\n",
       "            function processIteratorResult(result) {\n",
       "              var message = result.value;\n",
       "              var content = {data: message.data};\n",
       "              var metadata = message.metadata || {comm_id};\n",
       "              var msg = {content, metadata}\n",
       "              msg_handler(msg);\n",
       "              return messages.next().then(processIteratorResult);\n",
       "            }\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "        })\n",
       "        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n",
       "          return comm_promise.then((comm) => {\n",
       "            comm.send(data, metadata, buffers, disposeOnDone);\n",
       "          });\n",
       "        };\n",
       "        var comm = {\n",
       "          send: sendClosure\n",
       "        };\n",
       "      }\n",
       "      window.PyViz.comms[comm_id] = comm;\n",
       "      return comm;\n",
       "    }\n",
       "    window.PyViz.comm_manager = new JupyterCommManager();\n",
       "    \n",
       "\n",
       "\n",
       "var JS_MIME_TYPE = 'application/javascript';\n",
       "var HTML_MIME_TYPE = 'text/html';\n",
       "var EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\n",
       "var CLASS_NAME = 'output';\n",
       "\n",
       "/**\n",
       " * Render data to the DOM node\n",
       " */\n",
       "function render(props, node) {\n",
       "  var div = document.createElement(\"div\");\n",
       "  var script = document.createElement(\"script\");\n",
       "  node.appendChild(div);\n",
       "  node.appendChild(script);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when a new output is added\n",
       " */\n",
       "function handle_add_output(event, handle) {\n",
       "  var output_area = handle.output_area;\n",
       "  var output = handle.output;\n",
       "  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "    return\n",
       "  }\n",
       "  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "  if (id !== undefined) {\n",
       "    var nchildren = toinsert.length;\n",
       "    var html_node = toinsert[nchildren-1].children[0];\n",
       "    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var scripts = [];\n",
       "    var nodelist = html_node.querySelectorAll(\"script\");\n",
       "    for (var i in nodelist) {\n",
       "      if (nodelist.hasOwnProperty(i)) {\n",
       "        scripts.push(nodelist[i])\n",
       "      }\n",
       "    }\n",
       "\n",
       "    scripts.forEach( function (oldScript) {\n",
       "      var newScript = document.createElement(\"script\");\n",
       "      var attrs = [];\n",
       "      var nodemap = oldScript.attributes;\n",
       "      for (var j in nodemap) {\n",
       "        if (nodemap.hasOwnProperty(j)) {\n",
       "          attrs.push(nodemap[j])\n",
       "        }\n",
       "      }\n",
       "      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n",
       "      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n",
       "      oldScript.parentNode.replaceChild(newScript, oldScript);\n",
       "    });\n",
       "    if (JS_MIME_TYPE in output.data) {\n",
       "      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n",
       "    }\n",
       "    output_area._hv_plot_id = id;\n",
       "    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n",
       "      window.PyViz.plot_index[id] = Bokeh.index[id];\n",
       "    } else {\n",
       "      window.PyViz.plot_index[id] = null;\n",
       "    }\n",
       "  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "    var bk_div = document.createElement(\"div\");\n",
       "    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var script_attrs = bk_div.children[0].attributes;\n",
       "    for (var i = 0; i < script_attrs.length; i++) {\n",
       "      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "    }\n",
       "    // store reference to server id on output_area\n",
       "    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when an output is cleared or removed\n",
       " */\n",
       "function handle_clear_output(event, handle) {\n",
       "  var id = handle.cell.output_area._hv_plot_id;\n",
       "  var server_id = handle.cell.output_area._bokeh_server_id;\n",
       "  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n",
       "  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n",
       "  if (server_id !== null) {\n",
       "    comm.send({event_type: 'server_delete', 'id': server_id});\n",
       "    return;\n",
       "  } else if (comm !== null) {\n",
       "    comm.send({event_type: 'delete', 'id': id});\n",
       "  }\n",
       "  delete PyViz.plot_index[id];\n",
       "  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n",
       "    var doc = window.Bokeh.index[id].model.document\n",
       "    doc.clear();\n",
       "    const i = window.Bokeh.documents.indexOf(doc);\n",
       "    if (i > -1) {\n",
       "      window.Bokeh.documents.splice(i, 1);\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle kernel restart event\n",
       " */\n",
       "function handle_kernel_cleanup(event, handle) {\n",
       "  delete PyViz.comms[\"hv-extension-comm\"];\n",
       "  window.PyViz.plot_index = {}\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle update_display_data messages\n",
       " */\n",
       "function handle_update_output(event, handle) {\n",
       "  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n",
       "  handle_add_output(event, handle)\n",
       "}\n",
       "\n",
       "function register_renderer(events, OutputArea) {\n",
       "  function append_mime(data, metadata, element) {\n",
       "    // create a DOM node to render to\n",
       "    var toinsert = this.create_output_subarea(\n",
       "    metadata,\n",
       "    CLASS_NAME,\n",
       "    EXEC_MIME_TYPE\n",
       "    );\n",
       "    this.keyboard_manager.register_events(toinsert);\n",
       "    // Render to node\n",
       "    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "    render(props, toinsert[0]);\n",
       "    element.append(toinsert);\n",
       "    return toinsert\n",
       "  }\n",
       "\n",
       "  events.on('output_added.OutputArea', handle_add_output);\n",
       "  events.on('output_updated.OutputArea', handle_update_output);\n",
       "  events.on('clear_output.CodeCell', handle_clear_output);\n",
       "  events.on('delete.Cell', handle_clear_output);\n",
       "  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n",
       "\n",
       "  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "    safe: true,\n",
       "    index: 0\n",
       "  });\n",
       "}\n",
       "\n",
       "if (window.Jupyter !== undefined) {\n",
       "  try {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  } catch(err) {\n",
       "  }\n",
       "}\n"
      ],
      "application/vnd.holoviews_load.v0+json": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        })\n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='b92ec191-0d41-49ec-a493-30260b5f8031'>\n",
       "  <div id=\"ae86cf31-61a0-4a30-b824-876ca72ec660\" data-root-id=\"b92ec191-0d41-49ec-a493-30260b5f8031\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"07c8a609-7ac6-46e4-87a9-161c3f6732b7\":{\"version\":\"3.6.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"b92ec191-0d41-49ec-a493-30260b5f8031\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"fbe9319d-6135-4bda-bdb0-65bf83d9dd14\",\"attributes\":{\"plot_id\":\"b92ec191-0d41-49ec-a493-30260b5f8031\",\"comm_id\":\"482b905736b643758b23e4c3aaf625ba\",\"client_comm_id\":\"feb431fed97949bda1c957f320e4e16e\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_rendered\",\"kind\":\"Any\",\"default\":false},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"ReactiveESM1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"JSComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"ReactComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"AnyWidgetComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"request_value1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"_synced\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_request_sync\",\"kind\":\"Any\",\"default\":0}]}]}};\n",
       "  var render_items = [{\"docid\":\"07c8a609-7ac6-46e4-87a9-161c3f6732b7\",\"roots\":{\"b92ec191-0d41-49ec-a493-30260b5f8031\":\"ae86cf31-61a0-4a30-b824-876ca72ec660\"},\"root_ids\":[\"b92ec191-0d41-49ec-a493-30260b5f8031\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined && ( root.vtk !== undefined) && ( root.vtk !== undefined))\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "b92ec191-0d41-49ec-a493-30260b5f8031"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an instnance of our visualizer\n",
    "from visualization_utils import MinecraftVisualizerPyVista\n",
    "visualizer = MinecraftVisualizerPyVista()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a random sample from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample\n",
    "# You can rerun this to get a different sample, since the train loader has shuffle=True\n",
    "batch = next(iter(train_loader))\n",
    "sample = batch[0].unsqueeze(0).cuda()  # Add batch dim and move to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you find an interesting sample, like one with a cool feature or with a clear biome split, you can save it and later load it with the example code below\n",
    "# torch.save(sample, 'bowlaroundlake.pt')\n",
    "# sample = torch.load('treesandcorner.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 24, 24])\n"
     ]
    }
   ],
   "source": [
    "# Get style and structure code matrices\n",
    "# Make sure to rerun this if you get a new sample from the loader\n",
    "with torch.no_grad():\n",
    "    style_indices, struct_indices = encode_and_quantize(fqgan, sample)\n",
    "reconstructed, binary_reconstructed = decode_from_indices(style_indices, struct_indices, fqgan, two_stage=True)\n",
    "print(reconstructed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store codes and its corresponding chunks.\n",
    "structure_dict = {}\n",
    "for batch in train_loader:\n",
    "    for sample_idx in range(len(batch)):\n",
    "        sample= batch[sample_idx].unsqueeze(0).cuda()\n",
    "        with torch.no_grad():\n",
    "            style_indices, struct_indices = encode_and_quantize(fqgan, sample)\n",
    "        reconstructed, binary_reconstructed = decode_from_indices(style_indices, struct_indices, fqgan, two_stage=True)\n",
    "        for i in range(struct_indices.shape[1]):  \n",
    "            for j in range(struct_indices.shape[2]):  \n",
    "                for k in range(struct_indices.shape[3]): \n",
    "                    struct_code = struct_indices[0, i, j, k].item()  \n",
    "                    x_start, y_start, z_start = i * 4, j * 4, k * 4\n",
    "                    x_end, y_end, z_end = x_start + 4, y_start + 4, z_start + 4\n",
    "                    block = reconstructed[x_start:x_end, y_start:y_end, z_start:z_end]\n",
    "                    if struct_code not in structure_dict:\n",
    "                        structure_dict[struct_code] = []  # Initialize list if not present\n",
    "                    structure_dict[struct_code].append(block)\n",
    "\n",
    "# convert chunks into binary chunks\n",
    "binary_structure_dict = {\n",
    "    struct_code: [(b != 0).to(dtype=torch.int) for b in block_list]  # Convert each block tensor\n",
    "    for struct_code, block_list in structure_dict.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4D (unbatched) or 5D (batched) input to conv3d, but got input of size: [1, 4, 43, 24, 24, 24]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m sample= batch[sample_idx].unsqueeze(\u001b[32m0\u001b[39m).cuda()\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     style_indices, struct_indices = \u001b[43mencode_and_quantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfqgan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m reconstructed, binary_reconstructed = decode_from_indices(style_indices, struct_indices, fqgan, two_stage=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(struct_indices.shape[\u001b[32m1\u001b[39m]):  \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mencode_and_quantize\u001b[39m\u001b[34m(fqgan, terrain_chunks, device)\u001b[39m\n\u001b[32m     46\u001b[39m terrain_chunks = terrain_chunks.to(device)\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Get encodings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m h_style, h_struct = \u001b[43mfqgan\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mterrain_chunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Process style path\u001b[39;00m\n\u001b[32m     52\u001b[39m h_style = fqgan.quant_conv_style(h_style)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/minecraft_diffusion-master/Dual_codebook_2.0/fq_models.py:533\u001b[39m, in \u001b[36mEncoder.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m533\u001b[39m     h = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    534\u001b[39m     \u001b[38;5;66;03m# downsampling\u001b[39;00m\n\u001b[32m    535\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i_level, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.conv_blocks):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:725\u001b[39m, in \u001b[36mConv3d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m725\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:720\u001b[39m, in \u001b[36mConv3d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    709\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv3d(\n\u001b[32m    710\u001b[39m         F.pad(\n\u001b[32m    711\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    718\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    719\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected 4D (unbatched) or 5D (batched) input to conv3d, but got input of size: [1, 4, 43, 24, 24, 24]"
     ]
    }
   ],
   "source": [
    "# Use to find out which index is entirely not used, don't need to run \n",
    "import numpy as np\n",
    "structure_dict = {}\n",
    "structure_array = []\n",
    "for batch in train_loader:\n",
    "    for sample_idx in range(len(batch)):\n",
    "        sample= batch[sample_idx].unsqueeze(0).cuda()\n",
    "        with torch.no_grad():\n",
    "            style_indices, struct_indices = encode_and_quantize(fqgan, sample)\n",
    "        reconstructed, binary_reconstructed = decode_from_indices(style_indices, struct_indices, fqgan, two_stage=True)\n",
    "        for i in range(struct_indices.shape[1]):  \n",
    "            for j in range(struct_indices.shape[2]):  \n",
    "                for k in range(struct_indices.shape[3]): \n",
    "                    struct_code = struct_indices[0, i, j, k].item()  \n",
    "                    structure_array.append(struct_code)\n",
    "#print(structure_array)\n",
    "print(np.unique(structure_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([[[0.6  , 0.833, 0.833, 0.8  ],\n",
      "        [0.6  , 0.8  , 0.8  , 0.8  ],\n",
      "        [0.567, 0.833, 0.8  , 0.8  ],\n",
      "        [0.467, 0.733, 0.733, 0.733]],\n",
      "\n",
      "       [[0.8  , 0.933, 0.933, 0.9  ],\n",
      "        [0.867, 0.933, 0.9  , 0.867],\n",
      "        [0.833, 0.933, 0.933, 0.9  ],\n",
      "        [0.767, 0.867, 0.9  , 0.867]],\n",
      "\n",
      "       [[0.867, 0.933, 0.933, 0.933],\n",
      "        [0.867, 0.933, 0.9  , 0.9  ],\n",
      "        [0.867, 0.933, 0.9  , 0.9  ],\n",
      "        [0.767, 0.867, 0.867, 0.867]],\n",
      "\n",
      "       [[0.833, 0.833, 0.867, 0.9  ],\n",
      "        [0.8  , 0.9  , 0.867, 0.9  ],\n",
      "        [0.9  , 0.9  , 0.9  , 0.9  ],\n",
      "        [0.8  , 0.833, 0.8  , 0.833]]]), 1: array([[[0.789, 0.862, 0.821, 0.741],\n",
      "        [0.794, 0.853, 0.817, 0.746],\n",
      "        [0.822, 0.853, 0.831, 0.776],\n",
      "        [0.71 , 0.739, 0.726, 0.692]],\n",
      "\n",
      "       [[0.907, 0.953, 0.905, 0.851],\n",
      "        [0.899, 0.938, 0.897, 0.845],\n",
      "        [0.893, 0.908, 0.887, 0.845],\n",
      "        [0.769, 0.789, 0.776, 0.751]],\n",
      "\n",
      "       [[0.878, 0.932, 0.889, 0.837],\n",
      "        [0.875, 0.917, 0.878, 0.833],\n",
      "        [0.88 , 0.905, 0.882, 0.838],\n",
      "        [0.756, 0.78 , 0.773, 0.748]],\n",
      "\n",
      "       [[0.785, 0.862, 0.823, 0.752],\n",
      "        [0.779, 0.847, 0.813, 0.748],\n",
      "        [0.81 , 0.848, 0.825, 0.772],\n",
      "        [0.714, 0.742, 0.735, 0.703]]]), 3: array([[[0.846, 0.923, 0.949, 0.923],\n",
      "        [0.872, 0.923, 0.974, 0.949],\n",
      "        [0.897, 0.949, 0.974, 0.974],\n",
      "        [0.718, 0.872, 0.923, 0.897]],\n",
      "\n",
      "       [[0.821, 0.974, 1.   , 0.974],\n",
      "        [0.846, 0.974, 0.974, 0.974],\n",
      "        [0.846, 0.974, 1.   , 0.974],\n",
      "        [0.846, 0.949, 1.   , 0.974]],\n",
      "\n",
      "       [[0.769, 0.897, 0.897, 0.923],\n",
      "        [0.795, 0.923, 0.923, 0.949],\n",
      "        [0.795, 0.923, 0.974, 0.974],\n",
      "        [0.821, 0.923, 0.949, 0.949]],\n",
      "\n",
      "       [[0.692, 0.872, 0.872, 0.846],\n",
      "        [0.718, 0.897, 0.872, 0.872],\n",
      "        [0.744, 0.923, 0.897, 0.897],\n",
      "        [0.718, 0.846, 0.846, 0.846]]]), 4: array([[[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]],\n",
      "\n",
      "       [[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]],\n",
      "\n",
      "       [[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]],\n",
      "\n",
      "       [[0., 1., 1., 1.],\n",
      "        [0., 1., 1., 1.],\n",
      "        [0., 1., 1., 1.],\n",
      "        [0., 1., 1., 1.]]]), 5: array([[[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]],\n",
      "\n",
      "       [[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]],\n",
      "\n",
      "       [[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]],\n",
      "\n",
      "       [[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]]]), 6: array([[[0.777, 0.865, 0.845, 0.787],\n",
      "        [0.773, 0.853, 0.84 , 0.789],\n",
      "        [0.802, 0.844, 0.838, 0.801],\n",
      "        [0.644, 0.684, 0.688, 0.667]],\n",
      "\n",
      "       [[0.897, 0.961, 0.932, 0.886],\n",
      "        [0.89 , 0.948, 0.924, 0.884],\n",
      "        [0.878, 0.907, 0.9  , 0.87 ],\n",
      "        [0.708, 0.738, 0.742, 0.727]],\n",
      "\n",
      "       [[0.859, 0.93 , 0.904, 0.864],\n",
      "        [0.86 , 0.924, 0.899, 0.865],\n",
      "        [0.861, 0.903, 0.892, 0.863],\n",
      "        [0.69 , 0.729, 0.739, 0.726]],\n",
      "\n",
      "       [[0.735, 0.835, 0.818, 0.762],\n",
      "        [0.737, 0.827, 0.815, 0.768],\n",
      "        [0.77 , 0.831, 0.83 , 0.79 ],\n",
      "        [0.629, 0.675, 0.692, 0.671]]]), 7: array([[[0.817, 0.88 , 0.854, 0.803],\n",
      "        [0.737, 0.815, 0.789, 0.725],\n",
      "        [0.69 , 0.742, 0.727, 0.675],\n",
      "        [0.532, 0.56 , 0.558, 0.546]],\n",
      "\n",
      "       [[0.915, 0.952, 0.926, 0.883],\n",
      "        [0.854, 0.907, 0.877, 0.834],\n",
      "        [0.773, 0.808, 0.791, 0.751],\n",
      "        [0.588, 0.606, 0.602, 0.604]],\n",
      "\n",
      "       [[0.881, 0.928, 0.9  , 0.867],\n",
      "        [0.82 , 0.882, 0.851, 0.815],\n",
      "        [0.761, 0.807, 0.787, 0.75 ],\n",
      "        [0.565, 0.592, 0.592, 0.599]],\n",
      "\n",
      "       [[0.789, 0.862, 0.84 , 0.795],\n",
      "        [0.72 , 0.808, 0.785, 0.726],\n",
      "        [0.68 , 0.742, 0.728, 0.674],\n",
      "        [0.513, 0.552, 0.555, 0.544]]]), 8: array([[[0.923, 0.952, 0.947, 0.926],\n",
      "        [0.929, 0.954, 0.949, 0.931],\n",
      "        [0.924, 0.952, 0.948, 0.921],\n",
      "        [0.849, 0.897, 0.893, 0.848]],\n",
      "\n",
      "       [[0.968, 0.987, 0.978, 0.962],\n",
      "        [0.969, 0.984, 0.976, 0.965],\n",
      "        [0.968, 0.98 , 0.974, 0.962],\n",
      "        [0.927, 0.948, 0.941, 0.915]],\n",
      "\n",
      "       [[0.956, 0.977, 0.966, 0.951],\n",
      "        [0.958, 0.976, 0.966, 0.952],\n",
      "        [0.955, 0.973, 0.965, 0.948],\n",
      "        [0.905, 0.934, 0.921, 0.895]],\n",
      "\n",
      "       [[0.916, 0.944, 0.938, 0.915],\n",
      "        [0.922, 0.946, 0.94 , 0.921],\n",
      "        [0.915, 0.944, 0.938, 0.911],\n",
      "        [0.835, 0.885, 0.88 , 0.834]]]), 9: array([[[1., 1., 0., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [1., 1., 0., 0.]],\n",
      "\n",
      "       [[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]],\n",
      "\n",
      "       [[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]],\n",
      "\n",
      "       [[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]]]), 10: array([[[0.895, 0.926, 0.914, 0.882],\n",
      "        [0.908, 0.931, 0.918, 0.895],\n",
      "        [0.902, 0.927, 0.914, 0.882],\n",
      "        [0.826, 0.875, 0.855, 0.804]],\n",
      "\n",
      "       [[0.965, 0.986, 0.972, 0.943],\n",
      "        [0.969, 0.985, 0.972, 0.946],\n",
      "        [0.967, 0.982, 0.97 , 0.948],\n",
      "        [0.93 , 0.958, 0.938, 0.895]],\n",
      "\n",
      "       [[0.952, 0.977, 0.961, 0.934],\n",
      "        [0.957, 0.976, 0.961, 0.937],\n",
      "        [0.954, 0.974, 0.959, 0.931],\n",
      "        [0.908, 0.943, 0.918, 0.878]],\n",
      "\n",
      "       [[0.902, 0.937, 0.926, 0.892],\n",
      "        [0.914, 0.942, 0.93 , 0.899],\n",
      "        [0.908, 0.939, 0.926, 0.892],\n",
      "        [0.835, 0.879, 0.862, 0.821]]]), 11: array([[[1.   , 1.   , 1.   , 0.857],\n",
      "        [1.   , 1.   , 1.   , 0.857],\n",
      "        [1.   , 1.   , 1.   , 0.857],\n",
      "        [0.714, 1.   , 0.857, 0.857]],\n",
      "\n",
      "       [[0.857, 1.   , 1.   , 1.   ],\n",
      "        [0.857, 1.   , 1.   , 1.   ],\n",
      "        [0.857, 1.   , 1.   , 1.   ],\n",
      "        [0.857, 1.   , 1.   , 1.   ]],\n",
      "\n",
      "       [[0.857, 1.   , 1.   , 1.   ],\n",
      "        [0.857, 1.   , 1.   , 1.   ],\n",
      "        [0.857, 1.   , 1.   , 1.   ],\n",
      "        [0.857, 1.   , 1.   , 0.857]],\n",
      "\n",
      "       [[0.857, 1.   , 1.   , 0.857],\n",
      "        [0.857, 1.   , 1.   , 0.857],\n",
      "        [0.857, 1.   , 0.857, 0.714],\n",
      "        [0.857, 1.   , 0.857, 0.714]]]), 12: array([[[0.75, 1.  , 1.  , 1.  ],\n",
      "        [0.75, 1.  , 1.  , 1.  ],\n",
      "        [0.75, 1.  , 1.  , 1.  ],\n",
      "        [0.75, 1.  , 1.  , 1.  ]],\n",
      "\n",
      "       [[0.75, 1.  , 1.  , 1.  ],\n",
      "        [1.  , 1.  , 1.  , 1.  ],\n",
      "        [1.  , 1.  , 1.  , 1.  ],\n",
      "        [0.75, 1.  , 1.  , 1.  ]],\n",
      "\n",
      "       [[0.75, 1.  , 1.  , 1.  ],\n",
      "        [0.75, 1.  , 1.  , 1.  ],\n",
      "        [0.75, 1.  , 1.  , 1.  ],\n",
      "        [0.75, 1.  , 1.  , 1.  ]],\n",
      "\n",
      "       [[0.75, 1.  , 1.  , 1.  ],\n",
      "        [1.  , 1.  , 1.  , 1.  ],\n",
      "        [1.  , 1.  , 1.  , 1.  ],\n",
      "        [0.75, 1.  , 1.  , 1.  ]]]), 13: array([[[0.823, 0.874, 0.842, 0.78 ],\n",
      "        [0.841, 0.881, 0.853, 0.802],\n",
      "        [0.851, 0.888, 0.866, 0.811],\n",
      "        [0.776, 0.833, 0.807, 0.75 ]],\n",
      "\n",
      "       [[0.939, 0.969, 0.937, 0.887],\n",
      "        [0.945, 0.97 , 0.94 , 0.903],\n",
      "        [0.947, 0.97 , 0.94 , 0.911],\n",
      "        [0.894, 0.929, 0.903, 0.857]],\n",
      "\n",
      "       [[0.914, 0.954, 0.915, 0.875],\n",
      "        [0.927, 0.957, 0.924, 0.89 ],\n",
      "        [0.93 , 0.957, 0.927, 0.896],\n",
      "        [0.869, 0.914, 0.879, 0.845]],\n",
      "\n",
      "       [[0.819, 0.876, 0.85 , 0.796],\n",
      "        [0.844, 0.89 , 0.865, 0.822],\n",
      "        [0.853, 0.896, 0.876, 0.833],\n",
      "        [0.773, 0.836, 0.815, 0.762]]]), 14: array([[[0.75 , 0.917, 0.833, 0.583],\n",
      "        [0.875, 0.917, 0.75 , 0.625],\n",
      "        [0.833, 0.792, 0.75 , 0.708],\n",
      "        [0.583, 0.625, 0.583, 0.542]],\n",
      "\n",
      "       [[0.75 , 0.958, 0.875, 0.792],\n",
      "        [0.833, 0.958, 0.708, 0.708],\n",
      "        [0.833, 0.75 , 0.625, 0.667],\n",
      "        [0.542, 0.583, 0.583, 0.583]],\n",
      "\n",
      "       [[0.625, 0.792, 0.75 , 0.667],\n",
      "        [0.708, 0.75 , 0.75 , 0.667],\n",
      "        [0.708, 0.708, 0.708, 0.708],\n",
      "        [0.542, 0.583, 0.583, 0.583]],\n",
      "\n",
      "       [[0.5  , 0.708, 0.667, 0.542],\n",
      "        [0.542, 0.625, 0.625, 0.583],\n",
      "        [0.667, 0.583, 0.625, 0.583],\n",
      "        [0.417, 0.417, 0.458, 0.5  ]]]), 15: array([[[0.636, 0.636, 0.773, 0.682],\n",
      "        [0.636, 0.727, 0.727, 0.591],\n",
      "        [0.5  , 0.545, 0.591, 0.545],\n",
      "        [0.364, 0.5  , 0.455, 0.455]],\n",
      "\n",
      "       [[0.773, 0.818, 0.909, 0.864],\n",
      "        [0.727, 0.864, 0.818, 0.773],\n",
      "        [0.591, 0.682, 0.727, 0.591],\n",
      "        [0.455, 0.5  , 0.5  , 0.409]],\n",
      "\n",
      "       [[0.818, 0.955, 0.909, 0.909],\n",
      "        [0.773, 0.909, 0.864, 0.818],\n",
      "        [0.591, 0.682, 0.727, 0.727],\n",
      "        [0.455, 0.5  , 0.5  , 0.455]],\n",
      "\n",
      "       [[0.773, 0.864, 0.864, 0.818],\n",
      "        [0.773, 0.818, 0.818, 0.818],\n",
      "        [0.545, 0.682, 0.727, 0.682],\n",
      "        [0.364, 0.455, 0.545, 0.5  ]]]), 16: array([[[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]],\n",
      "\n",
      "       [[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]],\n",
      "\n",
      "       [[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]],\n",
      "\n",
      "       [[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]]]), 17: array([[[0.766, 0.861, 0.86 , 0.837],\n",
      "        [0.77 , 0.859, 0.86 , 0.836],\n",
      "        [0.772, 0.858, 0.863, 0.84 ],\n",
      "        [0.704, 0.798, 0.809, 0.783]],\n",
      "\n",
      "       [[0.874, 0.949, 0.935, 0.913],\n",
      "        [0.877, 0.941, 0.926, 0.909],\n",
      "        [0.882, 0.934, 0.924, 0.908],\n",
      "        [0.813, 0.883, 0.879, 0.86 ]],\n",
      "\n",
      "       [[0.875, 0.943, 0.929, 0.908],\n",
      "        [0.873, 0.936, 0.923, 0.905],\n",
      "        [0.877, 0.93 , 0.921, 0.905],\n",
      "        [0.806, 0.873, 0.868, 0.851]],\n",
      "\n",
      "       [[0.792, 0.867, 0.866, 0.849],\n",
      "        [0.797, 0.862, 0.864, 0.846],\n",
      "        [0.798, 0.863, 0.869, 0.843],\n",
      "        [0.72 , 0.802, 0.813, 0.785]]]), 18: array([[[1. , 1. , 1. , 1. ],\n",
      "        [1. , 1. , 1. , 1. ],\n",
      "        [1. , 1. , 1. , 1. ],\n",
      "        [1. , 1. , 1. , 1. ]],\n",
      "\n",
      "       [[1. , 1. , 1. , 1. ],\n",
      "        [1. , 1. , 1. , 1. ],\n",
      "        [1. , 1. , 1. , 1. ],\n",
      "        [1. , 1. , 1. , 1. ]],\n",
      "\n",
      "       [[1. , 1. , 1. , 1. ],\n",
      "        [1. , 1. , 1. , 1. ],\n",
      "        [1. , 1. , 1. , 1. ],\n",
      "        [0.5, 1. , 1. , 1. ]],\n",
      "\n",
      "       [[1. , 1. , 1. , 0.5],\n",
      "        [1. , 1. , 1. , 0.5],\n",
      "        [1. , 1. , 1. , 0.5],\n",
      "        [0.5, 1. , 0.5, 0.5]]]), 19: array([[[0.889, 0.889, 1.   , 1.   ],\n",
      "        [0.889, 0.889, 0.889, 1.   ],\n",
      "        [0.889, 0.889, 1.   , 1.   ],\n",
      "        [0.778, 0.778, 0.889, 0.889]],\n",
      "\n",
      "       [[0.778, 0.889, 1.   , 1.   ],\n",
      "        [0.889, 0.889, 1.   , 1.   ],\n",
      "        [0.889, 0.889, 1.   , 1.   ],\n",
      "        [0.889, 0.889, 0.889, 0.889]],\n",
      "\n",
      "       [[0.778, 0.889, 1.   , 1.   ],\n",
      "        [0.778, 1.   , 1.   , 1.   ],\n",
      "        [0.889, 1.   , 1.   , 1.   ],\n",
      "        [0.889, 0.889, 0.889, 0.889]],\n",
      "\n",
      "       [[0.778, 0.778, 0.889, 0.778],\n",
      "        [0.778, 0.889, 0.889, 0.889],\n",
      "        [0.889, 0.889, 1.   , 1.   ],\n",
      "        [0.667, 0.667, 0.778, 0.889]]])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dictionary to store probability of existence at each position, sorted by key\n",
    "probability_dict = {}\n",
    "sorted_keys = sorted(binary_structure_dict.keys())  # Ensure sorted order\n",
    "\n",
    "for struct_code in sorted_keys:\n",
    "    block_list = binary_structure_dict[struct_code]\n",
    "    block_array = np.array(block_list)  # Convert list of binary blocks to NumPy array\n",
    "\n",
    "    # Compute probability: mean along the first axis (number of samples)\n",
    "    probability_array = np.mean(block_array, axis=0)\n",
    "    probability_array = np.round(probability_array, 3)\n",
    "    probability_dict[struct_code] = probability_array\n",
    "\n",
    "print(probability_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for results\n",
    "key = 0  # The structure code to check\n",
    "\n",
    "if key in binary_structure_dict:\n",
    "    values = binary_structure_dict[key]  # Get the list of blocks\n",
    "    print(f\"First 50 values for key {key}:\")\n",
    "    \n",
    "    for i, block in enumerate(values[:50]):  # Iterate over the first 50 blocks\n",
    "        print(f\"Block {i+1}:\")\n",
    "        print(block)  # Each block is a 4x4x4 tensor\n",
    "        print(\"-\" * 30)  # Separator\n",
    "else:\n",
    "    print(f\"Key {key} not found in dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/root/autodl-tmp/minecraft_diffusion-master/Dual_codebook_2.0/Structure_Heatmap_V2/heatmap_0.png',\n",
       " '/root/autodl-tmp/minecraft_diffusion-master/Dual_codebook_2.0/Structure_Heatmap_V2/heatmap_1.png',\n",
       " '/root/autodl-tmp/minecraft_diffusion-master/Dual_codebook_2.0/Structure_Heatmap_V2/heatmap_2.png',\n",
       " '/root/autodl-tmp/minecraft_diffusion-master/Dual_codebook_2.0/Structure_Heatmap_V2/heatmap_3.png',\n",
       " '/root/autodl-tmp/minecraft_diffusion-master/Dual_codebook_2.0/Structure_Heatmap_V2/heatmap_4.png',\n",
       " '/root/autodl-tmp/minecraft_diffusion-master/Dual_codebook_2.0/Structure_Heatmap_V2/heatmap_5.png',\n",
       " '/root/autodl-tmp/minecraft_diffusion-master/Dual_codebook_2.0/Structure_Heatmap_V2/heatmap_6.png',\n",
       " '/root/autodl-tmp/minecraft_diffusion-master/Dual_codebook_2.0/Structure_Heatmap_V2/heatmap_7.png',\n",
       " '/root/autodl-tmp/minecraft_diffusion-master/Dual_codebook_2.0/Structure_Heatmap_V2/heatmap_8.png',\n",
       " '/root/autodl-tmp/minecraft_diffusion-master/Dual_codebook_2.0/Structure_Heatmap_V2/heatmap_9.png',\n",
       " '/root/autodl-tmp/minecraft_diffusion-master/Dual_codebook_2.0/Structure_Heatmap_V2/heatmap_10.png',\n",
       " '/root/autodl-tmp/minecraft_diffusion-master/Dual_codebook_2.0/Structure_Heatmap_V2/heatmap_11.png',\n",
       " '/root/autodl-tmp/minecraft_diffusion-master/Dual_codebook_2.0/Structure_Heatmap_V2/heatmap_12.png',\n",
       " '/root/autodl-tmp/minecraft_diffusion-master/Dual_codebook_2.0/Structure_Heatmap_V2/heatmap_14.png',\n",
       " '/root/autodl-tmp/minecraft_diffusion-master/Dual_codebook_2.0/Structure_Heatmap_V2/heatmap_15.png',\n",
       " '/root/autodl-tmp/minecraft_diffusion-master/Dual_codebook_2.0/Structure_Heatmap_V2/heatmap_17.png',\n",
       " '/root/autodl-tmp/minecraft_diffusion-master/Dual_codebook_2.0/Structure_Heatmap_V2/heatmap_19.png']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot structural heatmap\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Define colors and levels for heatmap\n",
    "colors = [\"white\", \"yellow\", \"orange\", \"red\", \"black\"]\n",
    "levels = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "cmap = mcolors.ListedColormap(colors)\n",
    "norm = mcolors.BoundaryNorm(levels, cmap.N)\n",
    "\n",
    "# Ensure the directory exists\n",
    "save_dir = os.path.join(os.getcwd(), \"Structure_Heatmap_V2\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Directory to save images\n",
    "image_paths = []\n",
    "\n",
    "# Plot heatmap and corresponding values\n",
    "for struct_code, prob_matrix in probability_dict.items():\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Left subplot: Probability values\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax1.set_title(f\"Probability Values for Structure Code {struct_code}\")\n",
    "    ax1.set_xticks(range(4))\n",
    "    ax1.set_yticks(range(4))\n",
    "\n",
    "    # Display values in table format\n",
    "    table_data = []\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            row = [f\"{prob_matrix[i, j, k]:.2f}\" for k in range(4)]\n",
    "            table_data.append(row)\n",
    "    \n",
    "    ax1.axis(\"tight\")\n",
    "    ax1.axis(\"off\")\n",
    "    table = ax1.table(cellText=table_data, colLabels=[\"Z=0\", \"Z=1\", \"Z=2\", \"Z=3\"],\n",
    "                      rowLabels=[f\"X={i}, Y={j}\" for i in range(4) for j in range(4)],\n",
    "                      cellLoc=\"center\", loc=\"center\")\n",
    "\n",
    "    # Right subplot: 3D Heatmap\n",
    "    ax2 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "    # Get the coordinates for the 4x4x4 grid\n",
    "    x, y, z = np.indices((4, 4, 4)).reshape(3, -1)\n",
    "    probs = prob_matrix.flatten()\n",
    "\n",
    "    # Normalize values to discrete levels\n",
    "    color_indices = np.digitize(probs, levels, right=True) - 1\n",
    "    color_indices = np.clip(color_indices, 0, len(colors) - 1)\n",
    "    colors_mapped = [colors[i] for i in color_indices]\n",
    "\n",
    "    # Scatter plot with smaller spheres\n",
    "    ax2.scatter(x, y, z, c=colors_mapped, s=300, alpha=0.8, edgecolors=\"k\")\n",
    "\n",
    "    # Create color bar legend\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax2, fraction=0.02, pad=0.1, ticks=levels)\n",
    "    cbar.set_label(\"Probability Levels\")\n",
    "\n",
    "    ax2.set_title(f\"3D Frequency Heatmap for Structure Code {struct_code}\")\n",
    "    ax2.set_xlabel(\"X\")\n",
    "    ax2.set_ylabel(\"Y\")\n",
    "    ax2.set_zlabel(\"Z\")\n",
    "    ax2.set_xticks(range(4))\n",
    "    ax2.set_yticks(range(4))\n",
    "    ax2.set_zticks(range(4))\n",
    "\n",
    "    # Save the figure instead of displaying\n",
    "    image_path = os.path.join(save_dir, f\"heatmap_{struct_code}.png\")\n",
    "    plt.savefig(image_path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    image_paths.append(image_path)\n",
    "\n",
    "# Return the list of saved image file paths\n",
    "image_paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Exploration (Block Type Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the style codes and corresponding chunks\n",
    "style_dict = {}\n",
    "for batch in train_loader:\n",
    "    for sample_idx in range(len(batch)):\n",
    "        sample= batch[sample_idx].unsqueeze(0).cuda()\n",
    "        with torch.no_grad():\n",
    "            style_indices, struct_indices = encode_and_quantize(fqgan, sample)\n",
    "        reconstructed, binary_reconstructed = decode_from_indices(style_indices, struct_indices, fqgan, two_stage=True)\n",
    "        for i in range(style_indices.shape[1]):  \n",
    "            for j in range(style_indices.shape[2]):  \n",
    "                for k in range(style_indices.shape[3]): \n",
    "                    style_code = style_indices[0, i, j, k].item()  \n",
    "                    x_start, y_start, z_start = i * 4, j * 4, k * 4\n",
    "                    x_end, y_end, z_end = x_start + 4, y_start + 4, z_start + 4\n",
    "                    block = reconstructed[x_start:x_end, y_start:y_end, z_start:z_end]\n",
    "                    if style_code not in style_dict:\n",
    "                        style_dict[style_code] = []  # Initialize list if not present\n",
    "                    style_dict[style_code].append(block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'style_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mstyle_dict\u001b[49m[\u001b[32m19\u001b[39m][\u001b[32m20\u001b[39m:\u001b[32m30\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'style_dict' is not defined"
     ]
    }
   ],
   "source": [
    "print(style_dict[19][20:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {0: 0.19814343205684462, 2: 1.965038043136515e-07, 3: 0.0002077700224276342, 4: 4.336183948521243e-05, 6: 2.9475570647047725e-06, 8: 6.563227064075961e-05, 10: 0.00050711081766543, 11: 0.08270622419250037, 12: 6.55012681045505e-08, 14: 0.010865874363327675, 15: 9.471483367918003e-05, 16: 0.0021671749565071578, 17: 0.0015244765138653085, 18: 0.0018967857217715735, 19: 0.0021692709970865037, 22: 2.489048187972919e-06, 23: 1.1528223186400888e-05, 24: 3.0785596009138736e-06, 25: 0.0001168542622985181, 27: 7.532645832023308e-06, 28: 1.3100253620910101e-06, 32: 0.010261166656186463, 33: 0.00016853476283300845, 34: 1.6506319562346726e-05, 35: 6.55012681045505e-08, 36: 0.000286633549225513, 37: 0.6668276687836676, 38: 6.091617933723197e-06, 39: 3.93007608627303e-07, 40: 0.020675147770860845, 41: 0.0012035858014211155, 42: 1.6375317026137626e-05}, 1: {0: 0.2187001873866858, 2: 9.506223914921577e-08, 3: 0.0001265278403076062, 4: 1.5019833785576092e-05, 6: 9.506223914921577e-08, 8: 1.349883795918864e-05, 10: 9.753385736709539e-05, 11: 0.09092437000352871, 14: 0.006592471222758965, 15: 1.482970930727766e-05, 16: 0.0016134913850796393, 17: 0.0024868281761434847, 18: 0.001005378241242106, 19: 0.0007277014406872468, 22: 0.00021284435345509412, 23: 3.1370538919241207e-06, 24: 2.661742696178042e-06, 25: 7.101149264446418e-05, 27: 1.9012447829843155e-07, 28: 4.753111957460789e-07, 32: 0.01268786199700668, 33: 0.0005199904481462103, 34: 8.840788240877067e-06, 36: 0.00015162427144299915, 37: 0.6440257565433241, 38: 3.707427326819415e-06, 39: 9.506223914921577e-08, 40: 0.019197629071705826, 41: 0.0007799856722193154, 42: 1.6160580655366682e-05}, 2: {0: 0.1714207938505577, 3: 0.00010950301056164249, 4: 2.8532474582963183e-05, 6: 7.711479617017076e-07, 8: 1.3109515348929029e-05, 10: 8.868201559569638e-05, 11: 0.059942102211035436, 14: 0.00540112032375876, 15: 1.3109515348929029e-05, 16: 0.0008366955384463528, 17: 0.0013587627085184088, 18: 0.0018484416641989933, 19: 0.0007611230381995855, 22: 3.0845918468068306e-06, 23: 1.7736403119139277e-05, 24: 1.5422959234034153e-06, 25: 0.00012878170960418518, 32: 0.0037022813641298985, 33: 0.00015731418418714837, 34: 5.398035731911954e-06, 36: 0.0001310951534892903, 37: 0.6887585134734971, 38: 1.1567219425525614e-05, 40: 0.0645797860527095, 41: 0.0006516200276379429, 42: 2.8532474582963183e-05}, 3: {0: 0.18463008806905315, 3: 8.489243307190115e-05, 4: 2.1223108267975287e-05, 6: 2.7384655829645533e-06, 8: 4.655391491039741e-05, 10: 0.00035120821101520394, 11: 0.06101369780484599, 14: 0.0038776672654778073, 15: 8.900013144634798e-05, 16: 0.001276809578057223, 17: 0.0012596941681636946, 18: 0.0009680475835779696, 19: 0.0023229034307496823, 22: 5.4769311659291065e-06, 23: 6.846163957411383e-06, 24: 2.053849187223415e-06, 25: 7.32539543443018e-05, 27: 6.846163957411383e-07, 28: 6.846163957411383e-07, 32: 0.008823336108311791, 33: 0.00030944661087499453, 34: 1.1638478727599352e-05, 36: 5.0661613284844236e-05, 37: 0.7044072865092231, 38: 4.10769837444683e-06, 39: 1.3692327914822766e-06, 40: 0.02967469767339964, 41: 0.0006757163825965036, 42: 8.21539674889366e-06}, 4: {0: 0.24728947071754692, 3: 0.00038822501550081734, 4: 6.940138661856717e-05, 6: 2.4660391184262443e-06, 8: 5.707119102643594e-05, 10: 0.00048034919113916914, 11: 0.05879283862240009, 12: 1.7614565131616031e-07, 14: 0.016143572797474776, 15: 5.072994757905417e-05, 16: 0.000999802716870526, 17: 0.00199326419029367, 18: 0.0025861704526238655, 19: 0.0023705681754128856, 22: 2.1137478157939237e-06, 23: 1.2682486894763543e-05, 24: 2.818330421058565e-06, 25: 9.001042782255791e-05, 27: 5.284369539484809e-07, 28: 2.289893467110084e-06, 32: 0.006577630911448058, 33: 0.0004521658869285835, 34: 4.579786934220168e-06, 36: 0.0006464545403303084, 37: 0.6120929273998084, 38: 1.831914773688067e-05, 39: 3.5229130263232063e-07, 40: 0.046868834902203935, 41: 0.0019740643143002085, 42: 3.0120906375063412e-05}, 5: {0: 0.1354125149461937, 3: 9.652750099641291e-05, 4: 6.538959744918295e-05, 8: 6.227580709445995e-05, 10: 0.0002989238740534077, 11: 0.04768147170187326, 14: 0.0025377391390992428, 15: 7.784475886807493e-05, 16: 0.0008126992825827023, 17: 0.0013607263850139498, 18: 0.0012174920286966919, 19: 0.002416301315265046, 22: 6.227580709445995e-06, 23: 3.1137903547229974e-06, 24: 3.1137903547229974e-06, 25: 0.00013389298525308887, 32: 0.009332029693104822, 33: 0.00025844459944200876, 34: 6.227580709445995e-06, 36: 3.113790354722997e-05, 37: 0.7794159774810682, 38: 9.34137106416899e-06, 40: 0.01805375647668394, 41: 0.0006850338780390594, 42: 2.179653248306098e-05}, 6: {0: 0.16571127574861721, 3: 0.00020041603089834064, 4: 4.321237840930765e-05, 6: 4.470246042342171e-06, 8: 1.862602517642571e-05, 10: 0.00015571357047491894, 11: 0.10618622448979592, 14: 0.004378605998474156, 15: 2.6076435246995996e-05, 16: 0.001678577388899485, 17: 0.001779157924852184, 18: 0.0012241023745946976, 19: 0.0014319688155636087, 22: 7.59941827198169e-05, 23: 2.384131222582491e-05, 24: 2.2351230211710854e-06, 25: 0.00010058053595269884, 27: 7.450410070570285e-07, 32: 0.006506443114629029, 33: 0.00018179000572191495, 34: 1.490082014114057e-05, 36: 7.450410070570284e-05, 37: 0.6786615785332825, 38: 2.980164028228114e-06, 40: 0.030620440349036812, 41: 0.0008337008868968148, 42: 6.183840358573335e-05}, 7: {0: 0.17068459795076196, 2: 1.6923543492152892e-07, 3: 9.9510435733859e-05, 4: 8.54638946353721e-05, 6: 3.0462378285875204e-06, 8: 6.414022983525946e-05, 10: 0.0003939800924973193, 11: 0.0435207536798553, 14: 0.00525780649214206, 15: 8.800242615919504e-05, 16: 0.0006859112177369567, 17: 0.0013019282008513218, 18: 0.002064503070607731, 19: 0.0037732732570104088, 22: 3.0462378285875204e-06, 23: 2.183137110487723e-05, 24: 2.200060653979876e-06, 25: 0.00013200363923879254, 27: 5.077063047645867e-07, 28: 8.461771746076445e-07, 32: 0.00636240617587488, 33: 0.0004472892544976009, 34: 1.0661832400056322e-05, 36: 0.0001685584931818428, 37: 0.7301234674039013, 38: 9.30794892068409e-06, 39: 1.8615897841368182e-06, 40: 0.03370459074810186, 41: 0.0009487338481700911, 42: 3.9601091771637763e-05}, 8: {0: 0.19135826817104332, 3: 0.00017653464889326724, 4: 3.7760136071175694e-05, 6: 1.20255210417757e-06, 8: 4.858310500877382e-05, 10: 0.0003458539851614691, 11: 0.08911656674260382, 14: 0.007784360280762245, 15: 6.565934488809531e-05, 16: 0.0021191373179817135, 17: 0.0014353661915463473, 18: 0.00170329480035711, 19: 0.0019053235538589416, 22: 3.1266354708616815e-06, 23: 1.2266031462611212e-05, 24: 5.2912292583813074e-06, 25: 0.0001431037003971308, 27: 1.827879198349906e-05, 28: 1.20255210417757e-06, 32: 0.010461241264661515, 33: 0.00021814295169781116, 34: 2.2126958716867285e-05, 36: 0.000163787596588985, 37: 0.6584607717883201, 38: 7.4558230459009325e-06, 39: 1.20255210417757e-06, 40: 0.0333121363482437, 41: 0.0010483849244220054, 42: 2.357002124188037e-05}, 9: {0: 0.09776070038910506, 3: 3.21011673151751e-05, 4: 4.912451361867704e-05, 6: 4.863813229571984e-07, 8: 2.237354085603113e-05, 10: 9.72762645914397e-05, 11: 0.026622568093385215, 14: 0.0011556420233463035, 15: 7.344357976653697e-05, 16: 0.00035651750972762647, 17: 0.0012115758754863814, 18: 0.00045963035019455254, 19: 0.001865272373540856, 22: 4.863813229571984e-07, 23: 3.404669260700389e-06, 24: 1.4591439688715954e-06, 25: 1.896887159533074e-05, 27: 9.727626459143968e-07, 32: 0.004471303501945525, 33: 0.0004980544747081712, 34: 5.8365758754863816e-06, 36: 3.21011673151751e-05, 37: 0.8568565175097276, 38: 7.295719844357976e-06, 39: 9.727626459143968e-07, 40: 0.0077558365758754865, 41: 0.0006327821011673152, 42: 7.295719844357976e-06}, 10: {0: 0.17432153297657155, 2: 8.145742318239164e-08, 3: 0.00015582805054791522, 4: 8.267928453012751e-05, 6: 2.769552388201316e-06, 8: 7.876932821737272e-05, 10: 0.0006041289790322076, 11: 0.035972371922864385, 12: 1.6291484636478329e-07, 14: 0.008302059113326174, 15: 5.7753313036315674e-05, 16: 0.0005139556115693, 17: 0.0025134909784274677, 18: 0.002553649488056387, 19: 0.0028933269427269597, 22: 1.832792021603812e-06, 23: 9.530518512339822e-06, 24: 4.9689028141258905e-06, 25: 4.2154216496887675e-05, 27: 5.702019622767415e-07, 28: 7.331168086415248e-07, 32: 0.0061861210887403685, 33: 0.0008625933827899363, 34: 6.43513643140894e-06, 36: 0.0002036028292443879, 37: 0.7301053277064717, 38: 0.00010239198094026629, 39: 8.145742318239164e-07, 40: 0.031831850373791826, 41: 0.0025652571708598775, 42: 2.3256094318572814e-05}, 11: {0: 0.06999066265520827, 3: 1.3380023243011805e-05, 4: 5.160866108018839e-05, 6: 2.3892898648235365e-06, 8: 1.9114318918588292e-05, 10: 0.00011779199033580036, 11: 0.0306082367423084, 14: 0.0006334007431647195, 15: 1.5052526148388281e-05, 16: 0.0002795469141843538, 17: 0.002102814010031195, 18: 0.00042601038289803657, 19: 0.0007667231176218729, 22: 3.106076824270598e-06, 23: 9.557159459294146e-06, 24: 7.16786959447061e-07, 25: 1.6247171080800048e-05, 32: 0.0017998520551715702, 33: 0.0005576602544498134, 34: 7.88465655391767e-06, 36: 1.0273946418741208e-05, 37: 0.8810214883173283, 38: 5.5192595877423694e-05, 39: 9.557159459294147e-07, 40: 0.010008018456786347, 41: 0.001470130053825922, 42: 1.2185378310600036e-05}, 12: {0: 0.21152834330983575, 2: 2.0768995600295972e-07, 3: 0.00025123561677824695, 4: 3.274578306313332e-05, 6: 6.92299853343199e-07, 8: 1.4399836949538541e-05, 10: 7.158380483568678e-05, 11: 0.05260855815540304, 14: 0.007037366469204287, 15: 8.584518181455668e-06, 16: 0.0009303817729079253, 17: 0.0015277673163577718, 18: 0.002647700789111065, 19: 0.0007592452491614864, 22: 1.3153697213520782e-06, 23: 6.992228518766311e-06, 24: 8.999898093461588e-07, 25: 0.00011146027638825505, 27: 3.461499266715995e-07, 28: 7.61529838677519e-07, 32: 0.002534925143001458, 33: 0.00017681338254385304, 34: 7.3383784454379105e-06, 36: 0.00021606678422841244, 37: 0.6607946023872714, 38: 1.0038347873476386e-05, 39: 2.0768995600295972e-07, 40: 0.05758384028143927, 41: 0.0011070566954811097, 42: 2.85227539577398e-05}, 13: {0: 0.14730472952509244, 3: 0.00024254481923136035, 4: 3.4043856397645805e-05, 6: 2.2601730388478544e-06, 8: 3.171305295133396e-05, 10: 0.0001983301841588992, 11: 0.05473736506766958, 12: 7.063040746399545e-08, 14: 0.004867000117528998, 15: 1.8010753903318837e-05, 16: 0.0009349347036009077, 17: 0.0013226956405782426, 18: 0.0012305229588377286, 19: 0.0011384915379121425, 22: 7.769344821039499e-06, 23: 8.405018488215458e-06, 24: 5.297280559799658e-06, 25: 7.529201435661915e-05, 27: 9.181952970319408e-07, 28: 1.1300865194239272e-06, 32: 0.006911185370351954, 33: 0.0002746816546274783, 34: 1.5256168012223016e-05, 36: 9.902383126452162e-05, 37: 0.7564607752619541, 38: 1.3772929455479112e-05, 39: 3.531520373199772e-07, 40: 0.02305482445236007, 41: 0.0009819039245644646, 42: 2.6698294021390277e-05}, 14: {0: 0.1558864405573392, 3: 7.205949801515128e-05, 4: 1.472821015673188e-05, 6: 2.4711762007939393e-07, 8: 8.352575558683515e-06, 10: 6.711714561356339e-05, 11: 0.050257298866026666, 14: 0.00219198271362824, 15: 8.204304986635879e-06, 16: 0.00048731594679656486, 17: 0.0018983081339258884, 18: 0.000730677379050752, 19: 0.0005575467744231286, 22: 2.6688702968574546e-06, 23: 4.9917759256037575e-06, 24: 1.5815527685081213e-06, 25: 4.878101820367237e-05, 27: 2.4711762007939393e-07, 28: 4.448117161429091e-07, 32: 0.004021048490407883, 33: 0.00035095644403675526, 34: 5.782552309857818e-06, 36: 5.169700612060922e-05, 37: 0.7635637919309177, 38: 1.2355881003969697e-06, 39: 9.884704803175758e-08, 40: 0.019109358443119454, 41: 0.0006529341757737746, 42: 4.1021524933179396e-06}, 15: {0: 0.14339931813201687, 3: 3.44398112940024e-05, 4: 0.00013836345239169384, 6: 1.8126216470527577e-06, 8: 0.00011278534692772715, 10: 0.0005866852064294092, 11: 0.02935258794034622, 12: 1.007012026140421e-06, 14: 0.001552409739498073, 15: 0.0001504475967053789, 16: 0.00045980169113571623, 17: 0.0015302554749229836, 18: 0.0008199091916835308, 19: 0.006043280571273894, 22: 6.444876967298694e-06, 23: 9.868717856176125e-06, 24: 2.6182312679650947e-06, 25: 6.585858650958353e-05, 27: 6.042072156842526e-07, 28: 6.042072156842526e-07, 32: 0.010062466970005542, 33: 0.0010742804294866012, 34: 8.66030342480762e-06, 36: 2.436969103259819e-05, 37: 0.7946760885397198, 38: 1.248694912414122e-05, 39: 2.8196336731931787e-06, 40: 0.008704209149147343, 41: 0.0011524245627150978, 42: 1.3091156339825472e-05}, 16: {0: 0.18259398093363602, 3: 0.00018169043421561052, 4: 3.9809638234157654e-05, 6: 1.88671271251932e-07, 8: 3.075341721406491e-05, 10: 0.00010622192571483771, 11: 0.05783415946193972, 14: 0.0024308406588098917, 15: 4.9431873068006184e-05, 16: 0.0005554482225656878, 17: 0.000979769911611283, 18: 0.0004111147000579598, 19: 0.0019542570276275114, 22: 3.018740340030912e-06, 23: 1.7546428226429676e-05, 24: 1.509370170015456e-06, 25: 9.509032071097373e-05, 27: 3.77342542503864e-07, 32: 0.01027258470585394, 33: 0.0003614941557187017, 34: 1.3773002801391035e-05, 35: 3.77342542503864e-07, 36: 2.0565168566460588e-05, 37: 0.7055239552139683, 38: 4.339439238794436e-06, 39: 3.77342542503864e-07, 40: 0.036009421488601236, 41: 0.0004999788688176198, 42: 7.924193392581143e-06}, 17: {0: 0.18109387168274127, 3: 0.00014789133169592044, 4: 2.7144611513808184e-05, 6: 2.8080632600491224e-06, 8: 7.581770802132631e-05, 10: 0.00031637512729886777, 11: 0.028279069070868027, 14: 0.0020480141376624933, 15: 0.0002180929131971485, 16: 0.0006290061702510034, 17: 0.0007460088060863836, 18: 0.00020030851255017074, 19: 0.004925342958126161, 22: 3.744084346732163e-06, 23: 2.0592463907026897e-05, 24: 2.8080632600491224e-06, 25: 0.00013010693104894267, 32: 0.01594231114838555, 33: 0.0005391481459294316, 34: 9.360210866830407e-06, 36: 5.616126520098245e-06, 37: 0.7575789627388726, 38: 4.680105433415204e-06, 40: 0.006726247528904331, 41: 0.0003191831905589169, 42: 7.488168693464326e-06}, 18: {0: 0.2215389539136796, 3: 4.953060229212387e-05, 4: 2.286027798098025e-05, 8: 1.52401853206535e-05, 10: 7.239088027310413e-05, 11: 0.062031364301389905, 14: 0.009384144111192393, 15: 2.286027798098025e-05, 16: 0.002499390392587174, 17: 0.0013449463545476712, 18: 0.002072665203608876, 19: 0.0007467690807120215, 22: 1.1430138990490125e-05, 23: 1.1430138990490125e-05, 25: 9.906120458424774e-05, 32: 0.004137710314557425, 33: 0.000182882223847842, 34: 1.1430138990490125e-05, 36: 0.0003009936600829066, 37: 0.6365101499634236, 38: 1.1430138990490125e-05, 40: 0.058194647646915384, 41: 0.0006858083394294074, 42: 4.191050963179712e-05}, 19: {0: 0.17887695668001455, 3: 5.688023298143429e-05, 4: 9.811840189297415e-05, 6: 2.8440116490717145e-06, 8: 6.825627957772115e-05, 10: 0.0003057312522752093, 11: 0.03623839643247179, 14: 0.002959194120859119, 15: 8.532034947215144e-05, 16: 0.0004977020385875501, 17: 0.0019367719330178376, 18: 0.0005744903531124864, 19: 0.0053581179468511105, 22: 1.9908081543502002e-05, 23: 1.564206406989443e-05, 24: 8.532034947215144e-06, 25: 0.00013793456497997815, 32: 0.010676419730615217, 33: 0.0011788428285402256, 34: 1.706406989443029e-05, 35: 1.4220058245358572e-06, 36: 2.133008736803786e-05, 37: 0.7421718579359301, 38: 3.2706133964324715e-05, 39: 1.4220058245358572e-06, 40: 0.017315764925373133, 41: 0.0013409514925373134, 42: 1.4220058245358572e-06}}\n"
     ]
    }
   ],
   "source": [
    "# Count the frequency of different block types\n",
    "from collections import defaultdict\n",
    "block_type_frequencies = defaultdict(lambda: defaultdict(int))\n",
    "block_total_counts = defaultdict(int)\n",
    "for style_code, block_list in style_dict.items():\n",
    "    for block_matrix in block_list:\n",
    "        flattened_blocks = block_matrix.flatten()\n",
    "        block_total_counts[style_code] += len(flattened_blocks)\n",
    "\n",
    "        # Count the frequency of each block type\n",
    "        for block_type in flattened_blocks:\n",
    "            block_type_frequencies[style_code][block_type.item()] += 1\n",
    "sorted_block_frequency_dict = {}\n",
    "for style_code in sorted(block_type_frequencies.keys()):  # Sort by style code\n",
    "    block_counts = block_type_frequencies[style_code]\n",
    "    total_blocks = block_total_counts[style_code] if block_total_counts[style_code] > 0 else 1  # Avoid division by zero\n",
    "\n",
    "    sorted_block_frequency_dict[style_code] = {\n",
    "        block_type: block_counts[block_type] / total_blocks\n",
    "        for block_type in sorted(block_counts.keys())  # Sort by block type\n",
    "    }\n",
    "print(sorted_block_frequency_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10180378 315645\n",
      "2737764 138506\n",
      "3687378 41887\n",
      "17926060 781558\n",
      "3945713 43218\n",
      "3474925 266080\n",
      "10710129 326415\n",
      "3739435 190858\n",
      "15449400 386645\n",
      "6774780 201948\n",
      "1761697 15946\n",
      "4314247 199158\n",
      "9544919 831776\n",
      "893160 83745\n",
      "521919 12177\n",
      "910905 41099\n",
      "250311 5798\n",
      "1028908 43345\n",
      "167061 15274\n",
      "809361 7186\n"
     ]
    }
   ],
   "source": [
    "# check for frequency \n",
    "for i in block_type_frequencies.values():\n",
    "    print(i[37], i[40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 11, 14, 16, 17, 18, 19, 32, 33, 37, 40, 41]\n"
     ]
    }
   ],
   "source": [
    "# check blocks that are most commonly used\n",
    "threshold = 0.001\n",
    "filtered_block_frequency_dict = {\n",
    "    style_code: {\n",
    "        block_type: freq for block_type, freq in block_counts.items() if freq >= threshold\n",
    "    }\n",
    "    for style_code, block_counts in sorted_block_frequency_dict.items()\n",
    "}\n",
    "common_block_index = {\n",
    "    style_code: [block_type for block_type, freq in block_counts.items() if freq >= threshold]\n",
    "    for style_code, block_counts in sorted_block_frequency_dict.items()\n",
    "}\n",
    "\n",
    "common_block_list = []\n",
    "for i in common_block_index.values():\n",
    "    for j in i:\n",
    "        if j not in common_block_list:\n",
    "            common_block_list.append(j)\n",
    "common_block_list.sort()\n",
    "print(common_block_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_index_to_name = {\n",
    "    0: \"AIR\",\n",
    "    1: \"BONE_BLOCK\",\n",
    "    2: \"BROWN_MUSHROOM\",\n",
    "    3: \"BROWN_MUSHROOM_BLOCK\",\n",
    "    4: \"CACTUS\",\n",
    "    5: \"CHEST\",\n",
    "    6: \"CLAY\",\n",
    "    7: \"COAL_ORE\",\n",
    "    8: \"COBBLESTONE\",\n",
    "    9: \"DEADBUSH\",\n",
    "    10: \"DIAMOND_ORE\",\n",
    "    11: \"DIRT\",\n",
    "    12: \"DOUBLE_PLANT\",\n",
    "    13: \"EMERALD_ORE\",\n",
    "    14: \"FLOWING_LAVA\",\n",
    "    15: \"FLOWING_WATER\",\n",
    "    16: \"GOLD_ORE\",\n",
    "    17: \"GRASS\",\n",
    "    18: \"GRAVEL\",\n",
    "    19: \"IRON_ORE\",\n",
    "    20: \"LAPIS_ORE\",\n",
    "    21: \"LAVA\",\n",
    "    22: \"LEAVES\",\n",
    "    23: \"LEAVES2\",\n",
    "    24: \"LOG\",\n",
    "    25: \"LOG2\",\n",
    "    26: \"MOB_SPAWNER\",\n",
    "    27: \"MONSTER_EGG\",\n",
    "    28: \"MOSSY_COBBLESTONE\",\n",
    "    29: \"PUMPKIN\",\n",
    "    30: \"REDSTONE_ORE\",\n",
    "    31: \"RED_FLOWER\",\n",
    "    32: \"RED_MUSHROOM_BLOCK\",\n",
    "    33: \"REEDS\",\n",
    "    34: \"SAND\",\n",
    "    35: \"SANDSTONE\",\n",
    "    36: \"SNOW_LAYER\",\n",
    "    37: \"STONE\",\n",
    "    38: \"TALLGRASS\",\n",
    "    39: \"VINE\",\n",
    "    40: \"WATER\",\n",
    "    41: \"WATERLILY\",\n",
    "    42: \"YELLOW_FLOWER\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histograms saved in directory: /root/autodl-tmp/minecraft_diffusion-master/Dual_codebook_2.0/Block_type_histograms_V2\n"
     ]
    }
   ],
   "source": [
    "# Histogram of block types\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "\n",
    "histogram_dir = os.path.join(os.getcwd(), \"Block_type_histograms_V2\")\n",
    "os.makedirs(histogram_dir, exist_ok=True)\n",
    "for style_code, block_data in filtered_block_frequency_dict.items():\n",
    "    block_types = list(block_data.keys())  # Block type labels\n",
    "    frequencies = list(block_data.values())  # Corresponding frequencies\n",
    "\n",
    "    # Convert block types to integers for correct sorting\n",
    "    block_types = [int(bt) for bt in block_types]\n",
    "\n",
    "    # Sort block types and frequencies accordingly\n",
    "    sorted_indices = sorted(range(len(block_types)), key=lambda k: block_types[k])\n",
    "    block_types = [block_types[i] for i in sorted_indices]\n",
    "    frequencies = [frequencies[i] for i in sorted_indices]\n",
    "\n",
    "    #x_labels = [f\"{bt}\\n{block_index_to_name.get(bt, 'UNKNOWN')}\" for bt in block_types]\n",
    "    x_labels = [block_index_to_name.get(bt, 'UNKNOWN') for bt in block_types]\n",
    "\n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(block_types, frequencies, color=\"royalblue\", alpha=0.7)\n",
    "    plt.xlabel(\"Block Type\")\n",
    "    plt.ylabel(\"Frequency Ratio\")\n",
    "    plt.title(f\"Block Type Frequency for Style Code {style_code}\")\n",
    "    plt.xticks(block_types, x_labels, rotation=90, fontsize=6)  # Ensure block types are properly labeled on x-axis\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Save the histogram image\n",
    "    save_path = os.path.join(histogram_dir, f\"histogram_style_code_{style_code}.png\")\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "print(f\"Histograms saved in directory: {histogram_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biome Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ...\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']]\n",
      "\n",
      " [['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ...\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']]\n",
      "\n",
      " [['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ...\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']]\n",
      "\n",
      " ...\n",
      "\n",
      " [['forest' 'forest' 'forest' ... 'forest' 'forest' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'plains']\n",
      "  ...\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'plains']]\n",
      "\n",
      " [['forest' 'forest' 'forest' ... 'forest' 'forest' 'forest']\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'forest']\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'forest']\n",
      "  ...\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'forest']\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'forest']\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'forest']]\n",
      "\n",
      " [['forest' 'forest' 'forest' ... 'forest' 'forest' 'forest']\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'forest']\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'forest']\n",
      "  ...\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'forest']\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'forest']\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'forest']]]\n"
     ]
    }
   ],
   "source": [
    "# check for biome data\n",
    "for batch in train_loader:\n",
    "    voxels, biomes = batch\n",
    "    biome_value = block_converter.convert_to_original_biomes(biomes)\n",
    "    print(biome_value[2])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store style code and its corresponding biome chunks and block chunks \n",
    "biome_dict = {}\n",
    "\n",
    "for batch in train_loader:\n",
    "    voxels, biomes = batch  # Now unpack both\n",
    "    for sample_idx in range(len(voxels)):\n",
    "        sample = voxels[sample_idx].unsqueeze(0).cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            style_indices, struct_indices = encode_and_quantize(fqgan, sample)\n",
    "            reconstructed, binary_reconstructed = decode_from_indices(\n",
    "                style_indices, struct_indices, fqgan, two_stage=True\n",
    "            )\n",
    "\n",
    "        biome_tensor = biomes[sample_idx]\n",
    "        biome_names = block_converter.convert_to_original_biomes(biome_tensor)\n",
    "\n",
    "        for i in range(style_indices.shape[1]):  \n",
    "            for j in range(style_indices.shape[2]):  \n",
    "                for k in range(style_indices.shape[3]):  \n",
    "                    style_code = style_indices[0, i, j, k].item()\n",
    "\n",
    "                    x_start, y_start, z_start = i * 4, j * 4, k * 4\n",
    "                    x_end, y_end, z_end = x_start + 4, y_start + 4, z_start + 4\n",
    "\n",
    "                    block = reconstructed[x_start:x_end, y_start:y_end, z_start:z_end]\n",
    "                    biome_chunk = biome_names[x_start:x_end, y_start:y_end, z_start:z_end]\n",
    "\n",
    "                    # Store both block and corresponding biome values\n",
    "                    if style_code not in biome_dict:\n",
    "                        biome_dict[style_code] = []\n",
    "\n",
    "                    biome_dict[style_code].append({\n",
    "                        \"block\": block,\n",
    "                        \"biome\": biome_chunk\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['swampland' 'swampland' 'swampland' 'swampland']\n",
      "  ['swampland' 'swampland' 'swampland' 'swampland']\n",
      "  ['swampland' 'swampland' 'swampland' 'swampland']\n",
      "  ['swampland' 'swampland' 'swampland' 'swampland']]\n",
      "\n",
      " [['swampland' 'swampland' 'swampland' 'swampland']\n",
      "  ['swampland' 'swampland' 'swampland' 'swampland']\n",
      "  ['swampland' 'swampland' 'swampland' 'swampland']\n",
      "  ['swampland' 'swampland' 'swampland' 'swampland']]\n",
      "\n",
      " [['swampland' 'swampland' 'swampland' 'swampland']\n",
      "  ['swampland' 'swampland' 'swampland' 'swampland']\n",
      "  ['swampland' 'swampland' 'swampland' 'swampland']\n",
      "  ['swampland' 'swampland' 'swampland' 'swampland']]\n",
      "\n",
      " [['swampland' 'swampland' 'swampland' 'swampland']\n",
      "  ['swampland' 'swampland' 'swampland' 'swampland']\n",
      "  ['swampland' 'swampland' 'swampland' 'swampland']\n",
      "  ['swampland' 'swampland' 'swampland' 'swampland']]]\n"
     ]
    }
   ],
   "source": [
    "# example for the first sample in index 0.\n",
    "print(biome_dict[0][0][\"biome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{np.str_('beaches'): 0.012664664858581944, np.str_('birch_forest'): 0.04071086977991507, np.str_('cave'): 0.3224620340193419, np.str_('desert'): 0.042630535360211504, np.str_('extreme_hills'): 0.15213599000235506, np.str_('forest'): 0.14629722481786206, np.str_('mutated_extreme_hills'): 0.00047291291565056863, np.str_('ocean'): 0.02534025039694297, np.str_('plains'): 0.12334860329253747, np.str_('river'): 0.02441863998602153, np.str_('savanna'): 0.029020994294657033, np.str_('swampland'): 0.04432703922328327, np.str_('taiga'): 0.03617024105263958}\n"
     ]
    }
   ],
   "source": [
    "# find the frequency of biome for each code index\n",
    "from collections import defaultdict\n",
    "\n",
    "biome_frequencies = defaultdict(lambda: defaultdict(int))\n",
    "biome_total_counts = defaultdict(int)\n",
    "\n",
    "\n",
    "for style_code, entries in biome_dict.items():\n",
    "    for entry in entries:\n",
    "        biome_matrix = entry[\"biome\"]  \n",
    "        flattened_biomes = biome_matrix.flatten()\n",
    "\n",
    "        # Update total biome count for the style code\n",
    "        biome_total_counts[style_code] += len(flattened_biomes)\n",
    "\n",
    "        # Count occurrences of each biome type\n",
    "        for biome in flattened_biomes:\n",
    "            biome_frequencies[style_code][biome] += 1\n",
    "\n",
    "sorted_biome_frequency_dict = {}\n",
    "for style_code in sorted(biome_frequencies.keys()):\n",
    "    biome_counts = biome_frequencies[style_code]\n",
    "    total_biomes = biome_total_counts[style_code] if biome_total_counts[style_code] > 0 else 1\n",
    "\n",
    "    sorted_biome_frequency_dict[style_code] = {\n",
    "        biome: biome_counts[biome] / total_biomes\n",
    "        for biome in sorted(biome_counts.keys())  # Sort by biome name\n",
    "    }\n",
    "\n",
    "print(sorted_biome_frequency_dict[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {np.str_('beaches'): 0.012664664858581944, np.str_('birch_forest'): 0.04071086977991507, np.str_('cave'): 0.3224620340193419, np.str_('desert'): 0.042630535360211504, np.str_('extreme_hills'): 0.15213599000235506, np.str_('forest'): 0.14629722481786206, np.str_('mutated_extreme_hills'): 0.00047291291565056863, np.str_('ocean'): 0.02534025039694297, np.str_('plains'): 0.12334860329253747, np.str_('river'): 0.02441863998602153, np.str_('savanna'): 0.029020994294657033, np.str_('swampland'): 0.04432703922328327, np.str_('taiga'): 0.03617024105263958}, 1: {np.str_('beaches'): 0.029120701688594434, np.str_('birch_forest'): 0.032540730439492904, np.str_('cave'): 0.1486817296373845, np.str_('desert'): 0.014762180068127129, np.str_('extreme_hills'): 0.10217897434294822, np.str_('forest'): 0.12550913049366125, np.str_('mutated_extreme_hills'): 0.0002714928591518485, np.str_('ocean'): 0.2171942873214788, np.str_('plains'): 0.1211893340416888, np.str_('river'): 0.07409541444003459, np.str_('savanna'): 0.017540261674844005, np.str_('swampland'): 0.09189284560975863, np.str_('taiga'): 0.02502291738283488}, 2: {np.str_('beaches'): 0.02980474686194972, np.str_('birch_forest'): 0.03586546170322241, np.str_('cave'): 0.111150997355542, np.str_('desert'): 0.07502826937574039, np.str_('extreme_hills'): 0.1028406983283675, np.str_('forest'): 0.1201792487824724, np.str_('mutated_extreme_hills'): 0.00020715918201289922, np.str_('ocean'): 0.24832627346806907, np.str_('plains'): 0.13344641083629488, np.str_('river'): 0.03707775424489356, np.str_('savanna'): 0.02084380347249644, np.str_('swampland'): 0.053080614088619256, np.str_('taiga'): 0.03214856230031949}, 3: {np.str_('beaches'): 0.03713339101921851, np.str_('birch_forest'): 0.047485540577466075, np.str_('cave'): 0.19168412423718006, np.str_('desert'): 0.025711016486018763, np.str_('extreme_hills'): 0.15817697422351762, np.str_('forest'): 0.18330164404772747, np.str_('mutated_extreme_hills'): 0.0013292421896347573, np.str_('ocean'): 0.03528042171418162, np.str_('plains'): 0.12490037799435286, np.str_('river'): 0.07793856453228891, np.str_('savanna'): 0.01757901448219328, np.str_('swampland'): 0.0637353128700246, np.str_('taiga'): 0.035744375626195465}, 4: {np.str_('beaches'): 0.023657995610374356, np.str_('birch_forest'): 0.04372237163436273, np.str_('cave'): 0.04850227209496429, np.str_('desert'): 0.010523895638195925, np.str_('extreme_hills'): 0.11762728368728555, np.str_('forest'): 0.17233917586324152, np.str_('mutated_extreme_hills'): 0.0007129277566539924, np.str_('ocean'): 0.2706285356579802, np.str_('plains'): 0.11846772697765, np.str_('river'): 0.058222433460076044, np.str_('savanna'): 0.02508385112368234, np.str_('swampland'): 0.07390877616000495, np.str_('taiga'): 0.03660275433552815}, 5: {np.str_('beaches'): 0.01814133863552468, np.str_('birch_forest'): 0.06803185509580859, np.str_('cave'): 0.10747004933051445, np.str_('desert'): 0.04834264572636666, np.str_('extreme_hills'): 0.09578026947212993, np.str_('forest'): 0.24044535890466123, np.str_('mutated_extreme_hills'): 0.00013423269237222726, np.str_('ocean'): 0.04680106714990436, np.str_('plains'): 0.20963895600523508, np.str_('river'): 0.03147756636128729, np.str_('savanna'): 0.03221427312997081, np.str_('swampland'): 0.041716479579851674, np.str_('taiga'): 0.05980590791637303}, 6: {np.str_('beaches'): 0.01549413973600364, np.str_('birch_forest'): 0.0534251251706873, np.str_('cave'): 0.019048702776513427, np.str_('desert'): 0.0028191852526172055, np.str_('extreme_hills'): 0.11777281520254892, np.str_('forest'): 0.1850449476558944, np.str_('mutated_extreme_hills'): 0.00032003868912152935, np.str_('ocean'): 0.3322627446517979, np.str_('plains'): 0.10470385753299954, np.str_('river'): 0.07886322257624033, np.str_('savanna'): 0.016287835685025034, np.str_('swampland'): 0.024780951297223488, np.str_('taiga'): 0.04917643377332726}, 7: {np.str_('beaches'): 0.02931348069052115, np.str_('birch_forest'): 0.021098924535355182, np.str_('cave'): 0.04234761854553471, np.str_('desert'): 0.025847815043085684, np.str_('extreme_hills'): 0.05343235691623453, np.str_('forest'): 0.06757359072914969, np.str_('mutated_extreme_hills'): 0.00013838699473769974, np.str_('ocean'): 0.5398728277435372, np.str_('plains'): 0.054918070106492085, np.str_('river'): 0.05797875909400251, np.str_('savanna'): 0.006280133618334659, np.str_('swampland'): 0.08490012173262913, np.str_('taiga'): 0.016297914250385806}, 8: {np.str_('beaches'): 0.017398688860810488, np.str_('birch_forest'): 0.02668287278653702, np.str_('cave'): 0.04614623963083008, np.str_('desert'): 0.006346252449229981, np.str_('extreme_hills'): 0.03863259719093922, np.str_('forest'): 0.10559385815524913, np.str_('mutated_extreme_hills'): 8.469299932245601e-05, np.str_('ocean'): 0.5784394513724844, np.str_('plains'): 0.056921707044626346, np.str_('river'): 0.05023668259810654, np.str_('savanna'): 0.007694473438444213, np.str_('swampland'): 0.042874114157007086, np.str_('taiga'): 0.022948369316413045}, 9: {np.str_('beaches'): 0.0202518830582403, np.str_('birch_forest'): 0.0823543752281767, np.str_('cave'): 0.044357550487097205, np.str_('desert'): 0.047261399227561826, np.str_('extreme_hills'): 0.08585448571373672, np.str_('forest'): 0.3149667102203947, np.str_('mutated_extreme_hills'): 0.0001501162500240186, np.str_('ocean'): 0.010285965451645755, np.str_('plains'): 0.1644319360913091, np.str_('river'): 0.057086207559133795, np.str_('savanna'): 0.030760020559921605, np.str_('swampland'): 0.05670851507407336, np.str_('taiga'): 0.08553083507868493}, 10: {np.str_('beaches'): 0.00784908933217693, np.str_('birch_forest'): 0.07856183716011625, np.str_('cave'): 0.11353752986107941, np.str_('desert'): 0.02420611372316307, np.str_('extreme_hills'): 0.0910188524216004, np.str_('forest'): 0.28375252963284187, np.str_('mutated_extreme_hills'): 8.083413216475708e-05, np.str_('ocean'): 0.00875728458179273, np.str_('plains'): 0.18532965110086577, np.str_('river'): 0.028476628474916693, np.str_('savanna'): 0.032718423335007074, np.str_('swampland'): 0.07891066021515193, np.str_('taiga'): 0.06680056602912311}, 11: {np.str_('beaches'): 0.05281861580847425, np.str_('birch_forest'): 0.042688936918107545, np.str_('cave'): 0.19228576857305718, np.str_('desert'): 0.1417590965228957, np.str_('extreme_hills'): 0.118076667300019, np.str_('forest'): 0.15472401672050162, np.str_('mutated_extreme_hills'): 0.0002464136424092723, np.str_('ocean'): 0.03170720121603648, np.str_('plains'): 0.10365107828234847, np.str_('river'): 0.06338471404142124, np.str_('savanna'): 0.023391482994489836, np.str_('swampland'): 0.03750831274938248, np.str_('taiga'): 0.037757695230856925}, 12: {np.str_('beaches'): 0.032008311922173176, np.str_('birch_forest'): 0.07009821683466193, np.str_('cave'): 0.07323714781386564, np.str_('desert'): 0.059840075103761776, np.str_('extreme_hills'): 0.10051441684783802, np.str_('forest'): 0.23395260996552253, np.str_('mutated_extreme_hills'): 0.00024705184794782267, np.str_('ocean'): 0.027816665568658454, np.str_('plains'): 0.17636756922941785, np.str_('river'): 0.05848952500164701, np.str_('savanna'): 0.02470243977424951, np.str_('swampland'): 0.0766999912159343, np.str_('taiga'): 0.06602597887432197}, 13: {np.str_('beaches'): 0.009667818740399386, np.str_('birch_forest'): 0.0771889400921659, np.str_('cave'): 0.009370199692780337, np.str_('desert'): 0.022532642089093703, np.str_('extreme_hills'): 0.04749423963133641, np.str_('forest'): 0.25304339477726573, np.str_('ocean'): 0.026171274961597542, np.str_('plains'): 0.20950460829493087, np.str_('river'): 0.10360983102918586, np.str_('savanna'): 0.042876344086021506, np.str_('swampland'): 0.14050499231950844, np.str_('taiga'): 0.05803571428571429}, 14: {np.str_('beaches'): 0.008585286368730462, np.str_('birch_forest'): 0.07605962820235544, np.str_('cave'): 0.05671339533237648, np.str_('desert'): 0.012169810857041741, np.str_('extreme_hills'): 0.05997289180314976, np.str_('forest'): 0.28399031183028256, np.str_('mutated_extreme_hills'): 0.0005621940189929416, np.str_('ocean'): 0.09127818957550972, np.str_('plains'): 0.13456590019858045, np.str_('river'): 0.05861932521971648, np.str_('savanna'): 0.024306443050667505, np.str_('swampland'): 0.1311872063073868, np.str_('taiga'): 0.06198941723520969}, 15: {np.str_('beaches'): 0.029045505309301133, np.str_('birch_forest'): 0.04831225140645716, np.str_('cave'): 0.11876948435876303, np.str_('desert'): 0.0531224991340285, np.str_('extreme_hills'): 0.11293462811003213, np.str_('forest'): 0.24127235102304082, np.str_('mutated_extreme_hills'): 0.0008013969016136931, np.str_('ocean'): 0.13322299661972503, np.str_('plains'): 0.10387962697531085, np.str_('river'): 0.04058569534525388, np.str_('savanna'): 0.02157164869029276, np.str_('swampland'): 0.04910767609082548, np.str_('taiga'): 0.04737424003535553}, 16: {np.str_('beaches'): 0.022919248101563743, np.str_('birch_forest'): 0.05416037892580004, np.str_('cave'): 0.2104512636628571, np.str_('desert'): 0.05840071022628945, np.str_('extreme_hills'): 0.09069705664504607, np.str_('forest'): 0.22900082029059746, np.str_('mutated_extreme_hills'): 0.0004304795065796305, np.str_('ocean'): 0.03860297935082825, np.str_('plains'): 0.13942430829081898, np.str_('river'): 0.03638914655166446, np.str_('savanna'): 0.026761980742207238, np.str_('swampland'): 0.04518607098109524, np.str_('taiga'): 0.04757555672465233}, 17: {np.str_('beaches'): 0.04234537412106389, np.str_('birch_forest'): 0.07343415622133904, np.str_('cave'): 0.07694512381534699, np.str_('desert'): 0.20602931060837665, np.str_('extreme_hills'): 0.10249398119841027, np.str_('forest'): 0.20474912106389484, np.str_('mutated_extreme_hills'): 0.00023287029960256803, np.str_('ocean'): 0.03763303462243962, np.str_('plains'): 0.0837377713237542, np.str_('river'): 0.030609905227759096, np.str_('savanna'): 0.018228370528890248, np.str_('swampland'): 0.04779812175175787, np.str_('taiga'): 0.07576285921736473}, 18: {np.str_('beaches'): 0.04211725603070175, np.str_('birch_forest'): 0.056186609100877194, np.str_('cave'): 0.12727521929824562, np.str_('desert'): 0.13887318393640352, np.str_('extreme_hills'): 0.08289645010964912, np.str_('forest'): 0.19103104440789473, np.str_('mutated_extreme_hills'): 8.223684210526316e-05, np.str_('ocean'): 0.03080283717105263, np.str_('plains'): 0.14412092242324562, np.str_('river'): 0.047384697094298246, np.str_('savanna'): 0.026475979989035087, np.str_('swampland'): 0.06325383771929824, np.str_('taiga'): 0.04949972587719298}, 19: {np.str_('beaches'): 0.045872522773309325, np.str_('birch_forest'): 0.04007524964759118, np.str_('cave'): 0.006463000035537024, np.str_('desert'): 0.10197052796171478, np.str_('extreme_hills'): 0.047480573093734824, np.str_('forest'): 0.14207169002238831, np.str_('mutated_extreme_hills'): 1.9545363010696643e-05, np.str_('ocean'): 0.3241078726353072, np.str_('plains'): 0.13812930146057167, np.str_('river'): 0.04700008291972187, np.str_('savanna'): 0.02326209147229889, np.str_('swampland'): 0.04909439818050439, np.str_('taiga'): 0.03445314443430981}}\n"
     ]
    }
   ],
   "source": [
    "print(sorted_biome_frequency_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'beaches', 1: 'birch_forest', 2: 'cave', 3: 'desert', 4: 'extreme_hills', 5: 'forest', 6: 'mutated_extreme_hills', 7: 'ocean', 8: 'plains', 9: 'river', 10: 'savanna', 11: 'swampland', 12: 'taiga'}\n"
     ]
    }
   ],
   "source": [
    "# Biome Mapping\n",
    "print(block_converter.index_to_biome)\n",
    "Biome_mapping = {0: 'beaches', 1: 'birch_forest', 2: 'cave', 3: 'desert', 4: 'extreme_hills', 5: 'forest', 6: 'mutated_extreme_hills', 7: 'ocean', 8: 'plains', 9: 'river', 10: 'savanna', 11: 'swampland', 12: 'taiga'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biome histograms saved in directory: /root/autodl-tmp/minecraft_diffusion-master/Dual_codebook_2.0/Biome_histograms_V1\n"
     ]
    }
   ],
   "source": [
    "# plot histogram of biome frequency. \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "histogram_dir = os.path.join(os.getcwd(), \"Biome_histograms_V1\")\n",
    "os.makedirs(histogram_dir, exist_ok=True)\n",
    "\n",
    "# Assume your biome frequency dictionary is called `sorted_biome_frequency_dict`\n",
    "for style_code, biome_data in sorted_biome_frequency_dict.items():\n",
    "    biome_names = list(biome_data.keys())         # Biome name labels (strings)\n",
    "    frequencies = list(biome_data.values())       # Corresponding frequency ratios\n",
    "\n",
    "    # Sort biome names and frequencies alphabetically\n",
    "    sorted_indices = sorted(range(len(biome_names)), key=lambda k: biome_names[k])\n",
    "    biome_names = [biome_names[i] for i in sorted_indices]\n",
    "    frequencies = [frequencies[i] for i in sorted_indices]\n",
    "\n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(biome_names, frequencies, color=\"seagreen\", alpha=0.75)\n",
    "    plt.xlabel(\"Biome Name\")\n",
    "    plt.ylabel(\"Frequency Ratio\")\n",
    "    plt.title(f\"Biome Frequency for Style Code {style_code}\")\n",
    "    plt.xticks(rotation=90, fontsize=7)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    # Save the histogram image\n",
    "    save_path = os.path.join(histogram_dir, f\"biome_histogram_style_code_{style_code}.png\")\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "print(f\"Biome histograms saved in directory: {histogram_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render the original dataset chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coords are the coordinates you want to highlight with a red box in the visualization. Helpful if you modify latent codes, so you can see where you expect the changes to show up.\n",
    "# X axis goes from 0 all the way on the left to 5 in the center\n",
    "# Y axis goes from 0 at the bottom to 5 at the top\n",
    "# Z axis goes from 0 at the center to 5 all the way on the right\n",
    "# Example:\n",
    "\n",
    "coords=[(0, 2, 5)]\n",
    "# coords = coords + [(x, y, 0) for x in [3, 4] for y in range(6)]\n",
    "# Don't forget: You need to convert from the one-hot encoded representaiton back into the original minecraft block IDs before rendering!\n",
    "converted_orig = block_converter.convert_to_original_blocks(sample.squeeze())\n",
    "plotter = visualizer.visualize_chunk(converted_orig, interactive=True, show_axis=False, highlight_latents=coords)\n",
    "plotter.show()\n",
    "\n",
    "# plotter.reset_camera()\n",
    "# plotter.show()\n",
    "# img = plotter.screenshot(window_size=(500, 500), \n",
    "#                                transparent_background=True, \n",
    "#                                return_img=True)\n",
    "# # Make the array C-contiguous and ensure correct format\n",
    "# img_array = np.ascontiguousarray(img)\n",
    "\n",
    "# # Save using imageio instead of plt.imsave\n",
    "# import imageio\n",
    "# imageio.imwrite('treesandcorner_highlight.png', img_array)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_coords = [(0, 2, 5)]\n",
    "  # Coordinates in the 6x6x6 latent space\n",
    "plotter = visualizer.visualize_latent_blocks(converted_orig, latent_coords, show_axis=False)\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_coords = [(2, 2, 5)]  # Coordinates in the 6x6x6 latent space\n",
    "plotter = visualizer.visualize_isolated_latent_blocks(converted_orig, latent_coords, show_axis=False)\n",
    "plotter.reset_camera()\n",
    "plotter.show()\n",
    "img = plotter.screenshot(window_size=(500, 500), \n",
    "                               transparent_background=True, \n",
    "                               return_img=True)\n",
    "# Make the array C-contiguous and ensure correct format\n",
    "img_array = np.ascontiguousarray(img)\n",
    "\n",
    "# Save using imageio instead of plt.imsave\n",
    "import imageio\n",
    "imageio.imwrite('treesandcorner_225.png', img_array)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render the reconstructed dataset chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct and convert from the style and structure matrices\n",
    "reconstructed, binary_reconstructed = decode_from_indices(style_indices, struct_indices, fqgan, two_stage=True)\n",
    "binary_reconstructed = (binary_reconstructed>0.5).float()\n",
    "binary_reconstructed = block_converter.convert_to_original_blocks(binary_reconstructed).squeeze()\n",
    "reconstructed = block_converter.convert_to_original_blocks(reconstructed)\n",
    "\n",
    "coords=[(0, 2, 5)]\n",
    "# coords=None\n",
    "fig = visualizer.visualize_chunk(reconstructed, highlight_latents=coords, show_axis=False, wireframe_highlight=True)\n",
    "fig.show()\n",
    "\n",
    "# fig.reset_camera()\n",
    "# fig.show()\n",
    "# img = fig.screenshot(window_size=(500, 500), \n",
    "#                                transparent_background=True, \n",
    "#                                return_img=True)\n",
    "# # Make the array C-contiguous and ensure correct format\n",
    "# img_array = np.ascontiguousarray(img)\n",
    "\n",
    "# # Save using imageio instead of plt.imsave\n",
    "# import imageio\n",
    "# imageio.imwrite('treesandcorner_recon_2codebook_highlight.png', img_array)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_coords = [(2, 2, 5)]  # Coordinates in the 6x6x6 latent space\n",
    "plotter = visualizer.visualize_isolated_latent_blocks(reconstructed, latent_coords, show_axis=False)\n",
    "plotter.reset_camera()\n",
    "plotter.show()\n",
    "img = plotter.screenshot(window_size=(500, 500), \n",
    "                               transparent_background=True, \n",
    "                               return_img=True)\n",
    "# Make the array C-contiguous and ensure correct format\n",
    "img_array = np.ascontiguousarray(img)\n",
    "\n",
    "# Save using imageio instead of plt.imsave\n",
    "import imageio\n",
    "imageio.imwrite('treesandcorner_recon_225.png', img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_reconstructed[binary_reconstructed != 5] = 217\n",
    "fig = visualizer.visualize_chunk(binary_reconstructed, highlight_latents=coords)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VQ reconstruction (single codebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct and convert from the style and structure matrices\n",
    "vq_reconstructed = vq_decode_from_indices(vq_latent_indices, vqgan)\n",
    "vq_reconstructed = block_converter.convert_to_original_blocks(vq_reconstructed)\n",
    "coords=None\n",
    "fig = visualizer.visualize_chunk(vq_reconstructed, highlight_latents=coords, show_axis=False)\n",
    "fig.show()\n",
    "\n",
    "# fig.reset_camera()\n",
    "# fig.show()\n",
    "# img = fig.screenshot(window_size=(500, 500), \n",
    "#                                transparent_background=True, \n",
    "#                                return_img=True)\n",
    "# # Make the array C-contiguous and ensure correct format\n",
    "# img_array = np.ascontiguousarray(img)\n",
    "\n",
    "# # Save using imageio instead of plt.imsave\n",
    "# import imageio\n",
    "# imageio.imwrite('treesandcorner_recon_singlecodebook.png', img_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots either style or structure codes next to the chunk\n",
    "NOTE: The slider can't take discrete values, if you slide it to layer 1 and nothing changes, keep moving the slider until you're past like 1.5, then it should change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize overlay\n",
    "plotter = visualizer.visualize_latent_space_with_blocks(\n",
    "    vq_latent_indices,  # Remove batch dimension\n",
    "    vq_reconstructed,         # Remove batch dimension\n",
    "    latent_type='style',\n",
    ")\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize overlay\n",
    "plotter = visualizer.visualize_latent_space_with_blocks(\n",
    "    style_indices,  # Remove batch dimension\n",
    "    reconstructed,         # Remove batch dimension\n",
    "    latent_type='style',\n",
    ")\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot both style and structure next to the chunk\n",
    "NOTE: This runs very slowly in notebook mode, so I'm rendering it a new window. Check to see if it opened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = visualizer.visualize_both_codes_with_blocks(\n",
    "    style_indices,\n",
    "    struct_indices,\n",
    "    reconstructed\n",
    ")\n",
    "plotter.show(interactive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make to change style indices, struct indices, or both\n",
    "Pass \"coords\" into this to specify the range of spatial coordinates you want to make modifications to. This will simply loop from 0 to max_code_value + 1 and set all of the values at those coordinates equal to that value\n",
    "\n",
    "NOTE: It currently only handles one max_code_value, so if your codebooks are different sizes, pass in the smaller value. If you try to go out of bounds, the CUDA kernel crashes and you must restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_latent_indices(indices, coords, code_value, max_code_value, mode='all'):\n",
    "    \"\"\"\n",
    "    Helper function to modify latent indices either all to the same value or randomly.\n",
    "    \n",
    "    Args:\n",
    "        indices: Original indices tensor [1,6,6,6]\n",
    "        coords: List of (x,y,z) coordinates to modify\n",
    "        code_value: Value to set for 'all' mode, or seed for random mode\n",
    "        max_code_value: Maximum possible code value\n",
    "        mode: Either 'all' or 'random'\n",
    "    \n",
    "    Returns:\n",
    "        Modified indices tensor\n",
    "    \"\"\"\n",
    "    modified = indices.clone()\n",
    "    \n",
    "    if mode == 'all':\n",
    "        # Set all specified coordinates to the same value\n",
    "        for coord in coords:\n",
    "            modified[0, coord[0], coord[1], coord[2]] = code_value\n",
    "    elif mode == 'random':\n",
    "        # Set a different random value for each coordinate\n",
    "        # Use the code_value as a seed for reproducibility\n",
    "        torch.manual_seed(code_value)\n",
    "        for coord in coords:\n",
    "            modified[0, coord[0], coord[1], coord[2]] = torch.randint(0, max_code_value + 1, (1,)).item()\n",
    "    \n",
    "    return modified\n",
    "\n",
    "\n",
    "\n",
    "def create_latent_modification_gif(fqgan, style_indices, struct_indices, coords, block_converter, \n",
    "                                 latent_type='style', max_code_value=31, duration=5.0, out_path='my_animation.gif', \n",
    "                                 show_axis=True, transparent_background=False, fps=3, mode='all', wireframe_highlight=True):\n",
    "    \"\"\"\n",
    "    Creates a GIF showing how modifying style or structure codes at specific coordinates affects reconstruction.\n",
    "    \n",
    "    Args:\n",
    "        fqgan: The trained FQGAN model\n",
    "        style_indices: Original style indices tensor [1,6,6,6]\n",
    "        struct_indices: Original structure indices tensor [1,6,6,6]\n",
    "        coords: List of (x,y,z) coordinates to modify\n",
    "        block_converter: BlockBiomeConverter instance for converting to block IDs\n",
    "        latent_type: Either 'style' or 'struct' to specify which codes to modify\n",
    "        max_code_value: Maximum value to try for codes\n",
    "        duration: Duration of output GIF in seconds\n",
    "        out_path: Path to save the output GIF\n",
    "        mode: Either 'all' or 'random' - determines how codes are modified\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    # First get base reconstruction\n",
    "    base_reconstruction, _ = decode_from_indices(style_indices, struct_indices, fqgan, two_stage=True)\n",
    "    base_reconstruction = block_converter.convert_to_original_blocks(base_reconstruction)\n",
    "    chunks.append(base_reconstruction)\n",
    "    \n",
    "    # For each possible code value\n",
    "    for code in range(max_code_value + 1):\n",
    "        if latent_type == 'style':\n",
    "            modified_style = modify_latent_indices(style_indices, coords, code, max_code_value, mode)\n",
    "            modified_struct = struct_indices\n",
    "            reconstruction, _ = decode_from_indices(modified_style, modified_struct, fqgan, two_stage=True)\n",
    "            \n",
    "        elif latent_type == 'both':\n",
    "            modified_style = modify_latent_indices(style_indices, coords, code, max_code_value, mode)\n",
    "            modified_struct = modify_latent_indices(struct_indices, coords, code, max_code_value, mode)\n",
    "            reconstruction, _ = decode_from_indices(modified_style, modified_struct, fqgan, two_stage=True)\n",
    "            \n",
    "        else:  # struct\n",
    "            modified_style = style_indices\n",
    "            modified_struct = modify_latent_indices(struct_indices, coords, code, max_code_value, mode)\n",
    "            reconstruction, _ = decode_from_indices(modified_style, modified_struct, fqgan, two_stage=True)\n",
    "        \n",
    "        # Convert to blocks and add to chunks\n",
    "        reconstruction = block_converter.convert_to_original_blocks(reconstruction)\n",
    "        chunks.append(reconstruction)\n",
    "    \n",
    "    # Create visualization\n",
    "    visualizer = MinecraftVisualizerPyVista()\n",
    "    visualizer.create_voxel_gif(chunks, duration=duration, output_path=out_path, \n",
    "                              highlight_latents=coords, show_axis=show_axis, \n",
    "                              transparent_background=transparent_background, fps=fps, wireframe_highlight=wireframe_highlight)\n",
    "    \n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_latent_modification_gif_single_codebook(vqgan, indices, coords, block_converter, max_code_value=31, duration=5.0, out_path='my_animation.gif', show_axis=True, transparent_background=False, fps=3, wireframe_highlight=True, mode='all'):\n",
    "    chunks = []\n",
    "    \n",
    "    # First get base reconstruction\n",
    "    base_reconstruction = vq_decode_from_indices(indices, vqgan)\n",
    "    base_reconstruction = block_converter.convert_to_original_blocks(base_reconstruction)\n",
    "    chunks.append(base_reconstruction)\n",
    "    \n",
    "    # For each possible code value\n",
    "    for code in range(max_code_value + 1):\n",
    "        # modified_indices = indices.clone()\n",
    "        modified_indices = modify_latent_indices(indices, coords, code, max_code_value, mode)\n",
    "        # Modify the structure codes at specified coordinates\n",
    "        # for coord in coords:\n",
    "        #     modified_indices[0, coord[0], coord[1], coord[2]] = code\n",
    "            \n",
    "        reconstruction = vq_decode_from_indices(modified_indices, vqgan)\n",
    "        \n",
    "        # Convert to blocks and add to chunks\n",
    "        reconstruction = block_converter.convert_to_original_blocks(reconstruction)\n",
    "        chunks.append(reconstruction)\n",
    "    \n",
    "    # Create visualization\n",
    "    visualizer = MinecraftVisualizerPyVista()\n",
    "    visualizer.create_voxel_gif(chunks, duration=duration, output_path=out_path, highlight_latents=coords, show_axis=show_axis, transparent_background=transparent_background, fps=fps, wireframe_highlight=wireframe_highlight)\n",
    "    \n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create style modification gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Pick a name for this sample\n",
    "sample_name = 'samplename'\n",
    "\n",
    "# Specify coordinate you want to modify\n",
    "# coords = [(x, y, z) for x in [0, 1, 2] for y in  range(6) for z in  [0, 1]] \n",
    "# coords = [(x, y, z) for x in range(6) for y in range(6) for z in range(6)] \n",
    "# coords = [(x, y, z) for x in [4, 5] for y in range(6) for z in [0, 1]] \n",
    "# coords = [(x, y, z) for x in [0, 1, 3] for y in [1, 2, 3, 4] for z in [0, 1, 2, 3]]\n",
    "# coords = [(x, y, z) for x in [3, 4, 5] for y in range(6) for z in [4, 5]] \n",
    "# coords = [(x, y, z) for x in [0] for y in range(6) for z in [4, 5]] \n",
    "# coords = [(0, 2, 5)]\n",
    "# coords = [(0, 4, 0)]\n",
    "coords = [(x, y, z) for x in [0, 1] for y in range(6) for z in [4, 5]] \n",
    "# coords = [(x, y, z) for x in [3, 4, 5] for y in range(4) for z in [4, 5]] \n",
    "# coords= [(x, y, z) for x in [4, 5] for y in range(6) for z in [0, 1, 2, 3]]\n",
    "\n",
    "\n",
    "\n",
    "max_style_code_value = fqgan_hparams.style_codebook_size - 1\n",
    "max_struct_code_value = fqgan_hparams.struct_codebook_size - 1\n",
    "# Run the gif creation\n",
    "gif_path = create_latent_modification_gif(\n",
    "    fqgan,\n",
    "    style_indices,\n",
    "    struct_indices,\n",
    "    coords,\n",
    "    block_converter,\n",
    "    latent_type='style',\n",
    "    max_code_value=max_style_code_value,  # Assuming 32 possible codes (0-31)\n",
    "    duration=100.0,  # 5 second animation,\n",
    "    out_path=f'visualizations/{sample_name}_style_change.gif',\n",
    "    transparent_background=True,\n",
    "    show_axis=False,\n",
    "    fps=1,\n",
    "    # mode='random'\n",
    "    mode='all'\n",
    ")\n",
    "\n",
    "gif_path = create_latent_modification_gif(\n",
    "    fqgan,\n",
    "    style_indices,\n",
    "    struct_indices,\n",
    "    coords,\n",
    "    block_converter,\n",
    "    latent_type='struct',\n",
    "    max_code_value=max_struct_code_value,  # Assuming 32 possible codes (0-31)\n",
    "    duration=10.0,  # 5 second animation,\n",
    "    out_path=f'visualizations/{sample_name}_struct_change.gif',\n",
    "    transparent_background=True,\n",
    "    show_axis=False,\n",
    "    fps=1,\n",
    "    # mode='random'\n",
    "    mode='all'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create structure modification gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_path = create_latent_modification_gif(\n",
    "    fqgan,\n",
    "    style_indices,\n",
    "    struct_indices,\n",
    "    coords,\n",
    "    block_converter,\n",
    "    latent_type='struct',\n",
    "    max_code_value=31,  # Assuming 32 possible codes (0-31)\n",
    "    duration=10.0,  # 5 second animation,\n",
    "    out_path=f'visualizations/{sample_name}_struct_change.gif',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create both modification gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_path = create_latent_modification_gif(\n",
    "    fqgan,\n",
    "    style_indices,\n",
    "    struct_indices,\n",
    "    coords,\n",
    "    block_converter,\n",
    "    latent_type='both',\n",
    "    max_code_value=19,  # Assuming 32 possible codes (0-31)\n",
    "    duration=10.0,  # 5 second animation,\n",
    "    out_path=f'visualizations/{sample_name}_both_change.gif',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create single codebook modification gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coords = [(x, y, z) for x in [0, 1] for y in range(6) for z in [4, 5]] \n",
    "sample_name = 'treesandcorner_asad_vqgan_rand_1_wireframe'\n",
    "coords = [(x, y, z) for x in [0, 1] for y in range(6) for z in [4, 5]] \n",
    "# coords = [(x, y, z) for x in [3, 4, 5] for y in range(4) for z in [4, 5]] \n",
    "gif_path = create_latent_modification_gif_single_codebook(\n",
    "    vqgan,\n",
    "    vq_latent_indices,\n",
    "    coords,\n",
    "    block_converter,\n",
    "    max_code_value=100,  # Assuming 32 possible codes (0-31)\n",
    "    duration=20.0,  # 5 second animation,\n",
    "    out_path=f'visualizations/{sample_name}_vqgan_change.gif',\n",
    "    transparent_background=True,\n",
    "    show_axis=False,\n",
    "    fps=1,\n",
    "    mode='random'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
