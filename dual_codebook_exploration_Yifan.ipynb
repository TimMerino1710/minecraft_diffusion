{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from sampler_utils import retrieve_autoencoder_components_state_dicts, latent_ids_to_onehot3d, get_latent_loaders\n",
    "from models3d import VQAutoEncoder, Generator\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, fixed\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from PIL import Image\n",
    "import torch.distributions as dists\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from models3d import BiomeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "#matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockBiomeConverter:\n",
    "    def __init__(self, block_mappings=None, biome_mappings=None):\n",
    "        \"\"\"\n",
    "        Initialize with pre-computed mappings for both blocks and biomes\n",
    "        \n",
    "        Args:\n",
    "            block_mappings: dict containing 'index_to_block' and 'block_to_index'\n",
    "            biome_mappings: dict containing 'index_to_biome' and 'biome_to_index'\n",
    "        \"\"\"\n",
    "        self.index_to_block = block_mappings['index_to_block'] if block_mappings else None\n",
    "        self.block_to_index = block_mappings['block_to_index'] if block_mappings else None\n",
    "        self.index_to_biome = biome_mappings['index_to_biome'] if biome_mappings else None\n",
    "        self.biome_to_index = biome_mappings['biome_to_index'] if biome_mappings else None\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataset(cls, data_path):\n",
    "        \"\"\"Create mappings from a dataset file\"\"\"\n",
    "        data = np.load(data_path, allow_pickle=True)\n",
    "        voxels = data['voxels']\n",
    "        biomes = data['biomes']\n",
    "        \n",
    "        # Create block mappings (blocks are integers)\n",
    "        unique_blocks = np.unique(voxels)\n",
    "        block_to_index = {int(block): idx for idx, block in enumerate(unique_blocks)}\n",
    "        index_to_block = {idx: int(block) for idx, block in enumerate(unique_blocks)}\n",
    "        \n",
    "        # Create biome mappings (biomes are strings)\n",
    "        unique_biomes = np.unique(biomes)\n",
    "        biome_to_index = {str(biome): idx for idx, biome in enumerate(unique_biomes)}\n",
    "        index_to_biome = {idx: str(biome) for idx, biome in enumerate(unique_biomes)}\n",
    "        \n",
    "        block_mappings = {'index_to_block': index_to_block, 'block_to_index': block_to_index}\n",
    "        biome_mappings = {'index_to_biome': index_to_biome, 'biome_to_index': biome_to_index}\n",
    "        \n",
    "        return cls(block_mappings, biome_mappings)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_arrays(cls, voxels, biomes):\n",
    "        \"\"\"Create mappings directly from numpy arrays\"\"\"\n",
    "        # Create block mappings (blocks are integers)\n",
    "        unique_blocks = np.unique(voxels)\n",
    "        block_to_index = {int(block): idx for idx, block in enumerate(unique_blocks)}\n",
    "        index_to_block = {idx: int(block) for idx, block in enumerate(unique_blocks)}\n",
    "        \n",
    "        # Create biome mappings (biomes are strings)\n",
    "        unique_biomes = np.unique(biomes)\n",
    "        biome_to_index = {str(biome): idx for idx, biome in enumerate(unique_biomes)}\n",
    "        index_to_biome = {idx: str(biome) for idx, biome in enumerate(unique_biomes)}\n",
    "        \n",
    "        block_mappings = {'index_to_block': index_to_block, 'block_to_index': block_to_index}\n",
    "        biome_mappings = {'index_to_biome': index_to_biome, 'biome_to_index': biome_to_index}\n",
    "        \n",
    "        return cls(block_mappings, biome_mappings)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_mappings(cls, path):\n",
    "        \"\"\"Load pre-saved mappings\"\"\"\n",
    "        mappings = torch.load(path)\n",
    "        return cls(mappings['block_mappings'], mappings['biome_mappings'])\n",
    "    \n",
    "    def save_mappings(self, path):\n",
    "        \"\"\"Save mappings for later use\"\"\"\n",
    "        torch.save({\n",
    "            'block_mappings': {\n",
    "                'index_to_block': self.index_to_block,\n",
    "                'block_to_index': self.block_to_index\n",
    "            },\n",
    "            'biome_mappings': {\n",
    "                'index_to_biome': self.index_to_biome,\n",
    "                'biome_to_index': self.biome_to_index\n",
    "            }\n",
    "        }, path)\n",
    "    \n",
    "    def convert_to_original_blocks(self, data):\n",
    "        \"\"\"\n",
    "        Convert from indices back to original block IDs.\n",
    "        Handles both one-hot encoded and already-indexed data.\n",
    "        \n",
    "        Args:\n",
    "            data: torch.Tensor of either:\n",
    "                - one-hot encoded blocks [B, C, H, W, D] or [C, H, W, D]\n",
    "                - indexed blocks [B, H, W, D] or [H, W, D]\n",
    "        Returns:\n",
    "            torch.Tensor of original block IDs with shape [B, H, W, D] or [H, W, D]\n",
    "        \"\"\"\n",
    "        # If one-hot encoded (dim == 5 or first dim == num_blocks), convert to indices first\n",
    "        if len(data.shape) == 5 or (len(data.shape) == 4 and data.shape[0] == len(self.block_to_index)):\n",
    "            data = torch.argmax(data, dim=1 if len(data.shape) == 5 else 0)\n",
    "        \n",
    "        # Now convert indices to original blocks\n",
    "        if len(data.shape) == 4:  # Batch dimension present\n",
    "            return torch.tensor([[[[self.index_to_block[int(b)] \n",
    "                                for b in row]\n",
    "                                for row in layer]\n",
    "                                for layer in slice_]\n",
    "                                for slice_ in data])\n",
    "        else:  # No batch dimension\n",
    "            return torch.tensor([[[self.index_to_block[int(b)] \n",
    "                                for b in row]\n",
    "                                for row in layer]\n",
    "                                for layer in data])\n",
    "\n",
    "    def convert_to_original_biomes(self, data):\n",
    "        \"\"\"\n",
    "        Convert from indices back to original biome strings.\n",
    "        Handles both one-hot encoded and already-indexed data.\n",
    "        \n",
    "        Args:\n",
    "            data: torch.Tensor of either:\n",
    "                - one-hot encoded biomes [B, C, H, W, D] or [C, H, W, D]\n",
    "                - indexed biomes [B, H, W, D] or [H, W, D]\n",
    "        Returns:\n",
    "            numpy array of original biome strings with shape [B, H, W, D] or [H, W, D]\n",
    "        \"\"\"\n",
    "        # If one-hot encoded (dim == 5 or first dim == num_biomes), convert to indices first\n",
    "        if len(data.shape) == 5 or (len(data.shape) == 4 and data.shape[0] == len(self.biome_to_index)):\n",
    "            data = torch.argmax(data, dim=1 if len(data.shape) == 5 else 0)\n",
    "        \n",
    "        # Now convert indices to original biomes\n",
    "        if len(data.shape) == 4:  # Batch dimension present\n",
    "            return np.array([[[[self.index_to_biome[int(b)] \n",
    "                            for b in row]\n",
    "                            for row in layer]\n",
    "                            for layer in slice_]\n",
    "                            for slice_ in data])\n",
    "        else:  # No batch dimension\n",
    "            return np.array([[[self.index_to_biome[int(b)] \n",
    "                            for b in row]\n",
    "                            for row in layer]\n",
    "                            for layer in data])\n",
    "        \n",
    "    def get_air_block_index(self):\n",
    "        \"\"\"\n",
    "        Find the one-hot index corresponding to the air block (ID 5).\n",
    "        Returns:\n",
    "            int: The index where air blocks are encoded in one-hot format\n",
    "        \"\"\"\n",
    "        # Find the index that maps to block ID 5 (air) in our index_to_block mapping\n",
    "        for idx, block_id in self.index_to_block.items():\n",
    "            if block_id == 5:  # Air block ID\n",
    "                return idx\n",
    "        raise ValueError(\"Air block (ID 5) not found in block mappings!\")\n",
    "    \n",
    "    def get_blockid_indices(self, block_ids):\n",
    "        \"\"\"\n",
    "        Find the one-hot index corresponding to the air block (ID 5).\n",
    "        Returns:\n",
    "            int: The index where air blocks are encoded in one-hot format\n",
    "        \"\"\"\n",
    "        # Find the index that maps to block ID 5 (air) in our index_to_block mapping\n",
    "        idxs = []\n",
    "        for idx, block_id in self.index_to_block.items():\n",
    "            if block_id in block_ids:  # Air block ID\n",
    "                idxs.append(idx)\n",
    "        if len(idxs) == 0:\n",
    "            raise ValueError(\"Air block (ID 5) not found in block mappings!\")\n",
    "        return idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minecraft Chunks Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class MinecraftDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        data_path = Path(data_path)\n",
    "\n",
    "        # Try to load processed data first\n",
    "        # assert processed_data_path.exists() and mappings_path.exists()\n",
    "\n",
    "        print(\"Loading pre-processed data...\")\n",
    "        processed_data = torch.load(data_path)\n",
    "        # Only keep the chunks, discard biome data\n",
    "        self.processed_chunks = processed_data['chunks']\n",
    "        # self.processed_biomes = processed_data['biomes']\n",
    "        # Delete the biomes to free memory\n",
    "        # del processed_data['biomes']\n",
    "        # del processed_data['chunks']\n",
    "        del processed_data\n",
    "        \n",
    "        \n",
    "        print(f\"Loaded {len(self.processed_chunks)} chunks of size {self.processed_chunks.shape[1:]}\")\n",
    "        print(f\"Number of unique block types: {self.processed_chunks.shape[1]}\")\n",
    "        print(f'Unique blocks: {torch.unique(torch.argmax(self.processed_chunks, dim=1)).tolist()}')\n",
    "        #print(f\"Loaded {len(self.processed_biomes)} chunks of size {self.processed_biomes.shape[1:]}\")\n",
    "        #print(f\"Number of unique biome types: {self.processed_biomes.shape[1]}\")\n",
    "        #print(f'Unique biomes: {torch.unique(torch.argmax(self.processed_biomes, dim=1)).tolist()}')\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return self.processed_chunks[idx]\n",
    "        return self.processed_chunks[idx] #, self.processed_biomes[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        # return len(self.processed_chunks), len(self.processed_biomes)\n",
    "        return len(self.processed_chunks)\n",
    "\n",
    "def get_minecraft_dataloaders(data_path, batch_size=32, val_split=0.1, num_workers=4):\n",
    "    \"\"\"\n",
    "    Creates training and validation dataloaders for Minecraft chunks.\n",
    "    \"\"\"\n",
    "    # Create dataset\n",
    "    dataset = MinecraftDataset(data_path)\n",
    "    \n",
    "    # Split into train and validation sets\n",
    "    val_size = int(val_split * len(dataset))\n",
    "    train_size = len(dataset) - val_size\n",
    "    \n",
    "    # Use a fixed seed for reproducibility\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "    \n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, \n",
    "        [train_size, val_size],\n",
    "        generator=generator\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders with memory pinning\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    \n",
    "    print(f\"\\nDataloader details:\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Training batches: {len(train_loader)}\")\n",
    "    print(f\"Validation batches: {len(val_loader)}\")\n",
    "    \n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "Loads hyperparameters and model from checkpoint, gives easy function to encode a structure into latent codes, as well as decode those latents back into the reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FQGAN Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from log_utils import log, load_stats, load_model\n",
    "import copy\n",
    "from fq_models import FQModel, HparamsFQGAN\n",
    "\n",
    "\n",
    "# Loads hparams from hparams.json file in saved model directory\n",
    "def load_hparams_from_json(log_dir):\n",
    "    import json\n",
    "    import os\n",
    "    json_path = os.path.join(log_dir, 'hparams.json')\n",
    "    \n",
    "    if not os.path.exists(json_path):\n",
    "        raise FileNotFoundError(f\"No hparams.json file found in {log_dir}\")\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        hparams = json.load(f)\n",
    "\n",
    "    return hparams\n",
    "\n",
    "# turns loaded hparams json into propery hyperparams object\n",
    "def dict_to_vcqgan_hparams(hparams_dict, dataset=None):\n",
    "    # Determine which hyperparameter class to use based on the dataset\n",
    "    if dataset == None:\n",
    "        dataset = hparams_dict.get('dataset', 'MNIST')  # Default to MNIST if not specified\n",
    "    \n",
    "    vq_hyper = HparamsFQGAN(dataset)\n",
    "    # Set attributes from the dictionary\n",
    "    for key, value in hparams_dict.items():\n",
    "        setattr(vq_hyper, key, value)\n",
    "    \n",
    "    return vq_hyper\n",
    "\n",
    "# Loads fqgan model weights from a given checkpoint file\n",
    "def load_fqgan_from_checkpoint(H, fqgan):\n",
    "    fqgan = load_model(fqgan, \"fqgan\", H.load_step, H.load_dir).cuda()\n",
    "    fqgan.eval()\n",
    "    return fqgan\n",
    "\n",
    "# Takes a chunk or batch of chunks from the dataset, returns the encoded style and structure indices matrices\n",
    "def encode_and_quantize(fqgan, terrain_chunks, device='cuda'):\n",
    "    \"\"\"Memory-efficient encoding function\"\"\"\n",
    "    fqgan.eval()\n",
    "    with torch.no_grad():\n",
    "        # Move input to device\n",
    "        terrain_chunks = terrain_chunks.to(device)\n",
    "        \n",
    "        # Get encodings\n",
    "        h_style, h_struct = fqgan.encoder(terrain_chunks)\n",
    "        \n",
    "        # Process style path\n",
    "        h_style = fqgan.quant_conv_style(h_style)\n",
    "        quant_style, _, style_stats = fqgan.quantize_style(h_style)\n",
    "        style_indices = style_stats[2]  # Get indices from tuple\n",
    "        style_indices = style_indices.view(\n",
    "            (h_style.size()[0], h_style.size()[2], h_style.size()[3], h_style.size()[4])\n",
    "        )\n",
    "        \n",
    "        # Clear intermediate tensors\n",
    "        del h_style, quant_style, style_stats\n",
    "        \n",
    "        # Process structure path\n",
    "        h_struct = fqgan.quant_conv_struct(h_struct)\n",
    "        quant_struct, _, struct_stats = fqgan.quantize_struct(h_struct)\n",
    "        struct_indices = struct_stats[2]  # Get indices from tuple\n",
    "        struct_indices = struct_indices.view(\n",
    "            (h_struct.size()[0], h_struct.size()[2], h_struct.size()[3], h_struct.size()[4])\n",
    "        )\n",
    "        \n",
    "        # Clear intermediate tensors\n",
    "        del h_struct, quant_struct, struct_stats\n",
    "        \n",
    "        # Move indices to CPU to save GPU memory\n",
    "        style_indices = style_indices.cpu()\n",
    "        struct_indices = struct_indices.cpu()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return style_indices, struct_indices\n",
    "\n",
    "# Takes style and structure indices, returns the reconstructed map\n",
    "def decode_from_indices(style_indices, struct_indices, fqgan, device='cuda', two_stage=False):\n",
    "    \"\"\"Memory-efficient decoding function\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Move indices to device only when needed\n",
    "        style_indices = style_indices.to(device)\n",
    "        struct_indices = struct_indices.to(device)\n",
    "        \n",
    "        # Get quantized vectors\n",
    "        quant_style = fqgan.quantize_style.get_codebook_entry(\n",
    "            style_indices.view(-1),\n",
    "            shape=[1, fqgan.embed_dim, *style_indices.shape[1:]]\n",
    "        )\n",
    "        quant_struct = fqgan.quantize_struct.get_codebook_entry(\n",
    "            struct_indices.view(-1),\n",
    "            shape=[1, fqgan.embed_dim, *struct_indices.shape[1:]]\n",
    "        )\n",
    "        \n",
    "        # Clear indices from GPU\n",
    "        del style_indices, struct_indices\n",
    "        \n",
    "        # Combine and decode\n",
    "        quant = torch.cat([quant_struct, quant_style], dim=1)\n",
    "        # quant = quant_style + quant_struct\n",
    "        del quant_style, quant_struct\n",
    "        \n",
    "        if two_stage:\n",
    "            decoded, binary_decoded = fqgan.decoder(quant)\n",
    "        else:\n",
    "            decoded = fqgan.decoder(quant)\n",
    "        \n",
    "        del quant\n",
    "        \n",
    "        # Convert to block IDs if one-hot encoded\n",
    "        if decoded.shape[1] > 1:\n",
    "            decoded = torch.argmax(decoded, dim=1)\n",
    "        \n",
    "        # Move result to CPU and clear GPU memory\n",
    "        result = decoded.squeeze(0).cpu()\n",
    "        if two_stage:\n",
    "            binary_result = binary_decoded.squeeze(0).cpu()\n",
    "            del decoded\n",
    "            del binary_decoded\n",
    "            torch.cuda.empty_cache()\n",
    "            return result, binary_result\n",
    "        \n",
    "        del decoded\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load FQ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important if you reload the model, can run into memory issues. There's a memory leak somewhere I haven't been able to fix\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with directory for whatever model you want\n",
    "# model_path = './FQGAN_2stagedecoder_nobiomemodel_16bothcbook_EMA3'\n",
    "model_path = \"../model_logs/FQGAN_2stagedecoder_logweighted3_32codes_cycleconsistency_STE_postquant_detachcycle_gumbel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # I'm manually setting the load step here, if it errors out take a look at what the actual .th file number is\n",
    "# fqgan_hparams =  dict_to_vcqgan_hparams(load_hparams_from_json(f\"{model_path}\"), 'minecraft')\n",
    "# fqgan_hparams.load_step = 10000\n",
    "# fqgan = FQModel(fqgan_hparams)\n",
    "# fqgan = load_fqgan_from_checkpoint(fqgan_hparams, fqgan)\n",
    "# print(f'loaded from: {fqgan_hparams.log_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed data...\n",
      "Loaded 11119 chunks of size torch.Size([42, 24, 24, 24])\n",
      "Number of unique block types: 42\n",
      "Unique blocks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "\n",
      "Dataloader details:\n",
      "Training samples: 10008\n",
      "Validation samples: 1111\n",
      "Batch size: 4\n",
      "Training batches: 2502\n",
      "Validation batches: 278\n"
     ]
    }
   ],
   "source": [
    "# loads a preprocesed dataset, which already has a mappings file created and everything one-hot encoded nicely. For more memory efficient, could try just loading the validation set file\n",
    "# data_path = '../../text2env/data/minecraft_biome_newworld_10k_processed_cleaned.pt'\n",
    "data_path = '../../text2env/data/24_newdataset_processed_cleaned3.pt'\n",
    "train_loader, val_loader = get_minecraft_dataloaders(\n",
    "    data_path,\n",
    "    batch_size=4,\n",
    "    num_workers=0, # Must be 0 if you're on windows, otherwise it errors\n",
    "    val_split=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mappings_path = '../../text2env/data/minecraft_biome_newworld_10k_mappings.pt'\n",
    "mappings_path = '../../text2env/data/24_newdataset_mappings3.pt'\n",
    "\n",
    "block_converter = BlockBiomeConverter.load_mappings(mappings_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.4.3'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'vtk': 'https://cdn.jsdelivr.net/npm/vtk.js@30.1.0/vtk'}, 'shim': {'vtk': {'exports': 'vtk'}}});\n      require([\"vtk\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 1;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window.vtk !== undefined) && (!(window.vtk instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.4.5/dist/bundled/abstractvtkplot/vtk.js@30.1.0/vtk.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.holoviz.org/panel/1.4.5/dist/bundled/abstractvtkplot/vtk.js@30.1.0/vtk.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.3.min.js\", \"https://cdn.holoviz.org/panel/1.4.5/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n\ttry {\n          inline_js[i].call(root, root.Bokeh);\n\t} catch(e) {\n\t  if (!reloading) {\n\t    throw e;\n\t  }\n\t}\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        })\n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='15709b10-ae65-4d55-8e48-b28abdb264ea'>\n",
       "  <div id=\"c7b16129-243c-49aa-b5ec-7aa31865ca2a\" data-root-id=\"15709b10-ae65-4d55-8e48-b28abdb264ea\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"179037cb-b340-43ac-ae06-6928fd2e89c5\":{\"version\":\"3.4.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"15709b10-ae65-4d55-8e48-b28abdb264ea\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"f1897b7e-9445-46fa-8fc8-361062df272e\",\"attributes\":{\"plot_id\":\"15709b10-ae65-4d55-8e48-b28abdb264ea\",\"comm_id\":\"36453a0546ab4185aa1fefaa289a8e32\",\"client_comm_id\":\"38ec3a8a4b114ee8ad2430cdb55d5356\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":null}]}]}};\n",
       "  var render_items = [{\"docid\":\"179037cb-b340-43ac-ae06-6928fd2e89c5\",\"roots\":{\"15709b10-ae65-4d55-8e48-b28abdb264ea\":\"c7b16129-243c-49aa-b5ec-7aa31865ca2a\"},\"root_ids\":[\"15709b10-ae65-4d55-8e48-b28abdb264ea\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined && ( root.vtk !== undefined) && ( root.vtk !== undefined))\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "15709b10-ae65-4d55-8e48-b28abdb264ea"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an instnance of our visualizer\n",
    "from visualization_utils import MinecraftVisualizerPyVista\n",
    "visualizer = MinecraftVisualizerPyVista()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get corresponding chunks for each structure code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # store codes and its corresponding chunks.\n",
    "# # We don't have a guarantee that air will be index 0, so I have a function to retrieve the correct index corresponding to air blocks\n",
    "# air_idx = block_converter.get_air_block_index()\n",
    "\n",
    "# structure_dict = {}\n",
    "# structure_dict_bin = {}\n",
    "# style_dict = {}\n",
    "# for batch in train_loader:\n",
    "#     for sample_idx in range(len(batch)):\n",
    "#         sample= batch[sample_idx].unsqueeze(0).cuda()\n",
    "#         with torch.no_grad():\n",
    "#             style_indices, struct_indices = encode_and_quantize(fqgan, sample)\n",
    "#         reconstructed, binary_reconstructed = decode_from_indices(style_indices, struct_indices, fqgan, two_stage=True)\n",
    "#         for i in range(struct_indices.shape[1]):  \n",
    "#             for j in range(struct_indices.shape[2]):  \n",
    "#                 for k in range(struct_indices.shape[3]): \n",
    "#                     style_code = style_indices[0, i, j, k].item()  \n",
    "#                     struct_code = struct_indices[0, i, j, k].item()  \n",
    "#                     x_start, y_start, z_start = i * 4, j * 4, k * 4\n",
    "#                     x_end, y_end, z_end = x_start + 4, y_start + 4, z_start + 4\n",
    "#                     block = reconstructed[x_start:x_end, y_start:y_end, z_start:z_end]\n",
    "#                     bin_block = binary_reconstructed[x_start:x_end, y_start:y_end, z_start:z_end]\n",
    "#                     if struct_code not in structure_dict:\n",
    "#                         structure_dict[struct_code] = []  # Initialize list if not present\n",
    "#                     if struct_code not in structure_dict_bin:\n",
    "#                         structure_dict_bin[struct_code] = []\n",
    "#                     if style_code not in style_dict:\n",
    "#                         style_dict[style_code] = []  # Initialize list if not present\n",
    "#                     structure_dict[struct_code].append(block)\n",
    "#                     style_dict[style_code].append(block)\n",
    "#                     structure_dict_bin[struct_code].append(bin_block)\n",
    "\n",
    "# # convert chunks into binary chunks\n",
    "# binary_structure_dict = {\n",
    "#     struct_code: [(b != air_idx).to(dtype=torch.int) for b in block_list]  # Convert each block tensor\n",
    "#     for struct_code, block_list in structure_dict.items()\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Assuming latent space shape is fixed (e.g., 6x6x6) - get this dynamically if possible\n",
    "# # Example: Infer from a dummy forward pass or model config if necessary\n",
    "# latent_depth, latent_height, latent_width = 6, 6, 6 # *** Adjust if different ***\n",
    "# latent_shape = (latent_depth, latent_height, latent_width)\n",
    "# num_structure_codes = 20\n",
    "# # Initialize count tensors for each code\n",
    "# position_counts = {\n",
    "#     code: torch.zeros(latent_shape, dtype=torch.long, device='cpu')\n",
    "#     for code in range(num_structure_codes)\n",
    "# }\n",
    "# total_samples_processed = 0\n",
    "\n",
    "# print(\"Collecting positional frequencies from train_loader...\")\n",
    "# for batch_idx, batch in enumerate(train_loader):\n",
    "#     # Limit batches for testing?\n",
    "#     # if batch_idx > 20: break\n",
    "\n",
    "#     if torch.cuda.is_available():\n",
    "#         batch = batch.cuda()\n",
    "\n",
    "#     # Only need the encoding part\n",
    "#     style_indices, struct_indices = encode_and_quantize(fqgan, batch) # Get indices for the whole batch\n",
    "\n",
    "#     # Process each sample in the batch\n",
    "#     for sample_idx in range(struct_indices.shape[0]): # Iterate through batch dimension\n",
    "#         struct_indices_sample = struct_indices[sample_idx].cpu() # Get indices for one sample, move to CPU\n",
    "\n",
    "#         # Iterate through the latent grid dimensions (D, H, W)\n",
    "#         for i in range(latent_depth):\n",
    "#             for j in range(latent_height):\n",
    "#                 for k in range(latent_width):\n",
    "#                     struct_code = struct_indices_sample[i, j, k].item()\n",
    "#                     if 0 <= struct_code < num_structure_codes:\n",
    "#                         position_counts[struct_code][i, j, k] += 1\n",
    "#                     else:\n",
    "#                         print(f\"Warning: Encountered out-of-bounds code {struct_code} at ({i},{j},{k})\")\n",
    "#         total_samples_processed += 1\n",
    "\n",
    "#     # Optional: Print progress\n",
    "#     if (batch_idx + 1) % 50 == 0:\n",
    "#             print(f\"Processed {batch_idx + 1} batches...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_positional_frequency(position_counts, struct_code, save_dir):\n",
    "    \"\"\"\n",
    "    Generates and saves a 3D scatter plot showing the positional frequency\n",
    "    of a structure code within the latent grid, using a Viridis colormap and\n",
    "    Minecraft coordinate conventions (Y=Height is vertical, Y=0 is bottom).\n",
    "\n",
    "    Args:\n",
    "        position_counts (dict): Maps struct_code to 6x6x6 count tensor.\n",
    "        struct_code (int): The structure code to visualize.\n",
    "        save_dir (str): The directory to save the plot image.\n",
    "    \"\"\"\n",
    "    if struct_code not in position_counts:\n",
    "        print(f\"Error: Code {struct_code} not found in position_counts dictionary.\")\n",
    "        return\n",
    "\n",
    "    counts_tensor = position_counts[struct_code].cpu() # Ensure it's on CPU\n",
    "    latent_shape = counts_tensor.shape\n",
    "    if len(latent_shape) != 3:\n",
    "        print(f\"Error: Count tensor for code {struct_code} is not 3D (shape: {latent_shape}).\")\n",
    "        return\n",
    "\n",
    "    counts_numpy = counts_tensor.numpy()\n",
    "    max_count = np.max(counts_numpy)\n",
    "\n",
    "    if max_count == 0:\n",
    "        print(f\"Info: Code {struct_code} never appeared. Skipping visualization.\")\n",
    "        # Optional: Create an empty plot placeholder if desired\n",
    "        return # Stop here if code never appeared\n",
    "\n",
    "    # --- Plotting Setup ---\n",
    "    # os.makedirs(save_dir, exist_ok=True)\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Generate grid indices (I, J, K) corresponding to tensor dimensions\n",
    "    # Assume I=Depth(Z_mc), J=Height(Y_mc), K=Width(X_mc)\n",
    "    i_indices, j_indices, k_indices = np.indices(latent_shape)\n",
    "\n",
    "    # Flatten coordinates and the raw counts (for color mapping)\n",
    "    i_coords_mc = i_indices.flatten() # Minecraft Z coordinates\n",
    "    j_coords_mc = j_indices.flatten() # Minecraft Y (Height) coordinates\n",
    "    k_coords_mc = k_indices.flatten() # Minecraft X coordinates\n",
    "    frequencies = counts_numpy.flatten()\n",
    "\n",
    "    # --- Create Scatter Plot (Viridis Colormap, Correct Axis Mapping) ---\n",
    "    # cmap = plt.get_cmap('viridis')\n",
    "    cmap = plt.get_cmap('Greys')\n",
    "    # Plot mapping:\n",
    "    # Plot X-axis <- Minecraft X data (k_coords_mc)\n",
    "    # Plot Y-axis <- Minecraft Z data (i_coords_mc)\n",
    "    # Plot Z-axis <- Minecraft Y (Height) data (j_coords_mc) <<< VERTICAL AXIS\n",
    "    scatter = ax.scatter(k_coords_mc, i_coords_mc, j_coords_mc, # Correct mapping\n",
    "                         c=frequencies, cmap=cmap, # Color based on frequency counts\n",
    "                         s=150, # Adjust size as needed\n",
    "                         alpha=0.8, # Add some transparency\n",
    "                         # vmin=0, vmax=max_count, # Optional: Explicitly set color limits\n",
    "                         edgecolors='grey', linewidth=0.5)\n",
    "\n",
    "    # --- Add Colorbar ---\n",
    "    cbar = fig.colorbar(scatter, ax=ax, shrink=0.6, aspect=20, pad=0.1)\n",
    "    cbar.set_label(\"Code Occurrence Count\")\n",
    "\n",
    "    # --- Set Labels, Ticks, Limits, Title, and Invert Z-axis ---\n",
    "    ax.set_title(f\"Code {struct_code} Positional Frequency\")\n",
    "    # Label plot axes according to the *Minecraft dimension* plotted on them\n",
    "    ax.set_xlabel(\"Z (Latent Dim K)\")\n",
    "    ax.set_ylabel(\"X (Latent Dim I)\")\n",
    "    ax.set_zlabel(\"Y (Height, Latent Dim J)\") # Vertical axis\n",
    "\n",
    "    # Set ticks based on the dimension size\n",
    "    ax.set_xticks(np.arange(latent_shape[2])) # K dimension\n",
    "    ax.set_yticks(np.arange(latent_shape[0])) # I dimension\n",
    "    ax.set_zticks(np.arange(latent_shape[1])) # J dimension\n",
    "\n",
    "    # Set limits for plot axes\n",
    "    ax.set_xlim(-0.5, latent_shape[2] - 0.5)\n",
    "    ax.set_ylim(-0.5, latent_shape[0] - 0.5)\n",
    "    ax.set_zlim(-0.5, latent_shape[1] - 0.5)\n",
    "\n",
    "    # --- Invert the Z-axis (which represents Y-Height) ---\n",
    "    # ax.invert_zaxis() # Ensures Y=0 is at the bottom\n",
    "    ax.invert_yaxis() # Ensures Y=0 is at the bottom\n",
    "\n",
    "    # Adjust view angle\n",
    "    ax.view_init(elev=20., azim=-75)\n",
    "    # plt.show()\n",
    "    # --- Save and Close ---\n",
    "    image_path = os.path.join(save_dir, f\"pos_freq_code_{struct_code}.png\")\n",
    "    try:\n",
    "        plt.savefig(image_path, bbox_inches='tight', dpi=150)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving positional frequency plot for code {struct_code}: {e}\")\n",
    "    finally:\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "def generate_all_positional_frequency_plots(position_counts, output_dir):\n",
    "    print(f\"Generating positional frequency plots for {len(position_counts)} codes...\")\n",
    "    os.makedirs(output_dir, exist_ok=True) # Ensure directory exists\n",
    "\n",
    "    # Iterate through the codes present in the dictionary\n",
    "    codes_to_plot = sorted(position_counts.keys())\n",
    "\n",
    "    for i, code in enumerate(codes_to_plot):\n",
    "        # Call the plotting function for the current code\n",
    "        plot_positional_frequency(position_counts, code, output_dir)\n",
    "\n",
    "        # Optional: Print progress\n",
    "        if (i + 1) % 10 == 0 or (i + 1) == len(codes_to_plot):\n",
    "            print(f\"Generated plot {i+1}/{len(codes_to_plot)} (Code {code})\")\n",
    "\n",
    "    print(f\"Finished generating positional frequency plots. Saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize some of the chunks in a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def visualize_structure_grid(structure_dict, struct_code, num_chunks=64, grid_size=8, spacing=2, save_path=None, code_type=\"Struct\"):\n",
    "    \"\"\"\n",
    "    Visualize a grid of 4x4x4 chunks for a specific structure code.\n",
    "    \n",
    "    Parameters:\n",
    "    - structure_dict: Dictionary containing chunks for each structure code\n",
    "    - struct_code: The structure code to visualize\n",
    "    - num_chunks: Number of chunks to display (default: 64)\n",
    "    - grid_size: Number of chunks per row/column (default: 8)\n",
    "    - spacing: Number of blocks spacing between chunks (default: 2)\n",
    "    \n",
    "    Returns:\n",
    "    - PyVista plotter with interactive visualization\n",
    "    \"\"\"\n",
    "    import pyvista as pv\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    # Check if the structure code exists\n",
    "    if struct_code not in structure_dict:\n",
    "        print(f\"Structure code {struct_code} not found in dictionary\")\n",
    "        return None\n",
    "    \n",
    "    # Randomly select num_chunks from the list\n",
    "    available_chunks = structure_dict[struct_code]\n",
    "    actual_chunks = len(available_chunks)\n",
    "    \n",
    "    if actual_chunks == 0:\n",
    "        print(f\"No chunks found for structure code {struct_code}\")\n",
    "        return None\n",
    "    \n",
    "    if actual_chunks < num_chunks:\n",
    "        print(f\"Only {actual_chunks} chunks available for structure code {struct_code}\")\n",
    "        chunks = available_chunks\n",
    "    else:\n",
    "        # Randomly select num_chunks from the list\n",
    "        chunks = random.sample(available_chunks, num_chunks)\n",
    "    \n",
    "    # Setup block colors mapping (simplified from the visualizer)\n",
    "    blocks_to_cols = {\n",
    "            0: (0.5, 0.25, 0.0),    # light brown\n",
    "            10: 'black', # bedrock\n",
    "            29: \"#006400\", # cacutus\n",
    "            38: \"#B8860B\",  # clay\n",
    "            60: \"brown\",  # dirt\n",
    "            92: \"gold\",  # gold ore\n",
    "            93: \"green\",  # grass\n",
    "            115: \"brown\",  # ladder...?\n",
    "            119: (.02, .28, .16, 0.9),  # transparent forest green (RGBA) for leaves\n",
    "            120: (.02, .28, .16, 0.9),  # leaves2\n",
    "            194: \"yellow\",  # sand\n",
    "            217: \"gray\",  # stone\n",
    "            240: (0.0, 0.0, 1.0, 0.4),  # water\n",
    "            227: (0.0, 1.0, 0.0, .3), # tall grass\n",
    "            237: (0.33, 0.7, 0.33, 0.3), # vine\n",
    "            40: \"#2F4F4F\",  # coal ore\n",
    "            62: \"#228B22\",  # double plant\n",
    "            108: \"#BEBEBE\",  # iron ore\n",
    "            131: \"saddlebrown\",  # log1\n",
    "            132: \"saddlebrown\",  #log2\n",
    "            95: \"lightgray\",  # gravel\n",
    "            243: \"wheat\",  # wheat\n",
    "            197: \"limegreen\",  # sapling\n",
    "            166: \"orange\",  #pumpkin\n",
    "            167: \"#FF8C00\",  # pumpkin stem\n",
    "            184: \"#FFA07A\",  # red flower\n",
    "            195: \"tan\",  # sandstone\n",
    "            250: \"white\",  #wool \n",
    "            251: \"gold\",   #yellow flower\n",
    "        }\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    chunk_size = 4\n",
    "    grid_dim = grid_size * chunk_size + (grid_size - 1) * spacing\n",
    "    \n",
    "    # Create plotter\n",
    "    plotter = pv.Plotter(notebook=True)\n",
    "    \n",
    "    # Remove existing lights and add custom lighting\n",
    "    plotter.remove_all_lights()\n",
    "    plotter.add_light(pv.Light(position=(1, -1, 1), intensity=1.0, color='white'))\n",
    "    plotter.add_light(pv.Light(position=(-1, 1, 0.5), intensity=0.5, color='white'))\n",
    "    plotter.add_light(pv.Light(position=(-0.5, -0.5, -1), intensity=0.3, color='white'))\n",
    "    plotter.add_title(f\"{code_type} Code {struct_code} - Pattern Visualization\", font_size=16)\n",
    "    # Place each chunk in the grid\n",
    "    for i in range(min(actual_chunks, num_chunks)):\n",
    "        # Calculate grid position\n",
    "        row = i // grid_size\n",
    "        col = i % grid_size\n",
    "        \n",
    "        # Calculate offset in the grid\n",
    "        x_offset = col * (chunk_size + spacing)\n",
    "        z_offset = row * (chunk_size + spacing)\n",
    "        \n",
    "        # Get current chunk\n",
    "        chunk = chunks[i]\n",
    "        chunk = block_converter.convert_to_original_blocks(chunk)\n",
    "        # Convert to numpy if needed\n",
    "        if isinstance(chunk, torch.Tensor):\n",
    "            chunk = chunk.detach().cpu().numpy()\n",
    "        \n",
    "        # Apply the same transformations as original visualizer\n",
    "        chunk = chunk.transpose(2, 0, 1)\n",
    "        chunk = np.rot90(chunk, 1, (0, 1))\n",
    "        \n",
    "        # Convert encoded blocks to original block IDs\n",
    "        \n",
    "        # Create grid for this chunk\n",
    "        grid = pv.ImageData()\n",
    "        grid.dimensions = np.array(chunk.shape) + 1\n",
    "        grid.origin = (x_offset, z_offset, 0)  # Position in the overall grid\n",
    "        grid.spacing = (1, 1, 1)  # Unit spacing\n",
    "        grid.cell_data[\"values\"] = chunk.flatten(order=\"F\")\n",
    "        \n",
    "        # Plot each block type in the chunk\n",
    "        mask = (chunk != 5) & (chunk != -1)\n",
    "        unique_blocks = np.unique(chunk[mask])\n",
    "        \n",
    "        for block_id in unique_blocks:\n",
    "            # Skip air blocks (0)\n",
    "            if block_id == 0:\n",
    "                continue\n",
    "                \n",
    "            threshold = grid.threshold([block_id-0.5, block_id+0.5])\n",
    "            \n",
    "            # Get color for this block type\n",
    "            if block_id in blocks_to_cols:\n",
    "                color = blocks_to_cols[int(block_id)]\n",
    "                opacity = 1.0 if isinstance(color, str) or len(color) == 3 else color[3]\n",
    "            else:\n",
    "                # Default for unknown blocks\n",
    "                color = (0.7, 0.7, 0.7)  # Gray\n",
    "                opacity = 1.0\n",
    "            \n",
    "            # Add mesh for this block type\n",
    "            plotter.add_mesh(threshold, \n",
    "                           color=color,\n",
    "                           opacity=opacity,\n",
    "                           show_edges=True,\n",
    "                           edge_color='black',\n",
    "                           line_width=0.5,\n",
    "                           edge_opacity=0.3,\n",
    "                           lighting=True)\n",
    "    \n",
    "    # Add a dummy cube to set overall bounds\n",
    "    total_size = grid_size * chunk_size + (grid_size - 1) * spacing\n",
    "    # outline = pv.Cube(bounds=(0, total_size, 0, chunk_size, 0, total_size))\n",
    "    # plotter.add_mesh(outline, opacity=0.0)\n",
    "    \n",
    "    # Set camera position and bounds\n",
    "    plotter.camera_position = 'iso'\n",
    "    plotter.camera.zoom(1)  # Zoom out to see the whole grid\n",
    "    \n",
    "    # Add grid lines or axes\n",
    "    # plotter.show_bounds(\n",
    "    #     grid='back',\n",
    "    #     location='back', \n",
    "    #     font_size=10,\n",
    "    #     bounds=[0, total_size, 0,total_size, 0, chunk_size],\n",
    "    #     axes_ranges=[0, total_size, 0, total_size, 0, chunk_size]\n",
    "    # )\n",
    "    # Save image if path provided\n",
    "    if save_path:\n",
    "        plotter.screenshot(save_path)\n",
    "        plotter.close()\n",
    "        print(f\"Saved visualization for structure code {struct_code} to {save_path}\")\n",
    "    return plotter\n",
    "\n",
    "# Usage example:\n",
    "# plotter = visualize_structure_grid(binary_structure_dict, 4)\n",
    "# plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop to visualize and save all structure codes\n",
    "def visualize_all_structure_codes(structure_dict, output_dir=\"structure_visualizations\", num_chunks=64, code_type=\"Struct\"):\n",
    "    import os\n",
    "    import math\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # vis_out_dir = os.path.join(output_dir, f\"{code_type}_visualizations\")\n",
    "    # os.makedirs(vis_out_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all unique structure codes\n",
    "    struct_codes = sorted(structure_dict.keys())\n",
    "    \n",
    "    print(f\"Found {len(struct_codes)} {code_type} codes to visualize\")\n",
    "    \n",
    "    # Loop through each structure code\n",
    "    for struct_code in struct_codes:\n",
    "        # Define output file path\n",
    "        output_path = os.path.join(output_dir, f\"{code_type}_code_{struct_code}.png\")\n",
    "        \n",
    "        # Skip if file already exists (optional, can be removed)\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"Skipping {code_type} code {struct_code} - file already exists\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Visualizing {code_type} code {struct_code}...\")\n",
    "        \n",
    "        # Generate and save visualization\n",
    "        try:\n",
    "            visualize_structure_grid(\n",
    "                structure_dict, \n",
    "                struct_code, \n",
    "                num_chunks=num_chunks, \n",
    "                grid_size=int(math.sqrt(num_chunks)),\n",
    "                save_path=output_path,\n",
    "                code_type=code_type\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error visualizing {code_type} code {struct_code}: {str(e)}\")\n",
    "    \n",
    "    print(f\"Visualization complete. Images saved to {output_dir}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import get_cmap\n",
    "\n",
    "# Ensure pyvista is set up for notebook display\n",
    "# pv.set_jupyter_backend('trame') # or 'ipygany', 'panel', etc. depending on your setup\n",
    "\n",
    "def visualize_probability_volume_pyvista(prob_matrix, title=\"Probability Volume\", cmap_name='viridis', fixed_opacity=0.5):\n",
    "    \"\"\"\n",
    "    Visualizes a 4x4x4 probability matrix using PyVista, where voxel color\n",
    "    maps to probability and opacity is fixed for visibility.\n",
    "\n",
    "    Args:\n",
    "        prob_matrix (np.ndarray): A 4x4x4 numpy array of probabilities (0.0 to 1.0).\n",
    "        title (str): The title for the plot.\n",
    "        cmap_name (str): Name of the matplotlib colormap to use (e.g., 'viridis', 'coolwarm', 'jet').\n",
    "        fixed_opacity (float): The fixed opacity value (0.0 to 1.0) for visible blocks.\n",
    "    \"\"\"\n",
    "    if not isinstance(prob_matrix, np.ndarray) or prob_matrix.shape != (4, 4, 4):\n",
    "        print(f\"Error: Input must be a 4x4x4 NumPy array. Got shape {prob_matrix.shape}\")\n",
    "        return\n",
    "\n",
    "    plotter = pv.Plotter(notebook=True)\n",
    "    plotter.add_title(title, font_size=12)\n",
    "\n",
    "    # Set up colormap and normalization\n",
    "    cmap = get_cmap(cmap_name)\n",
    "    norm = Normalize(vmin=0.0, vmax=1.0) # Probabilities range from 0 to 1\n",
    "\n",
    "    min_probability_threshold = 0.01 # Don't render blocks with near-zero probability\n",
    "\n",
    "    # Keep track if any blocks were added\n",
    "    blocks_added = False\n",
    "    for x in range(4):\n",
    "        for y in range(4):\n",
    "            for z in range(4):\n",
    "                probability = prob_matrix[x, y, z]\n",
    "\n",
    "                # Only add a cube if the probability is above the threshold\n",
    "                if probability >= min_probability_threshold:\n",
    "                    # Get the corresponding color from the colormap\n",
    "                    # Use matplotlib cmap directly to get RGBA, then take RGB\n",
    "                    color = cmap(norm(probability))\n",
    "\n",
    "                    # Create a cube for this voxel position\n",
    "                    cube = pv.Cube(bounds=(x, x + 1, y, y + 1, z, z + 1))\n",
    "\n",
    "                    # Add the cube mesh to the plotter\n",
    "                    plotter.add_mesh(\n",
    "                        cube,\n",
    "                        color=color[:3], # Pass RGB tuple\n",
    "                        opacity=fixed_opacity,\n",
    "                        show_edges=True,\n",
    "                        edge_color='grey',\n",
    "                        line_width=1\n",
    "                    )\n",
    "                    blocks_added = True\n",
    "\n",
    "    # Only add scalar bar if we actually plotted something\n",
    "    if blocks_added:\n",
    "        # Add a scalar bar manually configured\n",
    "        plotter.add_scalar_bar(\n",
    "            title=\"Probability\",\n",
    "            # cmap=cmap_name, # Pass the colormap name\n",
    "            # Define the limits for the scalar bar manually\n",
    "            # This requires creating a dummy actor or setting clim directly\n",
    "            # For simplicity, we'll rely on cmap name and default limits (0-1)\n",
    "            # which works since our data is normalized 0-1.\n",
    "            n_labels=6, # Number of labels on the colorbar (e.g., 0.0, 0.2, ..., 1.0)\n",
    "            fmt=\"%.2f\" # Format labels to 2 decimal places\n",
    "        )\n",
    "        # Note: If the range wasn't 0-1, you might need plotter.update_scalar_bar_range([min, max])\n",
    "        # or pass clim=[min, max] if supported by your PyVista version.\n",
    "\n",
    "    # Add axes bounds for context\n",
    "    plotter.show_bounds(\n",
    "        bounds=[0, 4, 0, 4, 0, 4],\n",
    "        grid='front',\n",
    "        location='outer',\n",
    "        xlabel='X',\n",
    "        ylabel='Y',\n",
    "        zlabel='Z',\n",
    "        ticks='inside',\n",
    "        minor_ticks=False,\n",
    "        n_xlabels=5,\n",
    "        n_ylabels=5,\n",
    "        n_zlabels=5,\n",
    "        fmt='%0.0f'\n",
    "    )\n",
    "\n",
    "    # Set camera view\n",
    "    plotter.camera_position = 'iso'\n",
    "    # plotter.camera.zoom(1.5)\n",
    "\n",
    "    # Show the interactive plot\n",
    "    plotter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# air_idx = block_converter.get_air_block_index()\n",
    "\n",
    "# style_dict = {}\n",
    "# for batch in train_loader:\n",
    "#     for sample_idx in range(len(batch)):\n",
    "#         sample= batch[sample_idx].unsqueeze(0).cuda()\n",
    "#         with torch.no_grad():\n",
    "#             style_indices, struct_indices = encode_and_quantize(fqgan, sample)\n",
    "#         reconstructed, binary_reconstructed = decode_from_indices(style_indices, struct_indices, fqgan, two_stage=True)\n",
    "#         for i in range(style_indices.shape[1]):  \n",
    "#             for j in range(style_indices.shape[2]):  \n",
    "#                 for k in range(style_indices.shape[3]): \n",
    "#                     style_code = style_indices[0, i, j, k].item()  \n",
    "#                     x_start, y_start, z_start = i * 4, j * 4, k * 4\n",
    "#                     x_end, y_end, z_end = x_start + 4, y_start + 4, z_start + 4\n",
    "#                     block = reconstructed[x_start:x_end, y_start:y_end, z_start:z_end]\n",
    "#                     if style_code not in style_dict:\n",
    "#                         style_dict[style_code] = []  # Initialize list if not present\n",
    "#                     style_dict[style_code].append(block)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Store the style codes and corresponding chunks\n",
    "# style_dict = {}\n",
    "# for batch in train_loader:\n",
    "#     for sample_idx in range(len(batch)):\n",
    "#         sample= batch[sample_idx].unsqueeze(0).cuda()\n",
    "#         with torch.no_grad():\n",
    "#             style_indices, struct_indices = encode_and_quantize(fqgan, sample)\n",
    "#         reconstructed, binary_reconstructed = decode_from_indices(style_indices, struct_indices, fqgan, two_stage=True)\n",
    "#         for i in range(style_indices.shape[1]):  \n",
    "#             for j in range(style_indices.shape[2]):  \n",
    "#                 for k in range(style_indices.shape[3]): \n",
    "#                     style_code = style_indices[0, i, j, k].item()  \n",
    "#                     x_start, y_start, z_start = i * 4, j * 4, k * 4\n",
    "#                     x_end, y_end, z_end = x_start + 4, y_start + 4, z_start + 4\n",
    "#                     block = reconstructed[x_start:x_end, y_start:y_end, z_start:z_end]\n",
    "#                     if style_code not in style_dict:\n",
    "#                         style_dict[style_code] = []  # Initialize list if not present\n",
    "#                     style_dict[style_code].append(block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count the frequency of different block types\n",
    "# from collections import defaultdict\n",
    "# block_type_frequencies = defaultdict(lambda: defaultdict(int))\n",
    "# block_total_counts = defaultdict(int)\n",
    "# for style_code, block_list in style_dict.items():\n",
    "#     for block_matrix in block_list:\n",
    "#         flattened_blocks = block_matrix.flatten()\n",
    "#         block_total_counts[style_code] += len(flattened_blocks)\n",
    "\n",
    "#         # Count the frequency of each block type\n",
    "#         for block_type in flattened_blocks:\n",
    "#             block_type_frequencies[style_code][block_type.item()] += 1\n",
    "# sorted_block_frequency_dict = {}\n",
    "# for style_code in sorted(block_type_frequencies.keys()):  # Sort by style code\n",
    "#     block_counts = block_type_frequencies[style_code]\n",
    "#     total_blocks = block_total_counts[style_code] if block_total_counts[style_code] > 0 else 1  # Avoid division by zero\n",
    "\n",
    "#     sorted_block_frequency_dict[style_code] = {\n",
    "#         block_type: block_counts[block_type] / total_blocks\n",
    "#         for block_type in sorted(block_counts.keys())  # Sort by block type\n",
    "#     }\n",
    "# print(sorted_block_frequency_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check blocks that are most commonly used\n",
    "# threshold = 0.001\n",
    "# filtered_block_frequency_dict = {\n",
    "#     style_code: {\n",
    "#         block_type: freq for block_type, freq in block_counts.items() if freq >= threshold\n",
    "#     }\n",
    "#     for style_code, block_counts in sorted_block_frequency_dict.items()\n",
    "# }\n",
    "# common_block_index = {\n",
    "#     style_code: [block_type for block_type, freq in block_counts.items() if freq >= threshold]\n",
    "#     for style_code, block_counts in sorted_block_frequency_dict.items()\n",
    "# }\n",
    "\n",
    "# common_block_list = []\n",
    "# for i in common_block_index.values():\n",
    "#     for j in i:\n",
    "#         if j not in common_block_list:\n",
    "#             common_block_list.append(j)\n",
    "# common_block_list.sort()\n",
    "# print(common_block_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_index_to_name = {\n",
    "    0:  \"AIR\",\n",
    "    1:  \"BONE_BLOCK\",\n",
    "    2:  \"BROWN_MUSHROOM\",\n",
    "    3:  \"BROWN_MUSHROOM_BLOCK\",\n",
    "    4:  \"CACTUS\",\n",
    "    5:  \"CHEST\",\n",
    "    6:  \"CLAY\",\n",
    "    7:  \"COAL_ORE\",\n",
    "    8:  \"COBBLESTONE\",\n",
    "    9:  \"DEADBUSH\",\n",
    "   10:  \"DIRT\",\n",
    "   11:  \"DOUBLE_PLANT\",\n",
    "   12:  \"EMERALD_ORE\",\n",
    "   13:  \"FLOWING_LAVA\",\n",
    "   14:  \"FLOWING_WATER\",\n",
    "   15:  \"GOLD_ORE\",\n",
    "   16:  \"GRASS\",\n",
    "   17:  \"GRAVEL\",\n",
    "   18:  \"IRON_ORE\",\n",
    "   19:  \"LAPIS_ORE\",\n",
    "   20:  \"LAVA\",\n",
    "   21:  \"LEAVES\",\n",
    "   22:  \"LEAVES2\",\n",
    "   23:  \"LOG\",\n",
    "   24:  \"LOG2\",\n",
    "   25:  \"MOB_SPAWNER\",\n",
    "   26:  \"MONSTER_EGG\",\n",
    "   27:  \"MOSSY_COBBLESTONE\",\n",
    "   28:  \"PUMPKIN\",\n",
    "   29:  \"RED_FLOWER\",\n",
    "   30:  \"RED_MUSHROOM_BLOCK\",\n",
    "   31:  \"REEDS\",\n",
    "   32:  \"SAND\",\n",
    "   33:  \"SANDSTONE\",\n",
    "   34:  \"SNOW_LAYER\",\n",
    "   35:  \"STONE\",\n",
    "   36:  \"STONE_SLAB\",\n",
    "   37:  \"TALLGRASS\",\n",
    "   38:  \"VINE\",\n",
    "   39:  \"WATER\",\n",
    "   40:  \"WATERLILY\",\n",
    "   41:  \"YELLOW_FLOWER\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map indices to block names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_index_to_name = {}\n",
    "for index in sorted(block_converter.index_to_block.keys()):\n",
    "    block_id = block_converter.get_block_id_from_index(index)\n",
    "    block_name = block_converter.get_block_name_from_index(index)\n",
    "    print(f\"{index:5d} | {block_id:7d} | {block_name}\")\n",
    "    block_index_to_name[index] = block_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codebook metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import itertools\n",
    "\n",
    "def calculate_average_frequency_maps(binary_structure_dict):\n",
    "    \"\"\"\n",
    "    Calculates the average frequency map for each structure code.\n",
    "\n",
    "    Args:\n",
    "        binary_structure_dict (dict): Dictionary mapping structure codes (int)\n",
    "                                      to lists of 4x4x4 binary tensors (torch.Tensor).\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary mapping structure codes (int) to their 4x4x4\n",
    "              average frequency map (float tensor). Returns empty dict if input is empty.\n",
    "              Codes with no associated chunks are skipped.\n",
    "    \"\"\"\n",
    "    if not binary_structure_dict:\n",
    "        return {}\n",
    "\n",
    "    avg_freq_maps = {}\n",
    "    for code, chunk_list in binary_structure_dict.items():\n",
    "        if not chunk_list:\n",
    "            print(f\"Warning: Code {code} has no associated chunks. Skipping.\")\n",
    "            continue\n",
    "        # Stack tensors along a new dimension (dim=0) and calculate the mean\n",
    "        stacked_chunks = torch.stack(chunk_list).float() # Ensure float for mean calculation\n",
    "        avg_freq_maps[code] = torch.mean(stacked_chunks, dim=0)\n",
    "    return avg_freq_maps\n",
    "\n",
    "\n",
    "def calculate_average_block_frequency_maps(\n",
    "    style_chunks_dict: dict,\n",
    "    num_block_types: int\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Calculates the average blocktype frequency map for each style code,\n",
    "    starting from raw 3D chunks of blockID ints.\n",
    "\n",
    "    Args:\n",
    "        style_chunks_dict (dict):\n",
    "            Mapping style_code (int)  list of torch.Tensor of shape (H, W, D),\n",
    "            where each entry is an integer in [0 .. num_block_types-1].\n",
    "        num_block_types (int):\n",
    "            Total number C of distinct block types / channels.\n",
    "\n",
    "    Returns:\n",
    "        dict:\n",
    "            Mapping style_code (int)  1D torch.float tensor of length C,\n",
    "            giving the average frequency of each block type (01).\n",
    "    \"\"\"\n",
    "    if not style_chunks_dict:\n",
    "        return {}\n",
    "\n",
    "    avg_block_freq_maps = {}\n",
    "    for code, chunks in style_chunks_dict.items():\n",
    "        if not chunks:\n",
    "            print(f\"Warning: Code {code} has no associated chunks. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Stack raw index tensors  shape (N, H, W, D)\n",
    "        stacked_idx = torch.stack(chunks).long()\n",
    "        N, H, W, D = stacked_idx.shape\n",
    "\n",
    "        # Onehot encode  shape (N, H, W, D, C)\n",
    "        onehot = F.one_hot(stacked_idx, num_classes=num_block_types).float()\n",
    "\n",
    "        # Sum over batch + spatial dims  (C,)\n",
    "        counts = onehot.sum(dim=(0, 1, 2, 3))\n",
    "\n",
    "        # Normalize by total voxels (N * H*W*D)\n",
    "        avg_block_freq_maps[code] = counts / (N * H * W * D)\n",
    "\n",
    "    return avg_block_freq_maps\n",
    "\n",
    "def calculate_sharpness_mad(avg_freq_maps):\n",
    "    \"\"\"\n",
    "    Calculates sharpness using Mean Absolute Deviation from 0.5 for each code's avg freq map.\n",
    "\n",
    "    Args:\n",
    "        avg_freq_maps (dict): Dictionary mapping codes to avg freq maps (4x4x4 float tensors).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (dict mapping code to sharpness score, float overall average sharpness)\n",
    "               Returns ({}, 0.0) if input is empty.\n",
    "    \"\"\"\n",
    "    if not avg_freq_maps:\n",
    "        return {}, 0.0\n",
    "\n",
    "    sharpness_scores = {}\n",
    "    total_sharpness = 0.0\n",
    "    for code, freq_map in avg_freq_maps.items():\n",
    "        # Calculate |F_i[x,y,z] - 0.5| for all voxels and average\n",
    "        mad = torch.mean(torch.abs(freq_map - 0.5))\n",
    "        sharpness_scores[code] = mad.item()\n",
    "        total_sharpness += sharpness_scores[code]\n",
    "\n",
    "    average_sharpness = total_sharpness / len(sharpness_scores) if sharpness_scores else 0.0\n",
    "    return sharpness_scores, average_sharpness\n",
    "\n",
    "def calculate_sharpness_entropy(avg_freq_maps, epsilon=1e-9):\n",
    "    \"\"\"\n",
    "    Calculates sharpness using binary entropy for each voxel in the avg freq map.\n",
    "\n",
    "    Args:\n",
    "        avg_freq_maps (dict): Dictionary mapping codes to avg freq maps (4x4x4 float tensors).\n",
    "        epsilon (float): Small value to avoid log(0).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (dict mapping code to sharpness entropy score, float overall average sharpness entropy)\n",
    "               Returns ({}, 0.0) if input is empty.\n",
    "    \"\"\"\n",
    "    if not avg_freq_maps:\n",
    "        return {}, 0.0\n",
    "\n",
    "    entropy_scores = {}\n",
    "    total_entropy = 0.0\n",
    "    for code, freq_map in avg_freq_maps.items():\n",
    "        # Clamp values to avoid log(0)\n",
    "        p = torch.clamp(freq_map, epsilon, 1.0 - epsilon)\n",
    "        # Calculate binary entropy: -p*log2(p) - (1-p)*log2(1-p)\n",
    "        voxel_entropies = -p * torch.log2(p) - (1.0 - p) * torch.log2(1.0 - p)\n",
    "        avg_entropy = torch.mean(voxel_entropies)\n",
    "        entropy_scores[code] = avg_entropy.item()\n",
    "        total_entropy += entropy_scores[code]\n",
    "\n",
    "    average_entropy = total_entropy / len(entropy_scores) if entropy_scores else 0.0\n",
    "    return entropy_scores, average_entropy\n",
    "\n",
    "def calculate_consistency_variance(binary_structure_dict):\n",
    "    \"\"\"\n",
    "    Calculates consistency using the average voxel variance across chunks for each code.\n",
    "\n",
    "    Args:\n",
    "        binary_structure_dict (dict): Dictionary mapping codes to lists of 4x4x4 binary tensors.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (dict mapping code to consistency variance score, float overall average consistency variance)\n",
    "               Returns ({}, 0.0) if input is empty. Codes with < 2 chunks have variance 0.\n",
    "    \"\"\"\n",
    "    if not binary_structure_dict:\n",
    "        return {}, 0.0\n",
    "\n",
    "    variance_scores = {}\n",
    "    total_variance = 0.0\n",
    "    num_valid_codes = 0\n",
    "    for code, chunk_list in binary_structure_dict.items():\n",
    "        if len(chunk_list) < 2: # Variance requires at least 2 samples\n",
    "             # Assign 0 variance, but maybe handle this differently? (e.g. skip?)\n",
    "            variance_scores[code] = 0.0\n",
    "            # print(f\"Warning: Code {code} has < 2 chunks. Assigning variance 0.\") # Optional warning\n",
    "            continue # Skip adding to total_variance if we want average over codes with enough data\n",
    "\n",
    "        stacked_chunks = torch.stack(chunk_list).float()\n",
    "        # Calculate variance across the chunks (dim=0) for each voxel\n",
    "        voxel_variances = torch.var(stacked_chunks, dim=0, unbiased=False) # Use population variance\n",
    "        avg_variance = torch.mean(voxel_variances)\n",
    "        variance_scores[code] = avg_variance.item()\n",
    "        total_variance += variance_scores[code]\n",
    "        num_valid_codes += 1 # Only count codes where variance could be computed\n",
    "\n",
    "    average_variance = total_variance / num_valid_codes if num_valid_codes > 0 else 0.0\n",
    "    return variance_scores, average_variance\n",
    "\n",
    "\n",
    "def calculate_consistency_entropy(binary_structure_dict, epsilon=1e-9):\n",
    "    \"\"\"\n",
    "    Calculates consistency using the average entropy of voxel distributions across chunks for each code.\n",
    "    This relies on calculating the probability p(voxel=1) at each position first.\n",
    "\n",
    "    Args:\n",
    "        binary_structure_dict (dict): Dictionary mapping codes to lists of 4x4x4 binary tensors.\n",
    "        epsilon (float): Small value to avoid log(0).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (dict mapping code to consistency entropy score, float overall average consistency entropy)\n",
    "               Returns ({}, 0.0) if input is empty. Codes with no chunks are skipped.\n",
    "    \"\"\"\n",
    "    # This metric is essentially the same as sharpness entropy applied to the avg freq maps\n",
    "    # because the entropy H(p) depends only on the mean frequency p at that voxel.\n",
    "    # H(p) = -p*log2(p) - (1-p)*log2(1-p), where p = mean(voxel_values_for_code)\n",
    "    # So we can reuse calculate_sharpness_entropy with the average frequency maps.\n",
    "    avg_freq_maps = calculate_average_frequency_maps(binary_structure_dict)\n",
    "    return calculate_sharpness_entropy(avg_freq_maps, epsilon)\n",
    "\n",
    "\n",
    "def calculate_uniqueness_metrics(avg_freq_maps, distance_metric='mae'):\n",
    "    \"\"\"\n",
    "    Calculates uniqueness metrics (average and minimum pairwise distance) between avg freq maps.\n",
    "\n",
    "    Args:\n",
    "        avg_freq_maps (dict): Dictionary mapping codes to avg freq maps (4x4x4 float tensors).\n",
    "        distance_metric (str): 'mae' (Mean Absolute Error) or 'mse' (Mean Squared Error).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (float average pairwise distance, float minimum pairwise distance)\n",
    "               Returns (0.0, 0.0) if less than 2 codes exist.\n",
    "    \"\"\"\n",
    "    codes = list(avg_freq_maps.keys())\n",
    "    if len(codes) < 2:\n",
    "        return 0.0, 0.0 # Cannot compare pairs if less than 2 codes\n",
    "\n",
    "    total_distance = 0.0\n",
    "    min_distance = float('inf')\n",
    "    num_pairs = 0\n",
    "\n",
    "    for code1, code2 in itertools.combinations(codes, 2):\n",
    "        map1 = avg_freq_maps[code1]\n",
    "        map2 = avg_freq_maps[code2]\n",
    "\n",
    "        if distance_metric == 'mae':\n",
    "            distance = torch.mean(torch.abs(map1 - map2)).item()\n",
    "        elif distance_metric == 'mse':\n",
    "            distance = torch.mean((map1 - map2)**2).item()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported distance_metric. Choose 'mae' or 'mse'.\")\n",
    "\n",
    "        total_distance += distance\n",
    "        min_distance = min(min_distance, distance)\n",
    "        num_pairs += 1\n",
    "\n",
    "    average_distance = total_distance / num_pairs if num_pairs > 0 else 0.0\n",
    "    # Handle case where min_distance wasn't updated (e.g., only one code pair and it was identical)\n",
    "    if min_distance == float('inf'):\n",
    "        min_distance = 0.0\n",
    "\n",
    "\n",
    "    return average_distance, min_distance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code level metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar_codes(avg_freq_maps, distance_metric='mae', tolerance=1e-5):\n",
    "    \"\"\"\n",
    "    Finds the pair(s) of codes with the minimum pairwise distance between their avg freq maps.\n",
    "\n",
    "    Args:\n",
    "        avg_freq_maps (dict): Dictionary mapping codes to avg freq maps (4x4x4 float tensors).\n",
    "        distance_metric (str): 'mae' (Mean Absolute Error) or 'mse' (Mean Squared Error).\n",
    "        tolerance (float): Tolerance for considering distances equal to the minimum.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (float minimum_distance, list of tuples containing the most similar code pairs [(code1, code2), ...])\n",
    "               Returns (inf, []) if less than 2 codes exist.\n",
    "    \"\"\"\n",
    "    codes = list(avg_freq_maps.keys())\n",
    "    if len(codes) < 2:\n",
    "        return float('inf'), []\n",
    "\n",
    "    min_distance = float('inf')\n",
    "    similar_pairs = []\n",
    "\n",
    "    # First pass to find the minimum distance\n",
    "    for code1, code2 in itertools.combinations(codes, 2):\n",
    "        map1 = avg_freq_maps[code1]\n",
    "        map2 = avg_freq_maps[code2]\n",
    "\n",
    "        if distance_metric == 'mae':\n",
    "            distance = torch.mean(torch.abs(map1 - map2)).item()\n",
    "        elif distance_metric == 'mse':\n",
    "            distance = torch.mean((map1 - map2)**2).item()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported distance_metric. Choose 'mae' or 'mse'.\")\n",
    "        min_distance = min(min_distance, distance)\n",
    "\n",
    "    # Handle case where min_distance wasn't updated (e.g., only one code pair and it was identical)\n",
    "    if min_distance == float('inf'):\n",
    "        # This case should ideally not happen if len(codes) >= 2,\n",
    "        # unless maybe all maps are identical? Check one pair.\n",
    "         if len(codes) >= 2:\n",
    "             code1, code2 = codes[0], codes[1]\n",
    "             map1 = avg_freq_maps[code1]\n",
    "             map2 = avg_freq_maps[code2]\n",
    "             if distance_metric == 'mae': distance = torch.mean(torch.abs(map1 - map2)).item()\n",
    "             else: distance = torch.mean((map1 - map2)**2).item()\n",
    "             min_distance = distance\n",
    "         else: # Should not be reachable due to initial check\n",
    "             return float('inf'), []\n",
    "\n",
    "\n",
    "    # Second pass to collect all pairs at (or very close to) the minimum distance\n",
    "    for code1, code2 in itertools.combinations(codes, 2):\n",
    "        map1 = avg_freq_maps[code1]\n",
    "        map2 = avg_freq_maps[code2]\n",
    "\n",
    "        if distance_metric == 'mae':\n",
    "            distance = torch.mean(torch.abs(map1 - map2)).item()\n",
    "        elif distance_metric == 'mse':\n",
    "            distance = torch.mean((map1 - map2)**2).item()\n",
    "        else: # Should not happen if first pass succeeded\n",
    "             raise ValueError(\"Unsupported distance_metric.\")\n",
    "\n",
    "\n",
    "        if abs(distance - min_distance) < tolerance:\n",
    "            similar_pairs.append(tuple(sorted((code1, code2)))) # Store sorted pairs\n",
    "\n",
    "    # Deduplicate pairs (if any floating point issues caused near duplicates)\n",
    "    similar_pairs = sorted(list(set(similar_pairs)))\n",
    "\n",
    "\n",
    "    return min_distance, similar_pairs\n",
    "\n",
    "def analyze_per_code_metrics(\n",
    "    code_sharpness_mad,\n",
    "    code_sharpness_entropy,\n",
    "    code_consistency_var,\n",
    "    avg_freq_maps, # Needed to get the list of codes that actually appeared\n",
    "    num_chunks_per_code, # Add a dict mapping code -> number of chunks\n",
    "    sharp_mad_threshold=0.3, # Example threshold: Lower than this might be \"blurry\"\n",
    "    sharp_entropy_threshold=0.5, # Example threshold: Higher than this might be \"blurry\"\n",
    "    cons_var_threshold=0.15, # Example threshold: Higher than this might be \"inconsistent\"\n",
    "    min_chunks_threshold=10 # Example threshold: Codes used less than this might be unreliable\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Prints sharpness and consistency metrics for each code and flags potential issues.\n",
    "\n",
    "    Args:\n",
    "        code_sharpness_mad (dict): Code -> Sharpness (MAD) score.\n",
    "        code_sharpness_entropy (dict): Code -> Sharpness (Entropy) score.\n",
    "                                       (Also used for Consistency Entropy).\n",
    "        code_consistency_var (dict): Code -> Consistency (Variance) score.\n",
    "        avg_freq_maps (dict): Code -> Average Frequency Map. Used to get active codes.\n",
    "        num_chunks_per_code(dict): Code -> integer count of chunks assigned.\n",
    "        sharp_mad_threshold (float): Threshold below which MAD is flagged low.\n",
    "        sharp_entropy_threshold (float): Threshold above which Entropy is flagged high.\n",
    "        cons_var_threshold (float): Threshold above which Variance is flagged high.\n",
    "        min_chunks_threshold (int): Threshold below which code usage is flagged low.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Per-Code Analysis ---\")\n",
    "    codes = sorted(list(avg_freq_maps.keys()))\n",
    "\n",
    "    if not codes:\n",
    "        print(\"No codes found in avg_freq_maps.\")\n",
    "        return\n",
    "\n",
    "    print(f\"{'Code':<6} {'NumChunks':<10} {'Sharp(MAD)':<12} {'Sharp(Entr)':<12} {'Cons(Var)':<12} {'Flags':<20}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for code in codes:\n",
    "        sharp_mad = code_sharpness_mad.get(code, float('nan'))\n",
    "        sharp_entropy = code_sharpness_entropy.get(code, float('nan'))\n",
    "        cons_var = code_consistency_var.get(code, float('nan')) # Variance might be missing if < 2 chunks\n",
    "        n_chunks = num_chunks_per_code.get(code, 0)\n",
    "\n",
    "\n",
    "        flags = []\n",
    "        if n_chunks < min_chunks_threshold:\n",
    "             flags.append(f\"LOW_USAGE({n_chunks})\")\n",
    "        if sharp_mad < sharp_mad_threshold:\n",
    "            flags.append(\"LOW_SHARPNESS_MAD\")\n",
    "        if sharp_entropy > sharp_entropy_threshold:\n",
    "            flags.append(\"HIGH_SHARPNESS_ENT\")\n",
    "        if cons_var > cons_var_threshold:\n",
    "            flags.append(\"HIGH_CONSIST_VAR\")\n",
    "        # Note: Consistency Entropy is same as Sharpness Entropy here\n",
    "\n",
    "        # Handle cases where variance wasn't computed (e.g., < 2 chunks)\n",
    "        cons_var_str = f\"{cons_var:<12.4f}\" if not torch.isnan(torch.tensor(cons_var)) else f\"{'N/A':<12}\" # Use torch.isnan for tensor check compatibility\n",
    "\n",
    "        print(f\"{code:<6} {n_chunks:<10} {sharp_mad:<12.4f} {sharp_entropy:<12.4f} {cons_var_str} {', '.join(flags)}\")\n",
    "\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4x4 frequency heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch # Assuming input is torch tensor\n",
    "\n",
    "def plot_frequency_heatmap(prob_matrix_tensor, struct_code, save_dir):\n",
    "    \"\"\"\n",
    "    Generates and saves a 3D frequency heatmap visualization for a structure code,\n",
    "    applying Minecraft-specific orientation adjustments. Uses the 'viridis' colormap\n",
    "    where color represents probability and ensures Y (Height) axis is vertical and correctly oriented.\n",
    "\n",
    "    Args:\n",
    "        prob_matrix_tensor (torch.Tensor): The 4x4x4 frequency map (PyTorch tensor).\n",
    "        struct_code (int): The structure code ID.\n",
    "        save_dir (str): The directory to save the heatmap image.\n",
    "    \"\"\"\n",
    "    # --- Input Validation and Conversion ---\n",
    "    if not isinstance(prob_matrix_tensor, torch.Tensor) or prob_matrix_tensor.shape != (4, 4, 4):\n",
    "        print(f\"Error: Input for code {struct_code} is not a 4x4x4 torch tensor.\")\n",
    "        return\n",
    "\n",
    "    prob_matrix_np = prob_matrix_tensor.cpu().numpy()\n",
    "\n",
    "    # --- Orientation Adjustment ---\n",
    "    # Apply the same sequence: Transpose Z <-> X, then rotate around Z.\n",
    "    prob_matrix_transposed = prob_matrix_np.transpose(2, 0, 1) # (X,Y,Z) -> (Z,X,Y)\n",
    "    prob_matrix_oriented = np.rot90(prob_matrix_transposed, k=1, axes=(1, 2)) # (Z,X,Y) -> (Z,Y_mc,X_mc)\n",
    "\n",
    "    # --- Plotting Setup ---\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    fig = plt.figure(figsize=(9, 8)) # Adjusted size slightly for colorbar\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Generate grid indices corresponding to the oriented matrix axes (Z_mc, Y_mc, X_mc)\n",
    "    z_indices, y_indices, x_indices = np.indices(prob_matrix_oriented.shape)\n",
    "\n",
    "    # Flatten coordinates and probability values\n",
    "    x_coords_mc = x_indices.flatten() # Minecraft X coordinates\n",
    "    y_coords_mc = y_indices.flatten() # Minecraft Y (Height) coordinates\n",
    "    z_coords_mc = z_indices.flatten() # Minecraft Z coordinates\n",
    "    probabilities = prob_matrix_oriented.flatten()\n",
    "\n",
    "    # --- Create Scatter Plot (Viridis Colormap) ---\n",
    "    # Plot mapping:\n",
    "    # Plot X-axis <- Minecraft X data (x_coords_mc)\n",
    "    # Plot Y-axis <- Minecraft Z data (z_coords_mc)\n",
    "    # Plot Z-axis <- Minecraft Y (Height) data (y_coords_mc) <<< VERTICAL AXIS\n",
    "    scatter = ax.scatter(x_coords_mc, z_coords_mc, y_coords_mc,\n",
    "                         c=probabilities, # Color based on probability values\n",
    "                         cmap='viridis',  # Use the viridis colormap\n",
    "                         vmax=1,\n",
    "                         vmin=0,\n",
    "                         s=300,\n",
    "                         edgecolors='k', linewidth=0.5)\n",
    "\n",
    "    # --- Add Colorbar ---\n",
    "    cbar = fig.colorbar(scatter, ax=ax, shrink=0.6, aspect=10)\n",
    "    cbar.set_label('Probability')\n",
    "\n",
    "    # --- Set Labels, Ticks, Limits, Title, and INVERT Z-AXIS ---\n",
    "    ax.set_title(f\"Structure Code {struct_code} Frequency Map (Color = Probability)\") # Updated title\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Z\")\n",
    "    ax.set_zlabel(\"Y (Height)\")\n",
    "\n",
    "    ax.set_xticks(np.arange(4))\n",
    "    ax.set_yticks(np.arange(4)) # Corresponds to Z data range\n",
    "    ax.set_zticks(np.arange(4)) # Corresponds to Y (Height) data range\n",
    "\n",
    "    ax.set_xlim(-0.5, 3.5)\n",
    "    ax.set_ylim(-0.5, 3.5)\n",
    "    ax.set_zlim(-0.5, 3.5)\n",
    "    # --- Invert the Z-axis (which represents Y-Height) ---\n",
    "    ax.invert_zaxis()\n",
    "    # --- Optionally invert X too depending on preferred view ---\n",
    "    # ax.invert_xaxis()\n",
    "\n",
    "    # Adjust view angle if desired\n",
    "    ax.view_init(elev=25., azim=-125)\n",
    "\n",
    "    # --- Save and Close ---\n",
    "    image_path = os.path.join(save_dir, f\"heatmap_code_{struct_code}.png\")\n",
    "    try:\n",
    "        plt.savefig(image_path, bbox_inches='tight', dpi=150)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving heatmap for code {struct_code}: {e}\")\n",
    "    finally:\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## block count frequency heatmap (style codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_block_frequency_heatmap(\n",
    "    style_code: int,\n",
    "    block_freqs,                    # torch.Tensor or array of shape (C,)\n",
    "    save_dir: str,\n",
    "    block_converter,                # object with index_to_block(idx) -> blockID\n",
    "    block_index_to_name: dict,      # maps Minecraft blockID  block name\n",
    "    cmap: str = \"Greys\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot and save a 1C heatmap of blocktype frequencies for one style code.\n",
    "\n",
    "    Args:\n",
    "        style_code (int): Identifier of the style code.\n",
    "        block_freqs (Tensor or ndarray): 1D of length C, values in [0,1].\n",
    "        save_dir (str): Directory where the PNG will be written.\n",
    "        block_converter: Object with method index_to_block(idx) returning\n",
    "                         the Minecraft blockID for channel idx.\n",
    "        block_index_to_name (dict): Mapping from blockID to block name.\n",
    "        cmap (str): Matplotlib colormap name.\n",
    "    \"\"\"\n",
    "    # ensure numpy array on CPU\n",
    "    if isinstance(block_freqs, torch.Tensor):\n",
    "        # print('converting to numpy')\n",
    "        freqs = block_freqs.detach().cpu().numpy()\n",
    "    else:\n",
    "        print('already numpy')\n",
    "        freqs = np.array(block_freqs)\n",
    "\n",
    "    # print(freqs)\n",
    "    # print(freqs.shape)\n",
    "    C = freqs.shape[0]\n",
    "    heatmap = freqs.reshape(C, 1)   # shape (1, C)\n",
    "\n",
    "    # Map each channel idx  Minecraft blockID  block name\n",
    "    block_ids = [i for i in range(C)]\n",
    "    print(block_ids)\n",
    "    names = [block_index_to_name.get(bid, str(bid)) for bid in block_ids]\n",
    "\n",
    "    # ensure output dir exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    image_path = os.path.join(save_dir, f\"block_heatmap_code_{style_code}.png\")\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(3, max(8, C * 0.2)))\n",
    "    im = plt.imshow(\n",
    "        heatmap,\n",
    "        aspect='auto',\n",
    "        cmap=cmap,\n",
    "        vmin=0.0, vmax=1.0         # fixed scale across all codes\n",
    "    )\n",
    "     # ---- Bigger, labeled colorbar ----\n",
    "    cbar = plt.colorbar(\n",
    "        im,\n",
    "        orientation='vertical',\n",
    "        fraction=0.12,    # thicker bar\n",
    "        pad=0.05          # more space from the heatmap\n",
    "    )\n",
    "    cbar.ax.tick_params(labelsize=8)               # larger tick labels\n",
    "    cbar.set_label('Frequency', rotation=270, labelpad=15)\n",
    "\n",
    "    plt.yticks(np.arange(C), names, fontsize=6)\n",
    "    # only one column, no X-ticks needed\n",
    "    plt.xticks([])\n",
    "\n",
    "    plt.title(f\"Style Code {style_code} Block Frequencies\", pad=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(image_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    return image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_positional_frequencies(fqgan):\n",
    "    latent_depth, latent_height, latent_width = 6, 6, 6 # *** Adjust if different ***\n",
    "    latent_shape = (latent_depth, latent_height, latent_width)\n",
    "    num_structure_codes = fqgan.struct_codebook_size\n",
    "    num_style_codes = fqgan.style_codebook_size\n",
    "    # Initialize count tensors for each code\n",
    "    struct_position_counts = {\n",
    "        code: torch.zeros(latent_shape, dtype=torch.long, device='cpu')\n",
    "        for code in range(num_structure_codes)\n",
    "    }\n",
    "    style_position_counts = {\n",
    "        code: torch.zeros(latent_shape, dtype=torch.long, device='cpu')\n",
    "        for code in range(num_style_codes)\n",
    "    }\n",
    "    total_samples_processed = 0\n",
    "\n",
    "    print(\"Collecting positional frequencies from train_loader...\")\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Limit batches for testing?\n",
    "        # if batch_idx > 20: break\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            batch = batch.cuda()\n",
    "\n",
    "        # Only need the encoding part\n",
    "        style_indices, struct_indices = encode_and_quantize(fqgan, batch) # Get indices for the whole batch\n",
    "\n",
    "        # Process each sample in the batch\n",
    "        for sample_idx in range(struct_indices.shape[0]): # Iterate through batch dimension\n",
    "            struct_indices_sample = struct_indices[sample_idx].cpu() # Get indices for one sample, move to CPU\n",
    "\n",
    "            # Iterate through the latent grid dimensions (D, H, W)\n",
    "            for i in range(latent_depth):\n",
    "                for j in range(latent_height):\n",
    "                    for k in range(latent_width):\n",
    "                        struct_code = struct_indices_sample[i, j, k].item()\n",
    "                        if 0 <= struct_code < num_structure_codes:\n",
    "                            position_counts[struct_code][i, j, k] += 1\n",
    "                        else:\n",
    "                            print(f\"Warning: Encountered out-of-bounds code {struct_code} at ({i},{j},{k})\")\n",
    "            total_samples_processed += 1\n",
    "\n",
    "        # Optional: Print progress\n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "                print(f\"Processed {batch_idx + 1} batches...\")\n",
    "    return position_counts\n",
    "\n",
    "def plot_positional_frequency(position_counts, struct_code, save_dir):\n",
    "    \"\"\"\n",
    "    Generates and saves a 3D scatter plot showing the positional frequency\n",
    "    of a structure code within the latent grid, using a Viridis colormap and\n",
    "    Minecraft coordinate conventions (Y=Height is vertical, Y=0 is bottom).\n",
    "\n",
    "    Args:\n",
    "        position_counts (dict): Maps struct_code to 6x6x6 count tensor.\n",
    "        struct_code (int): The structure code to visualize.\n",
    "        save_dir (str): The directory to save the plot image.\n",
    "    \"\"\"\n",
    "    if struct_code not in position_counts:\n",
    "        print(f\"Error: Code {struct_code} not found in position_counts dictionary.\")\n",
    "        return\n",
    "\n",
    "    counts_tensor = position_counts[struct_code].cpu() # Ensure it's on CPU\n",
    "    latent_shape = counts_tensor.shape\n",
    "    if len(latent_shape) != 3:\n",
    "        print(f\"Error: Count tensor for code {struct_code} is not 3D (shape: {latent_shape}).\")\n",
    "        return\n",
    "\n",
    "    counts_numpy = counts_tensor.numpy()\n",
    "    max_count = np.max(counts_numpy)\n",
    "\n",
    "    if max_count == 0:\n",
    "        print(f\"Info: Code {struct_code} never appeared. Skipping visualization.\")\n",
    "        # Optional: Create an empty plot placeholder if desired\n",
    "        return # Stop here if code never appeared\n",
    "\n",
    "    # --- Plotting Setup ---\n",
    "    # os.makedirs(save_dir, exist_ok=True)\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Generate grid indices (I, J, K) corresponding to tensor dimensions\n",
    "    # Assume I=Depth(Z_mc), J=Height(Y_mc), K=Width(X_mc)\n",
    "    i_indices, j_indices, k_indices = np.indices(latent_shape)\n",
    "\n",
    "    # Flatten coordinates and the raw counts (for color mapping)\n",
    "    i_coords_mc = i_indices.flatten() # Minecraft Z coordinates\n",
    "    j_coords_mc = j_indices.flatten() # Minecraft Y (Height) coordinates\n",
    "    k_coords_mc = k_indices.flatten() # Minecraft X coordinates\n",
    "    frequencies = counts_numpy.flatten()\n",
    "\n",
    "    # --- Create Scatter Plot (Viridis Colormap, Correct Axis Mapping) ---\n",
    "    # cmap = plt.get_cmap('viridis')\n",
    "    cmap = plt.get_cmap('Greys')\n",
    "    # Plot mapping:\n",
    "    # Plot X-axis <- Minecraft X data (k_coords_mc)\n",
    "    # Plot Y-axis <- Minecraft Z data (i_coords_mc)\n",
    "    # Plot Z-axis <- Minecraft Y (Height) data (j_coords_mc) <<< VERTICAL AXIS\n",
    "    scatter = ax.scatter(k_coords_mc, i_coords_mc, j_coords_mc, # Correct mapping\n",
    "                         c=frequencies, cmap=cmap, # Color based on frequency counts\n",
    "                         s=150, # Adjust size as needed\n",
    "                         alpha=0.8, # Add some transparency\n",
    "                         # vmin=0, vmax=max_count, # Optional: Explicitly set color limits\n",
    "                         edgecolors='grey', linewidth=0.5)\n",
    "\n",
    "    # --- Add Colorbar ---\n",
    "    cbar = fig.colorbar(scatter, ax=ax, shrink=0.6, aspect=20, pad=0.1)\n",
    "    cbar.set_label(\"Code Occurrence Count\")\n",
    "\n",
    "    # --- Set Labels, Ticks, Limits, Title, and Invert Z-axis ---\n",
    "    ax.set_title(f\"Code {struct_code} Positional Frequency\")\n",
    "    # Label plot axes according to the *Minecraft dimension* plotted on them\n",
    "    ax.set_xlabel(\"Z (Latent Dim K)\")\n",
    "    ax.set_ylabel(\"X (Latent Dim I)\")\n",
    "    ax.set_zlabel(\"Y (Height, Latent Dim J)\") # Vertical axis\n",
    "\n",
    "    # Set ticks based on the dimension size\n",
    "    ax.set_xticks(np.arange(latent_shape[2])) # K dimension\n",
    "    ax.set_yticks(np.arange(latent_shape[0])) # I dimension\n",
    "    ax.set_zticks(np.arange(latent_shape[1])) # J dimension\n",
    "\n",
    "    # Set limits for plot axes\n",
    "    ax.set_xlim(-0.5, latent_shape[2] - 0.5)\n",
    "    ax.set_ylim(-0.5, latent_shape[0] - 0.5)\n",
    "    ax.set_zlim(-0.5, latent_shape[1] - 0.5)\n",
    "\n",
    "    # --- Invert the Z-axis (which represents Y-Height) ---\n",
    "    # ax.invert_zaxis() # Ensures Y=0 is at the bottom\n",
    "    ax.invert_yaxis() # Ensures Y=0 is at the bottom\n",
    "\n",
    "    # Adjust view angle\n",
    "    ax.view_init(elev=20., azim=-75)\n",
    "    # plt.show()\n",
    "    # --- Save and Close ---\n",
    "    image_path = os.path.join(save_dir, f\"pos_freq_code_{struct_code}.png\")\n",
    "    try:\n",
    "        plt.savefig(image_path, bbox_inches='tight', dpi=150)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving positional frequency plot for code {struct_code}: {e}\")\n",
    "    finally:\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "def generate_all_positional_frequency_plots(position_counts, output_dir):\n",
    "    print(f\"Generating positional frequency plots for {len(position_counts)} codes...\")\n",
    "    os.makedirs(output_dir, exist_ok=True) # Ensure directory exists\n",
    "\n",
    "    # Iterate through the codes present in the dictionary\n",
    "    codes_to_plot = sorted(position_counts.keys())\n",
    "\n",
    "    for i, code in enumerate(codes_to_plot):\n",
    "        # Call the plotting function for the current code\n",
    "        plot_positional_frequency(position_counts, code, output_dir)\n",
    "\n",
    "        # Optional: Print progress\n",
    "        if (i + 1) % 10 == 0 or (i + 1) == len(codes_to_plot):\n",
    "            print(f\"Generated plot {i+1}/{len(codes_to_plot)} (Code {code})\")\n",
    "\n",
    "    print(f\"Finished generating positional frequency plots. Saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Example Usage ---\n",
    "# # Assuming binary_structure_dict is populated as shown in your notebook snippet\n",
    "\n",
    "# # 1. Calculate average frequency maps (needed for several metrics)\n",
    "# avg_freq_maps = calculate_average_frequency_maps(binary_structure_dict)\n",
    "\n",
    "# # 2. Calculate Sharpness (MAD)\n",
    "# code_sharpness_mad, model_avg_sharpness_mad = calculate_sharpness_mad(avg_freq_maps)\n",
    "# print(f\"Model Average Sharpness (MAD): {model_avg_sharpness_mad}\")\n",
    "\n",
    "# # 3. Calculate Sharpness (Entropy)\n",
    "# code_sharpness_entropy, model_avg_sharpness_entropy = calculate_sharpness_entropy(avg_freq_maps)\n",
    "# print(f\"Model Average Sharpness (Entropy): {model_avg_sharpness_entropy}\") # Lower is better\n",
    "\n",
    "# # 4. Calculate Consistency (Variance)\n",
    "# code_consistency_var, model_avg_consistency_var = calculate_consistency_variance(binary_structure_dict)\n",
    "# print(f\"Model Average Consistency (Variance): {model_avg_consistency_var}\") # Lower is better\n",
    "\n",
    "# # 5. Calculate Consistency (Entropy) - Note: This reuses sharpness entropy calculation\n",
    "# code_consistency_entropy, model_avg_consistency_entropy = calculate_consistency_entropy(binary_structure_dict)\n",
    "# print(f\"Model Average Consistency (Entropy): {model_avg_consistency_entropy}\") # Lower is better\n",
    "\n",
    "# # 6. Calculate Uniqueness\n",
    "# model_avg_pairwise_dist, model_min_pairwise_dist = calculate_uniqueness_metrics(avg_freq_maps, distance_metric='mae')\n",
    "# print(f\"Model Average Pairwise Distance (MAE): {model_avg_pairwise_dist}\") # Higher is better\n",
    "# print(f\"Model Minimum Pairwise Distance (MAE): {model_min_pairwise_dist}\") # Higher is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # avg_freq_maps = calculate_average_frequency_maps(binary_structure_dict)\n",
    "# # code_sharpness_mad, model_avg_sharpness_mad = calculate_sharpness_mad(avg_freq_maps)\n",
    "# # code_sharpness_entropy, model_avg_sharpness_entropy = calculate_sharpness_entropy(avg_freq_maps)\n",
    "# # code_consistency_var, model_avg_consistency_var = calculate_consistency_variance(binary_structure_dict)\n",
    "# # model_avg_pairwise_dist, model_min_pairwise_dist = calculate_uniqueness_metrics(avg_freq_maps) # Needed for context below\n",
    "\n",
    "# # You also need the number of chunks per code:\n",
    "# num_chunks_per_code = {code: len(chunks) for code, chunks in binary_structure_dict.items()}\n",
    "\n",
    "# # 1. Print the per-code metrics and flags\n",
    "# analyze_per_code_metrics(\n",
    "#     code_sharpness_mad,\n",
    "#     code_sharpness_entropy,\n",
    "#     code_consistency_var,\n",
    "#     avg_freq_maps,\n",
    "#     num_chunks_per_code\n",
    "# )\n",
    "\n",
    "# # 2. Find and print the most similar code pairs (redundant codes)\n",
    "# min_dist, similar_pairs = find_most_similar_codes(avg_freq_maps, distance_metric='mae')\n",
    "# print(f\"\\n--- Code Uniqueness Analysis ---\")\n",
    "# print(f\"Minimum Pairwise Distance (MAE): {min_dist:.6f}\")\n",
    "# if similar_pairs:\n",
    "#     print(\"Most Similar Code Pairs (Potential Redundancy):\")\n",
    "#     for pair in similar_pairs:\n",
    "#         print(f\"  - Codes {pair[0]} and {pair[1]}\")\n",
    "# else:\n",
    "#     print(\"No highly similar code pairs found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run evaluation sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sweep params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "CHECKPOINT_STEPS = [5000, 10000, 15000, 20000, 24999] # Example list of checkpoint steps to evaluate\n",
    "MODEL_BASE_PATH = \"../model_logs/FQGAN_2stagedecoder_logweighted3_32codes_cycleconsistency_STE_postquant_detachcycle_gumbel\" # *** CHANGE THIS ***\n",
    "# MODEL_HPARAMS_FILE = \"/path/to/your/hparams.json\" # *** CHANGE THIS ***\n",
    "OUTPUT_DIR = \"results/interpretability_analysis17\"\n",
    "# Thresholds for analyze_per_code_metrics (can be adjusted)\n",
    "SHARP_MAD_THRESHOLD = 0.3\n",
    "SHARP_ENTROPY_THRESHOLD = 0.5\n",
    "CONS_VAR_THRESHOLD = 0.15\n",
    "MIN_CHUNKS_THRESHOLD = 10\n",
    "DISTANCE_METRIC = 'mae' # For uniqueness metrics\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_data = {} # Store metrics for all checkpoints: {step: metrics_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_for_step(step, base_path):\n",
    "    \"\"\"Loads the FQGAN model for a specific checkpoint step.\"\"\"\n",
    "    print(f\"\\n--- Loading Checkpoint Step: {step} ---\")\n",
    "    try:\n",
    "        fqgan_hparams = dict_to_vcqgan_hparams(load_hparams_from_json(f\"{base_path}\"), 'minecraft')\n",
    "        fqgan_hparams.load_step = step # Set the step to load\n",
    "        fqgan = FQModel(fqgan_hparams)\n",
    "        fqgan = load_fqgan_from_checkpoint(fqgan_hparams, fqgan) \n",
    "        print(f'Loaded model for step {step}')\n",
    "        fqgan.eval() # Set to evaluation mode\n",
    "        if torch.cuda.is_available():\n",
    "             fqgan.cuda()\n",
    "        return fqgan\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model for step {step}: {e}\")\n",
    "        # Depending on the error, you might want to skip this step or stop\n",
    "        return None # Indicate failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run dataset through model, save dictionary of codes and corresponding chunks\n",
    "TODO: add handling for the binary reconstruction & style codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # Ensure no gradients are computed\n",
    "def collect_data_for_model(model, loader, block_converter):\n",
    "    \"\"\"\n",
    "    Collects structure dictionary data (original and binary) for a given model.\n",
    "    Returns original structure dict, binary structure dict, and chunk counts per code.\n",
    "    \"\"\"\n",
    "    print(\"Collecting data from train_loader...\")\n",
    "    latent_depth, latent_height, latent_width = 6, 6, 6 # *** Adjust if different ***\n",
    "    latent_shape = (latent_depth, latent_height, latent_width)\n",
    "    num_structure_codes = fqgan.struct_codebook_size\n",
    "    num_style_codes = fqgan.style_codebook_size\n",
    "\n",
    "     # Initialize count tensors for each code\n",
    "    struct_position_counts = {\n",
    "        code: torch.zeros(latent_shape, dtype=torch.long, device='cpu')\n",
    "        for code in range(num_structure_codes)\n",
    "    }\n",
    "    style_position_counts = {\n",
    "        code: torch.zeros(latent_shape, dtype=torch.long, device='cpu')\n",
    "        for code in range(num_style_codes)\n",
    "    }\n",
    "    total_samples_processed = 0\n",
    "\n",
    "    air_idx = block_converter.get_air_block_index()\n",
    "    structure_dict_original = {} # Store original chunks here\n",
    "    style_dict_original = {}\n",
    "    # style_dict = {} # Keep this if you plan to analyze style later\n",
    "    i = 0\n",
    "    # --- (Loop through loader, encode, decode as before) ---\n",
    "    for batch_idx, batch in enumerate(loader):\n",
    "        # Limit number of batches?\n",
    "        # if batch_idx > 20: break\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            batch = batch.cuda()\n",
    "\n",
    "        for sample_idx in range(len(batch)):\n",
    "            sample = batch[sample_idx].unsqueeze(0)\n",
    "            try:\n",
    "                style_indices, struct_indices = encode_and_quantize(model, sample)\n",
    "                reconstructed, _ = decode_from_indices(style_indices, struct_indices, model, two_stage=True)\n",
    "\n",
    "                struct_indices_cpu = struct_indices.squeeze(0).cpu()\n",
    "                style_indices_cpu = style_indices.squeeze(0).cpu() # If needed for style\n",
    "                reconstructed_cpu = reconstructed.cpu()\n",
    "\n",
    "                for i in range(struct_indices_cpu.shape[0]): # D\n",
    "                    for j in range(struct_indices_cpu.shape[1]): # H\n",
    "                        for k in range(struct_indices_cpu.shape[2]): # W\n",
    "                            # style_code = style_indices_cpu[i, j, k].item() # If needed\n",
    "                            struct_code = struct_indices_cpu[i, j, k].item()\n",
    "                            style_code  = style_indices_cpu[i, j, k].item()\n",
    "                            x_start, y_start, z_start = i * 4, j * 4, k * 4\n",
    "                            x_end, y_end, z_end = x_start + 4, y_start + 4, z_start + 4\n",
    "                            block_chunk = reconstructed_cpu[x_start:x_end, y_start:y_end, z_start:z_end]\n",
    "\n",
    "                            # Store the original chunk tensor\n",
    "                            if struct_code not in structure_dict_original:\n",
    "                                structure_dict_original[struct_code] = []\n",
    "                            if style_code not in style_dict_original:\n",
    "                                style_dict_original[style_code] = []\n",
    "\n",
    "                            structure_dict_original[struct_code].append(block_chunk)\n",
    "                            style_dict_original[style_code].append(block_chunk)\n",
    "                            struct_position_counts[struct_code][i, j, k] += 1\n",
    "                            style_position_counts[style_code][i, j, k] += 1\n",
    "                            # --- (Style dictionary population if needed) ---\n",
    "\n",
    "            except Exception as e:\n",
    "                 print(f\"Error processing sample {sample_idx} in batch {batch_idx}: {e}\")\n",
    "                 continue\n",
    "\n",
    "    print(f\"Finished data collection. Found {len(structure_dict_original)} unique structure codes and {len(style_dict_original)} unique style codes.\")\n",
    "\n",
    "    # Convert structure chunks to binary\n",
    "    print(\"Converting structure chunks to binary...\")\n",
    "    binary_structure_dict = {}\n",
    "    num_chunks_per_code = {}\n",
    "    for struct_code, block_list in structure_dict_original.items(): # Iterate original dict\n",
    "         num_chunks_per_code[struct_code] = len(block_list) # Get count first\n",
    "         if block_list:\n",
    "             try:\n",
    "                 # Convert to binary for metrics\n",
    "                 binary_chunks = [(b.cpu() != air_idx).to(dtype=torch.int) for b in block_list]\n",
    "                 binary_structure_dict[struct_code] = binary_chunks\n",
    "             except Exception as e:\n",
    "                 print(f\"Error converting chunks to binary for code {struct_code}: {e}\")\n",
    "                 # Ensure binary dict entry exists even if conversion failed? Or skip?\n",
    "                 # binary_structure_dict[struct_code] = [] # Or handle differently\n",
    "         # else: # No need for else, count is already 0 from len() above\n",
    "\n",
    "    # Clean up GPU memory if possible\n",
    "    # del style_dict # Keep structure_dict_original\n",
    "    if torch.cuda.is_available():\n",
    "       torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Finished binary conversion. Processed {len(binary_structure_dict)} codes.\")\n",
    "    # Return the original dictionary as well\n",
    "    return structure_dict_original, style_dict_original, binary_structure_dict, num_chunks_per_code, struct_position_counts, style_position_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint_report(step, metrics, num_chunks_per_code, similar_pairs, min_dist, output_dir):\n",
    "    \"\"\"Saves a text report summarizing metrics for a single checkpoint.\"\"\"\n",
    "    report_path = os.path.join(output_dir, f\"checkpoint_{step}_report.txt\")\n",
    "    print(f\"Saving report for step {step} to {report_path}\")\n",
    "\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(f\"--- Analysis Report for Checkpoint Step: {step} ---\\n\\n\")\n",
    "\n",
    "        f.write(\"--- Model-Level Metrics ---\\n\")\n",
    "        f.write(f\"Avg Sharpness (MAD):      {metrics['avg_sharpness_mad']:.4f} (Higher is better)\\n\")\n",
    "        f.write(f\"Avg Sharpness (Entropy):  {metrics['avg_sharpness_entropy']:.4f} (Lower is better)\\n\")\n",
    "        f.write(f\"Avg Consistency (Var):    {metrics['avg_consistency_var']:.4f} (Lower is better)\\n\")\n",
    "        # Consistency Entropy is same as Sharpness Entropy in our current impl.\n",
    "        # f.write(f\"Avg Consistency (Entropy):{metrics['avg_consistency_entropy']:.4f} (Lower is better)\\n\")\n",
    "        f.write(f\"Avg Pairwise Dist ({DISTANCE_METRIC.upper()}): {metrics['avg_pairwise_dist']:.4f} (Higher is better)\\n\")\n",
    "        f.write(f\"Min Pairwise Dist ({DISTANCE_METRIC.upper()}): {min_dist:.6f} (Higher is better)\\n\\n\")\n",
    "\n",
    "\n",
    "        f.write(\"--- Code Uniqueness Analysis ---\\n\")\n",
    "        f.write(f\"Minimum Pairwise Distance ({DISTANCE_METRIC.upper()}): {min_dist:.6f}\\n\")\n",
    "        if similar_pairs:\n",
    "            f.write(\"Most Similar Code Pairs (Potential Redundancy):\\n\")\n",
    "            for pair in similar_pairs:\n",
    "                f.write(f\"  - Codes {pair[0]} and {pair[1]}\\n\")\n",
    "        else:\n",
    "            f.write(\"No highly similar code pairs found.\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "\n",
    "        f.write(\"--- Per-Code Analysis ---\\n\")\n",
    "        codes = sorted(list(metrics['code_sharpness_mad'].keys())) # Use keys from one of the dicts\n",
    "        if not codes:\n",
    "            f.write(\"No codes found with sufficient data for per-code analysis.\\n\")\n",
    "        else:\n",
    "            header = f\"{'Code':<6} {'NumChunks':<10} {'Sharp(MAD)':<12} {'Sharp(Entr)':<12} {'Cons(Var)':<12} {'Flags':<20}\\n\"\n",
    "            f.write(header)\n",
    "            f.write(\"-\" * (len(header) - 1) + \"\\n\") # Adjust separator length\n",
    "\n",
    "            for code in codes:\n",
    "                sharp_mad = metrics['code_sharpness_mad'].get(code, float('nan'))\n",
    "                sharp_entropy = metrics['code_sharpness_entropy'].get(code, float('nan'))\n",
    "                cons_var = metrics['code_consistency_var'].get(code, float('nan'))\n",
    "                n_chunks = num_chunks_per_code.get(code, 0)\n",
    "\n",
    "                flags = []\n",
    "                if n_chunks < MIN_CHUNKS_THRESHOLD:\n",
    "                     flags.append(f\"LOW_USAGE({n_chunks})\")\n",
    "                # Use np.isnan for checking because values are Python floats now\n",
    "                if not np.isnan(sharp_mad) and sharp_mad < SHARP_MAD_THRESHOLD:\n",
    "                    flags.append(\"LOW_SHARPNESS_MAD\")\n",
    "                if not np.isnan(sharp_entropy) and sharp_entropy > SHARP_ENTROPY_THRESHOLD:\n",
    "                    flags.append(\"HIGH_SHARPNESS_ENT\")\n",
    "                if not np.isnan(cons_var) and cons_var > CONS_VAR_THRESHOLD:\n",
    "                    flags.append(\"HIGH_CONSIST_VAR\")\n",
    "\n",
    "                cons_var_str = f\"{cons_var:<12.4f}\" if not np.isnan(cons_var) else f\"{'N/A':<12}\"\n",
    "\n",
    "                f.write(f\"{code:<6} {n_chunks:<10} {sharp_mad:<12.4f} {sharp_entropy:<12.4f} {cons_var_str} {', '.join(flags)}\\n\")\n",
    "            f.write(\"-\" * (len(header) -1 ) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading Checkpoint Step: 5000 ---\n",
      "using padding mode: reflect\n",
      "With cycle consistency: True type: post_quant_conv, using gumbel: True\n",
      "Using EMA quantizer\n",
      "Using SlightlyLessDumbTwoStageGenerator\n",
      "Detaching binary reconstruction from comp graph for final loss\n",
      "NO biome supervision\n",
      "Disentangle Ratio:  0.5\n",
      "Loading fqgan_5000.th\n",
      "Loaded model for step 5000\n",
      "Collecting data from train_loader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TimBits\\miniconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished data collection. Found 32 unique structure codes and 32 unique style codes.\n",
      "Converting structure chunks to binary...\n",
      "Finished binary conversion. Processed 32 codes.\n",
      "Calculating metrics for step 5000...\n",
      "Generating visualizations for step 5000...\n",
      "Found 32 Struct codes to visualize\n",
      "Visualizing Struct code 0...\n",
      "Saved visualization for structure code 0 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_0.png\n",
      "Visualizing Struct code 1...\n",
      "Saved visualization for structure code 1 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_1.png\n",
      "Visualizing Struct code 2...\n",
      "Saved visualization for structure code 2 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_2.png\n",
      "Visualizing Struct code 3...\n",
      "Saved visualization for structure code 3 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_3.png\n",
      "Visualizing Struct code 4...\n",
      "Saved visualization for structure code 4 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_4.png\n",
      "Visualizing Struct code 5...\n",
      "Saved visualization for structure code 5 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_5.png\n",
      "Visualizing Struct code 6...\n",
      "Saved visualization for structure code 6 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_6.png\n",
      "Visualizing Struct code 7...\n",
      "Saved visualization for structure code 7 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_7.png\n",
      "Visualizing Struct code 8...\n",
      "Saved visualization for structure code 8 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_8.png\n",
      "Visualizing Struct code 9...\n",
      "Saved visualization for structure code 9 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_9.png\n",
      "Visualizing Struct code 10...\n",
      "Saved visualization for structure code 10 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_10.png\n",
      "Visualizing Struct code 11...\n",
      "Saved visualization for structure code 11 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_11.png\n",
      "Visualizing Struct code 12...\n",
      "Saved visualization for structure code 12 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_12.png\n",
      "Visualizing Struct code 13...\n",
      "Saved visualization for structure code 13 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_13.png\n",
      "Visualizing Struct code 14...\n",
      "Saved visualization for structure code 14 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_14.png\n",
      "Visualizing Struct code 15...\n",
      "Saved visualization for structure code 15 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_15.png\n",
      "Visualizing Struct code 16...\n",
      "Saved visualization for structure code 16 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_16.png\n",
      "Visualizing Struct code 17...\n",
      "Saved visualization for structure code 17 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_17.png\n",
      "Visualizing Struct code 18...\n",
      "Saved visualization for structure code 18 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_18.png\n",
      "Visualizing Struct code 19...\n",
      "Saved visualization for structure code 19 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_19.png\n",
      "Visualizing Struct code 20...\n",
      "Saved visualization for structure code 20 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_20.png\n",
      "Visualizing Struct code 21...\n",
      "Saved visualization for structure code 21 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_21.png\n",
      "Visualizing Struct code 22...\n",
      "Saved visualization for structure code 22 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_22.png\n",
      "Visualizing Struct code 23...\n",
      "Saved visualization for structure code 23 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_23.png\n",
      "Visualizing Struct code 24...\n",
      "Saved visualization for structure code 24 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_24.png\n",
      "Visualizing Struct code 25...\n",
      "Saved visualization for structure code 25 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_25.png\n",
      "Visualizing Struct code 26...\n",
      "Saved visualization for structure code 26 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_26.png\n",
      "Visualizing Struct code 27...\n",
      "Saved visualization for structure code 27 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_27.png\n",
      "Visualizing Struct code 28...\n",
      "Saved visualization for structure code 28 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_28.png\n",
      "Visualizing Struct code 29...\n",
      "Saved visualization for structure code 29 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_29.png\n",
      "Visualizing Struct code 30...\n",
      "Saved visualization for structure code 30 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_30.png\n",
      "Visualizing Struct code 31...\n",
      "Saved visualization for structure code 31 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\\Struct_code_31.png\n",
      "Visualization complete. Images saved to results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations/\n",
      "Found 32 Style codes to visualize\n",
      "Visualizing Style code 0...\n",
      "Saved visualization for structure code 0 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_0.png\n",
      "Visualizing Style code 1...\n",
      "Saved visualization for structure code 1 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_1.png\n",
      "Visualizing Style code 2...\n",
      "Saved visualization for structure code 2 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_2.png\n",
      "Visualizing Style code 3...\n",
      "Saved visualization for structure code 3 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_3.png\n",
      "Visualizing Style code 4...\n",
      "Saved visualization for structure code 4 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_4.png\n",
      "Visualizing Style code 5...\n",
      "Saved visualization for structure code 5 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_5.png\n",
      "Visualizing Style code 6...\n",
      "Saved visualization for structure code 6 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_6.png\n",
      "Visualizing Style code 7...\n",
      "Saved visualization for structure code 7 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_7.png\n",
      "Visualizing Style code 8...\n",
      "Saved visualization for structure code 8 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_8.png\n",
      "Visualizing Style code 9...\n",
      "Saved visualization for structure code 9 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_9.png\n",
      "Visualizing Style code 10...\n",
      "Saved visualization for structure code 10 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_10.png\n",
      "Visualizing Style code 11...\n",
      "Saved visualization for structure code 11 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_11.png\n",
      "Visualizing Style code 12...\n",
      "Saved visualization for structure code 12 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_12.png\n",
      "Visualizing Style code 13...\n",
      "Saved visualization for structure code 13 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_13.png\n",
      "Visualizing Style code 14...\n",
      "Saved visualization for structure code 14 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_14.png\n",
      "Visualizing Style code 15...\n",
      "Saved visualization for structure code 15 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_15.png\n",
      "Visualizing Style code 16...\n",
      "Saved visualization for structure code 16 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_16.png\n",
      "Visualizing Style code 17...\n",
      "Saved visualization for structure code 17 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_17.png\n",
      "Visualizing Style code 18...\n",
      "Saved visualization for structure code 18 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_18.png\n",
      "Visualizing Style code 19...\n",
      "Saved visualization for structure code 19 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_19.png\n",
      "Visualizing Style code 20...\n",
      "Saved visualization for structure code 20 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_20.png\n",
      "Visualizing Style code 21...\n",
      "Saved visualization for structure code 21 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_21.png\n",
      "Visualizing Style code 22...\n",
      "Saved visualization for structure code 22 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_22.png\n",
      "Visualizing Style code 23...\n",
      "Saved visualization for structure code 23 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_23.png\n",
      "Visualizing Style code 24...\n",
      "Saved visualization for structure code 24 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_24.png\n",
      "Visualizing Style code 25...\n",
      "Saved visualization for structure code 25 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_25.png\n",
      "Visualizing Style code 26...\n",
      "Saved visualization for structure code 26 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_26.png\n",
      "Visualizing Style code 27...\n",
      "Saved visualization for structure code 27 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_27.png\n",
      "Visualizing Style code 28...\n",
      "Saved visualization for structure code 28 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_28.png\n",
      "Visualizing Style code 29...\n",
      "Saved visualization for structure code 29 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_29.png\n",
      "Visualizing Style code 30...\n",
      "Saved visualization for structure code 30 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_30.png\n",
      "Visualizing Style code 31...\n",
      "Saved visualization for structure code 31 to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\\Style_code_31.png\n",
      "Visualization complete. Images saved to results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations/\n",
      "Generating positional frequency plots for 32 codes...\n",
      "Generated plot 10/32 (Code 9)\n",
      "Generated plot 20/32 (Code 19)\n",
      "Generated plot 30/32 (Code 29)\n",
      "Generated plot 32/32 (Code 31)\n",
      "Finished generating positional frequency plots. Saved to: results/interpretability_analysis17\\checkpoint_5000_visualizations/Struct_visualizations\n",
      "Generating positional frequency plots for 32 codes...\n",
      "Generated plot 10/32 (Code 9)\n",
      "Generated plot 20/32 (Code 19)\n",
      "Generated plot 30/32 (Code 29)\n",
      "Generated plot 32/32 (Code 31)\n",
      "Finished generating positional frequency plots. Saved to: results/interpretability_analysis17\\checkpoint_5000_visualizations/Style_visualizations\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "Metrics calculated.\n",
      "Saving report for step 5000 to results/interpretability_analysis17\\checkpoint_5000_report.txt\n",
      "\n",
      "--- Loading Checkpoint Step: 10000 ---\n",
      "using padding mode: reflect\n",
      "With cycle consistency: True type: post_quant_conv, using gumbel: True\n",
      "Using EMA quantizer\n",
      "Using SlightlyLessDumbTwoStageGenerator\n",
      "Detaching binary reconstruction from comp graph for final loss\n",
      "NO biome supervision\n",
      "Disentangle Ratio:  0.5\n",
      "Loading fqgan_10000.th\n",
      "Loaded model for step 10000\n",
      "Collecting data from train_loader...\n",
      "Finished data collection. Found 32 unique structure codes and 32 unique style codes.\n",
      "Converting structure chunks to binary...\n",
      "Finished binary conversion. Processed 32 codes.\n",
      "Calculating metrics for step 10000...\n",
      "Generating visualizations for step 10000...\n",
      "Found 32 Struct codes to visualize\n",
      "Visualizing Struct code 0...\n",
      "Saved visualization for structure code 0 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_0.png\n",
      "Visualizing Struct code 1...\n",
      "Saved visualization for structure code 1 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_1.png\n",
      "Visualizing Struct code 2...\n",
      "Saved visualization for structure code 2 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_2.png\n",
      "Visualizing Struct code 3...\n",
      "Saved visualization for structure code 3 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_3.png\n",
      "Visualizing Struct code 4...\n",
      "Saved visualization for structure code 4 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_4.png\n",
      "Visualizing Struct code 5...\n",
      "Saved visualization for structure code 5 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_5.png\n",
      "Visualizing Struct code 6...\n",
      "Saved visualization for structure code 6 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_6.png\n",
      "Visualizing Struct code 7...\n",
      "Saved visualization for structure code 7 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_7.png\n",
      "Visualizing Struct code 8...\n",
      "Saved visualization for structure code 8 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_8.png\n",
      "Visualizing Struct code 9...\n",
      "Saved visualization for structure code 9 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_9.png\n",
      "Visualizing Struct code 10...\n",
      "Saved visualization for structure code 10 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_10.png\n",
      "Visualizing Struct code 11...\n",
      "Saved visualization for structure code 11 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_11.png\n",
      "Visualizing Struct code 12...\n",
      "Saved visualization for structure code 12 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_12.png\n",
      "Visualizing Struct code 13...\n",
      "Saved visualization for structure code 13 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_13.png\n",
      "Visualizing Struct code 14...\n",
      "Saved visualization for structure code 14 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_14.png\n",
      "Visualizing Struct code 15...\n",
      "Saved visualization for structure code 15 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_15.png\n",
      "Visualizing Struct code 16...\n",
      "Saved visualization for structure code 16 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_16.png\n",
      "Visualizing Struct code 17...\n",
      "Saved visualization for structure code 17 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_17.png\n",
      "Visualizing Struct code 18...\n",
      "Saved visualization for structure code 18 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_18.png\n",
      "Visualizing Struct code 19...\n",
      "Saved visualization for structure code 19 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_19.png\n",
      "Visualizing Struct code 20...\n",
      "Saved visualization for structure code 20 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_20.png\n",
      "Visualizing Struct code 21...\n",
      "Saved visualization for structure code 21 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_21.png\n",
      "Visualizing Struct code 22...\n",
      "Saved visualization for structure code 22 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_22.png\n",
      "Visualizing Struct code 23...\n",
      "Saved visualization for structure code 23 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_23.png\n",
      "Visualizing Struct code 24...\n",
      "Saved visualization for structure code 24 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_24.png\n",
      "Visualizing Struct code 25...\n",
      "Saved visualization for structure code 25 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_25.png\n",
      "Visualizing Struct code 26...\n",
      "Saved visualization for structure code 26 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_26.png\n",
      "Visualizing Struct code 27...\n",
      "Saved visualization for structure code 27 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_27.png\n",
      "Visualizing Struct code 28...\n",
      "Saved visualization for structure code 28 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_28.png\n",
      "Visualizing Struct code 29...\n",
      "Saved visualization for structure code 29 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_29.png\n",
      "Visualizing Struct code 30...\n",
      "Saved visualization for structure code 30 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_30.png\n",
      "Visualizing Struct code 31...\n",
      "Saved visualization for structure code 31 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\\Struct_code_31.png\n",
      "Visualization complete. Images saved to results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations/\n",
      "Found 32 Style codes to visualize\n",
      "Visualizing Style code 0...\n",
      "Saved visualization for structure code 0 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_0.png\n",
      "Visualizing Style code 1...\n",
      "Saved visualization for structure code 1 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_1.png\n",
      "Visualizing Style code 2...\n",
      "Saved visualization for structure code 2 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_2.png\n",
      "Visualizing Style code 3...\n",
      "Saved visualization for structure code 3 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_3.png\n",
      "Visualizing Style code 4...\n",
      "Saved visualization for structure code 4 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_4.png\n",
      "Visualizing Style code 5...\n",
      "Saved visualization for structure code 5 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_5.png\n",
      "Visualizing Style code 6...\n",
      "Saved visualization for structure code 6 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_6.png\n",
      "Visualizing Style code 7...\n",
      "Saved visualization for structure code 7 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_7.png\n",
      "Visualizing Style code 8...\n",
      "Saved visualization for structure code 8 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_8.png\n",
      "Visualizing Style code 9...\n",
      "Saved visualization for structure code 9 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_9.png\n",
      "Visualizing Style code 10...\n",
      "Saved visualization for structure code 10 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_10.png\n",
      "Visualizing Style code 11...\n",
      "Saved visualization for structure code 11 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_11.png\n",
      "Visualizing Style code 12...\n",
      "Saved visualization for structure code 12 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_12.png\n",
      "Visualizing Style code 13...\n",
      "Saved visualization for structure code 13 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_13.png\n",
      "Visualizing Style code 14...\n",
      "Saved visualization for structure code 14 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_14.png\n",
      "Visualizing Style code 15...\n",
      "Saved visualization for structure code 15 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_15.png\n",
      "Visualizing Style code 16...\n",
      "Saved visualization for structure code 16 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_16.png\n",
      "Visualizing Style code 17...\n",
      "Saved visualization for structure code 17 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_17.png\n",
      "Visualizing Style code 18...\n",
      "Saved visualization for structure code 18 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_18.png\n",
      "Visualizing Style code 19...\n",
      "Saved visualization for structure code 19 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_19.png\n",
      "Visualizing Style code 20...\n",
      "Saved visualization for structure code 20 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_20.png\n",
      "Visualizing Style code 21...\n",
      "Saved visualization for structure code 21 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_21.png\n",
      "Visualizing Style code 22...\n",
      "Saved visualization for structure code 22 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_22.png\n",
      "Visualizing Style code 23...\n",
      "Saved visualization for structure code 23 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_23.png\n",
      "Visualizing Style code 24...\n",
      "Saved visualization for structure code 24 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_24.png\n",
      "Visualizing Style code 25...\n",
      "Saved visualization for structure code 25 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_25.png\n",
      "Visualizing Style code 26...\n",
      "Saved visualization for structure code 26 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_26.png\n",
      "Visualizing Style code 27...\n",
      "Saved visualization for structure code 27 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_27.png\n",
      "Visualizing Style code 28...\n",
      "Saved visualization for structure code 28 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_28.png\n",
      "Visualizing Style code 29...\n",
      "Saved visualization for structure code 29 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_29.png\n",
      "Visualizing Style code 30...\n",
      "Saved visualization for structure code 30 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_30.png\n",
      "Visualizing Style code 31...\n",
      "Saved visualization for structure code 31 to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\\Style_code_31.png\n",
      "Visualization complete. Images saved to results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations/\n",
      "Generating positional frequency plots for 32 codes...\n",
      "Generated plot 10/32 (Code 9)\n",
      "Generated plot 20/32 (Code 19)\n",
      "Generated plot 30/32 (Code 29)\n",
      "Generated plot 32/32 (Code 31)\n",
      "Finished generating positional frequency plots. Saved to: results/interpretability_analysis17\\checkpoint_10000_visualizations/Struct_visualizations\n",
      "Generating positional frequency plots for 32 codes...\n",
      "Generated plot 10/32 (Code 9)\n",
      "Generated plot 20/32 (Code 19)\n",
      "Generated plot 30/32 (Code 29)\n",
      "Generated plot 32/32 (Code 31)\n",
      "Finished generating positional frequency plots. Saved to: results/interpretability_analysis17\\checkpoint_10000_visualizations/Style_visualizations\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "Metrics calculated.\n",
      "Saving report for step 10000 to results/interpretability_analysis17\\checkpoint_10000_report.txt\n",
      "\n",
      "--- Loading Checkpoint Step: 15000 ---\n",
      "using padding mode: reflect\n",
      "With cycle consistency: True type: post_quant_conv, using gumbel: True\n",
      "Using EMA quantizer\n",
      "Using SlightlyLessDumbTwoStageGenerator\n",
      "Detaching binary reconstruction from comp graph for final loss\n",
      "NO biome supervision\n",
      "Disentangle Ratio:  0.5\n",
      "Loading fqgan_15000.th\n",
      "Error loading model for step 15000: [Errno 2] No such file or directory: '../model_logs/FQGAN_2stagedecoder_logweighted3_32codes_cycleconsistency_STE_postquant_detachcycle_gumbel/saved_models\\\\fqgan_15000.th'\n",
      "Skipping step 15000 due to loading error.\n",
      "\n",
      "--- Loading Checkpoint Step: 20000 ---\n",
      "using padding mode: reflect\n",
      "With cycle consistency: True type: post_quant_conv, using gumbel: True\n",
      "Using EMA quantizer\n",
      "Using SlightlyLessDumbTwoStageGenerator\n",
      "Detaching binary reconstruction from comp graph for final loss\n",
      "NO biome supervision\n",
      "Disentangle Ratio:  0.5\n",
      "Loading fqgan_20000.th\n",
      "Error loading model for step 20000: [Errno 2] No such file or directory: '../model_logs/FQGAN_2stagedecoder_logweighted3_32codes_cycleconsistency_STE_postquant_detachcycle_gumbel/saved_models\\\\fqgan_20000.th'\n",
      "Skipping step 20000 due to loading error.\n",
      "\n",
      "--- Loading Checkpoint Step: 24999 ---\n",
      "using padding mode: reflect\n",
      "With cycle consistency: True type: post_quant_conv, using gumbel: True\n",
      "Using EMA quantizer\n",
      "Using SlightlyLessDumbTwoStageGenerator\n",
      "Detaching binary reconstruction from comp graph for final loss\n",
      "NO biome supervision\n",
      "Disentangle Ratio:  0.5\n",
      "Loading fqgan_24999.th\n",
      "Error loading model for step 24999: [Errno 2] No such file or directory: '../model_logs/FQGAN_2stagedecoder_logweighted3_32codes_cycleconsistency_STE_postquant_detachcycle_gumbel/saved_models\\\\fqgan_24999.th'\n",
      "Skipping step 24999 due to loading error.\n",
      "\n",
      "--- Finished evaluating all checkpoints ---\n",
      "Generating overall summary plots...\n",
      "Overall summary plot saved to results/interpretability_analysis17\\overall_metrics_evolution.png\n",
      "Analysis complete.\n"
     ]
    }
   ],
   "source": [
    "model_name_file = os.path.join(OUTPUT_DIR, \"model_name.txt\")\n",
    "with open(model_name_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(MODEL_BASE_PATH)\n",
    "for step in CHECKPOINT_STEPS:\n",
    "    # 1. Load Model\n",
    "    fqgan = load_model_for_step(step, MODEL_BASE_PATH)\n",
    "    fqgan.to('cuda')\n",
    "    if fqgan is None:\n",
    "        print(f\"Skipping step {step} due to loading error.\")\n",
    "        continue\n",
    "\n",
    "    # 2. Collect Data\n",
    "    # Ensure train_loader is correctly defined/passed\n",
    "    structure_dict_original, style_dict_original, binary_structure_dict, num_chunks_per_code, struct_position_counts, style_position_counts = collect_data_for_model(fqgan, train_loader, block_converter)\n",
    "\n",
    "    if not binary_structure_dict: # Check binary dict as it's needed for metrics\n",
    "         print(f\"Skipping metrics/plotting for step {step} as no data was collected/converted.\")\n",
    "         del fqgan, structure_dict_original # Clean up original dict too\n",
    "         if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "         continue\n",
    "\n",
    "    # 3. Calculate Metrics\n",
    "    print(f\"Calculating metrics for step {step}...\")\n",
    "    metrics = {}\n",
    "    avg_freq_maps = calculate_average_frequency_maps(binary_structure_dict)\n",
    "    block_freq_maps = calculate_average_block_frequency_maps(style_dict_original, fqgan.in_channels)\n",
    "    # pos_freq_maps = calculate_positional_frequencies(fqgan)\n",
    "\n",
    "    if not avg_freq_maps:\n",
    "         print(f\"Skipping metrics for step {step} as no average frequency maps could be calculated (no valid codes?).\")\n",
    "         del fqgan, binary_structure_dict, num_chunks_per_code\n",
    "         if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "         continue\n",
    "    \n",
    "    \n",
    "    # --- Visualization Section ---\n",
    "    print(f\"Generating visualizations for step {step}...\")\n",
    "    struct_vis_dir = os.path.join(OUTPUT_DIR, f\"checkpoint_{step}_visualizations/Struct_visualizations\") # Combined dir\n",
    "    style_vis_dir = os.path.join(OUTPUT_DIR, f\"checkpoint_{step}_visualizations/Style_visualizations\")\n",
    "    os.makedirs(struct_vis_dir, exist_ok=True)\n",
    "    os.makedirs(style_vis_dir, exist_ok=True)\n",
    "    visualize_all_structure_codes(structure_dict_original, struct_vis_dir)\n",
    "    visualize_all_structure_codes(style_dict_original, style_vis_dir, code_type=\"Style\")\n",
    "    generate_all_positional_frequency_plots(struct_position_counts, struct_vis_dir)\n",
    "    generate_all_positional_frequency_plots(style_position_counts, style_vis_dir)\n",
    "    for code, freq_map_tensor in avg_freq_maps.items():\n",
    "        # --- Heatmap Generation ---\n",
    "        plot_frequency_heatmap(freq_map_tensor, code, struct_vis_dir) # Save to combined dir\n",
    "            \n",
    "    for code, block_freq_map_tensor in block_freq_maps.items():\n",
    "        # --- Heatmap Generation ---\n",
    "        plot_block_frequency_heatmap(code, block_freq_map_tensor, style_vis_dir, block_converter, block_index_to_name) # Save to combined dir\n",
    "        # --- Grid Visualization ---\n",
    "        # if code in structure_dict_original and structure_dict_original[code]: # Check if code exists and has chunks\n",
    "        #     grid_save_path = os.path.join(vis_dir, f\"grid_vis_code_{code}.png\")\n",
    "        #     try:\n",
    "        #         # Call the grid visualization function\n",
    "        #         # Make sure block_converter is accessible here\n",
    "        #         visualize_structure_grid(\n",
    "        #             structure_dict=structure_dict_original,\n",
    "        #             struct_code=code,\n",
    "        #             save_path=grid_save_path,\n",
    "        #             # block_converter=block_converter, # Pass if not global\n",
    "        #             # You might need to adjust num_chunks, grid_size if defaults aren't desired\n",
    "        #             num_chunks=16, # Example: visualize fewer chunks?\n",
    "        #             grid_size=4  # Example: smaller grid?\n",
    "        #         )\n",
    "        #     except ImportError as e:\n",
    "        #          print(f\"PyVista import error, skipping grid visualization: {e}\")\n",
    "        #          # Optional: break the inner loop if PyVista isn't installed\n",
    "        #          # break\n",
    "        #     except Exception as e:\n",
    "        #          print(f\"Error generating grid visualization for code {code} at step {step}: {e}\")\n",
    "        #          # Continue to next code\n",
    "        #          continue\n",
    "        # else:\n",
    "        #     print(f\"Skipping grid visualization for code {code}: No original chunks found.\")\n",
    "\n",
    "\n",
    "    # print(f\"Visualizations saved to: {vis_dir}\")\n",
    "    # --- End Visualization Section ---\n",
    "    \n",
    "    metrics['code_sharpness_mad'], metrics['avg_sharpness_mad'] = calculate_sharpness_mad(avg_freq_maps)\n",
    "    metrics['code_sharpness_entropy'], metrics['avg_sharpness_entropy'] = calculate_sharpness_entropy(avg_freq_maps)\n",
    "    metrics['code_consistency_var'], metrics['avg_consistency_var'] = calculate_consistency_variance(binary_structure_dict)\n",
    "    # Note: Cons. Entropy = Sharp. Entropy\n",
    "    # metrics['code_consistency_entropy'], metrics['avg_consistency_entropy'] = calculate_consistency_entropy(binary_structure_dict)\n",
    "    metrics['avg_pairwise_dist'], metrics['min_pairwise_dist'] = calculate_uniqueness_metrics(avg_freq_maps, distance_metric=DISTANCE_METRIC)\n",
    "\n",
    "    # Find most similar pairs (for reporting) - use the calculated min distance\n",
    "    min_dist_actual, similar_pairs = find_most_similar_codes(avg_freq_maps, distance_metric=DISTANCE_METRIC)\n",
    "    # Sanity check: min_dist_actual should be very close to metrics['min_pairwise_dist']\n",
    "    if not np.isclose(min_dist_actual, metrics['min_pairwise_dist']):\n",
    "         print(f\"Warning: Mismatch in minimum distance calculation for step {step}: {min_dist_actual} vs {metrics['min_pairwise_dist']}\")\n",
    "         # Use the one from find_most_similar_codes for consistency in the report\n",
    "         min_dist_report = min_dist_actual\n",
    "    else:\n",
    "         min_dist_report = metrics['min_pairwise_dist'] # Use the one stored in metrics\n",
    "\n",
    "\n",
    "    # Add num_chunks for reference if needed later, but it's mainly for the report\n",
    "    metrics['num_chunks_per_code'] = num_chunks_per_code\n",
    "    metrics['most_similar_pairs'] = similar_pairs\n",
    "    # Add avg_freq_maps if you want to visualize them later\n",
    "    # metrics['avg_freq_maps'] = avg_freq_maps # Can consume a lot of memory!\n",
    "\n",
    "    all_metrics_data[step] = metrics\n",
    "    print(\"Metrics calculated.\")\n",
    "\n",
    "    # 4. Save Checkpoint Report\n",
    "    save_checkpoint_report(step, metrics, num_chunks_per_code, similar_pairs, min_dist_report, OUTPUT_DIR)\n",
    "\n",
    "    # 5. Clean up GPU memory before next loop iteration\n",
    "    del fqgan, binary_structure_dict, num_chunks_per_code, avg_freq_maps, metrics\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n--- Finished evaluating all checkpoints ---\")\n",
    "\n",
    "# --- Generate Overall Summary Plots ---\n",
    "if all_metrics_data:\n",
    "    print(\"Generating overall summary plots...\")\n",
    "    steps = sorted(all_metrics_data.keys())\n",
    "    num_plots = 5 # Adjust if you add/remove metrics\n",
    "    fig, axes = plt.subplots(num_plots, 1, figsize=(10, 5 * num_plots), sharex=True)\n",
    "    fig.suptitle('Model Interpretability Metrics vs. Checkpoint Step', fontsize=16)\n",
    "\n",
    "    plot_configs = [\n",
    "        {'key': 'avg_sharpness_mad', 'label': 'Avg Sharpness (MAD)', 'goal': 'max'},\n",
    "        {'key': 'avg_sharpness_entropy', 'label': 'Avg Sharpness (Entropy)', 'goal': 'min'},\n",
    "        {'key': 'avg_consistency_var', 'label': 'Avg Consistency (Var)', 'goal': 'min'},\n",
    "        {'key': 'avg_pairwise_dist', 'label': f'Avg Pairwise Dist ({DISTANCE_METRIC.upper()})', 'goal': 'max'},\n",
    "        {'key': 'min_pairwise_dist', 'label': f'Min Pairwise Dist ({DISTANCE_METRIC.upper()})', 'goal': 'max'}\n",
    "    ]\n",
    "\n",
    "    for i, config in enumerate(plot_configs):\n",
    "        metric_key = config['key']\n",
    "        metric_label = config['label']\n",
    "        goal = config['goal']\n",
    "\n",
    "        values = [all_metrics_data[step][metric_key] for step in steps]\n",
    "        axes[i].plot(steps, values, marker='o', linestyle='-')\n",
    "        axes[i].set_ylabel(metric_label)\n",
    "        axes[i].grid(True)\n",
    "\n",
    "        # Find and mark the best value\n",
    "        if values:\n",
    "            if goal == 'max':\n",
    "                best_val = np.max(values)\n",
    "                best_idx = np.argmax(values)\n",
    "            else: # goal == 'min'\n",
    "                best_val = np.min(values)\n",
    "                best_idx = np.argmin(values)\n",
    "            best_step = steps[best_idx]\n",
    "            axes[i].axhline(best_val, color='r', linestyle='--', label=f'Best: {best_val:.4f} at step {best_step}')\n",
    "            axes[i].plot(best_step, best_val, 'r*', markersize=10) # Mark best point\n",
    "            axes[i].legend()\n",
    "            axes[i].set_title(f\"{metric_label} (Best is {'Higher' if goal == 'max' else 'Lower'})\")\n",
    "        else:\n",
    "             axes[i].set_title(metric_label)\n",
    "\n",
    "\n",
    "    axes[-1].set_xlabel(\"Checkpoint Step\")\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.97]) # Adjust layout to prevent title overlap\n",
    "    plot_path = os.path.join(OUTPUT_DIR, \"overall_metrics_evolution.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    print(f\"Overall summary plot saved to {plot_path}\")\n",
    "    plt.close(fig) # Close the figure to free memory\n",
    "else:\n",
    "    print(\"No metrics data collected, skipping overall plots.\")\n",
    "\n",
    "print(\"Analysis complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.7483e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        3.2005e-07, 2.6068e-03, 0.0000e+00, 0.0000e+00, 1.2660e-01, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0216e-01, 2.6398e-02,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2571e-03, 9.5694e-05, 1.4940e-03,\n",
       "        5.6123e-03, 0.0000e+00, 0.0000e+00, 3.2005e-07, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 1.3116e-03, 2.2051e-04, 2.3875e-04, 7.4630e-02,\n",
       "        0.0000e+00, 2.2051e-03, 6.4009e-07, 7.8341e-02, 0.0000e+00, 0.0000e+00])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_freq_maps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'results/interpretability_analysis17\\\\checkpoint_10000_visualizations/Style_visualizations\\\\block_heatmap_code_0.png'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Comm:No such comm: 1e77a464ed574cc3b19baeb2181976b6\n",
      "WARNING:Comm:No such comm: 1e77a464ed574cc3b19baeb2181976b6\n",
      "WARNING:Comm:No such comm: 1e77a464ed574cc3b19baeb2181976b6\n",
      "WARNING:Comm:No such comm: 1e77a464ed574cc3b19baeb2181976b6\n"
     ]
    }
   ],
   "source": [
    "plot_block_frequency_heatmap(0, block_freq_maps[0], style_vis_dir, block_converter, block_index_to_name) # Save to combined dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already numpy\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplot_block_frequency_heatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock_freq_map_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstruct_vis_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_converter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_index_to_name\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Save to combined dir\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[28], line 31\u001b[0m, in \u001b[0;36mplot_block_frequency_heatmap\u001b[1;34m(style_code, block_freqs, save_dir, block_converter, block_index_to_name, cmap)\u001b[0m\n\u001b[0;32m     27\u001b[0m     freqs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(block_freqs)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# print(freqs)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# print(freqs.shape)\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m C \u001b[38;5;241m=\u001b[39m \u001b[43mfreqs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     32\u001b[0m heatmap \u001b[38;5;241m=\u001b[39m freqs\u001b[38;5;241m.\u001b[39mreshape(C, \u001b[38;5;241m1\u001b[39m)   \u001b[38;5;66;03m# shape (1, C)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Map each channel idx  Minecraft blockID  block name\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "plot_block_frequency_heatmap(block_freq_map_tensor, code, struct_vis_dir, block_converter, block_index_to_name) # Save to combined dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biome Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ...\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']]\n",
      "\n",
      " [['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ...\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']]\n",
      "\n",
      " [['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ...\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'plains' 'plains' 'plains']]\n",
      "\n",
      " ...\n",
      "\n",
      " [['forest' 'forest' 'forest' ... 'forest' 'forest' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'plains']\n",
      "  ...\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'plains']\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'plains']]\n",
      "\n",
      " [['forest' 'forest' 'forest' ... 'forest' 'forest' 'forest']\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'forest']\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'forest']\n",
      "  ...\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'forest']\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'forest']\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'forest']]\n",
      "\n",
      " [['forest' 'forest' 'forest' ... 'forest' 'forest' 'forest']\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'forest']\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'forest']\n",
      "  ...\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'forest']\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'forest']\n",
      "  ['forest' 'forest' 'forest' ... 'forest' 'forest' 'forest']]]\n"
     ]
    }
   ],
   "source": [
    "# check for biome data\n",
    "for batch in train_loader:\n",
    "    voxels, biomes = batch\n",
    "    biome_value = block_converter.convert_to_original_biomes(biomes)\n",
    "    print(biome_value[2])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store style code and its corresponding biome chunks and block chunks \n",
    "biome_dict = {}\n",
    "\n",
    "for batch in train_loader:\n",
    "    voxels, biomes = batch  # Now unpack both\n",
    "    for sample_idx in range(len(voxels)):\n",
    "        sample = voxels[sample_idx].unsqueeze(0).cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            style_indices, struct_indices = encode_and_quantize(fqgan, sample)\n",
    "            reconstructed, binary_reconstructed = decode_from_indices(\n",
    "                style_indices, struct_indices, fqgan, two_stage=True\n",
    "            )\n",
    "\n",
    "        biome_tensor = biomes[sample_idx]\n",
    "        biome_names = block_converter.convert_to_original_biomes(biome_tensor)\n",
    "\n",
    "        for i in range(style_indices.shape[1]):  \n",
    "            for j in range(style_indices.shape[2]):  \n",
    "                for k in range(style_indices.shape[3]):  \n",
    "                    style_code = style_indices[0, i, j, k].item()\n",
    "\n",
    "                    x_start, y_start, z_start = i * 4, j * 4, k * 4\n",
    "                    x_end, y_end, z_end = x_start + 4, y_start + 4, z_start + 4\n",
    "\n",
    "                    block = reconstructed[x_start:x_end, y_start:y_end, z_start:z_end]\n",
    "                    biome_chunk = biome_names[x_start:x_end, y_start:y_end, z_start:z_end]\n",
    "\n",
    "                    # Store both block and corresponding biome values\n",
    "                    if style_code not in biome_dict:\n",
    "                        biome_dict[style_code] = []\n",
    "\n",
    "                    biome_dict[style_code].append({\n",
    "                        \"block\": block,\n",
    "                        \"biome\": biome_chunk\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['swampland' 'swampland' 'swampland' 'swampland']\n",
      "  ['swampland' 'swampland' 'swampland' 'swampland']\n",
      "  ['swampland' 'swampland' 'swampland' 'swampland']\n",
      "  ['swampland' 'swampland' 'swampland' 'swampland']]\n",
      "\n",
      " [['swampland' 'swampland' 'swampland' 'swampland']\n",
      "  ['swampland' 'swampland' 'swampland' 'swampland']\n",
      "  ['swampland' 'swampland' 'swampland' 'swampland']\n",
      "  ['swampland' 'swampland' 'swampland' 'swampland']]\n",
      "\n",
      " [['swampland' 'swampland' 'swampland' 'swampland']\n",
      "  ['swampland' 'swampland' 'swampland' 'swampland']\n",
      "  ['swampland' 'swampland' 'swampland' 'swampland']\n",
      "  ['swampland' 'swampland' 'swampland' 'swampland']]\n",
      "\n",
      " [['swampland' 'swampland' 'swampland' 'swampland']\n",
      "  ['swampland' 'swampland' 'swampland' 'swampland']\n",
      "  ['swampland' 'swampland' 'swampland' 'swampland']\n",
      "  ['swampland' 'swampland' 'swampland' 'swampland']]]\n"
     ]
    }
   ],
   "source": [
    "# example for the first sample in index 0.\n",
    "print(biome_dict[0][0][\"biome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{np.str_('beaches'): 0.012664664858581944, np.str_('birch_forest'): 0.04071086977991507, np.str_('cave'): 0.3224620340193419, np.str_('desert'): 0.042630535360211504, np.str_('extreme_hills'): 0.15213599000235506, np.str_('forest'): 0.14629722481786206, np.str_('mutated_extreme_hills'): 0.00047291291565056863, np.str_('ocean'): 0.02534025039694297, np.str_('plains'): 0.12334860329253747, np.str_('river'): 0.02441863998602153, np.str_('savanna'): 0.029020994294657033, np.str_('swampland'): 0.04432703922328327, np.str_('taiga'): 0.03617024105263958}\n"
     ]
    }
   ],
   "source": [
    "# find the frequency of biome for each code index\n",
    "from collections import defaultdict\n",
    "\n",
    "biome_frequencies = defaultdict(lambda: defaultdict(int))\n",
    "biome_total_counts = defaultdict(int)\n",
    "\n",
    "\n",
    "for style_code, entries in biome_dict.items():\n",
    "    for entry in entries:\n",
    "        biome_matrix = entry[\"biome\"]  \n",
    "        flattened_biomes = biome_matrix.flatten()\n",
    "\n",
    "        # Update total biome count for the style code\n",
    "        biome_total_counts[style_code] += len(flattened_biomes)\n",
    "\n",
    "        # Count occurrences of each biome type\n",
    "        for biome in flattened_biomes:\n",
    "            biome_frequencies[style_code][biome] += 1\n",
    "\n",
    "sorted_biome_frequency_dict = {}\n",
    "for style_code in sorted(biome_frequencies.keys()):\n",
    "    biome_counts = biome_frequencies[style_code]\n",
    "    total_biomes = biome_total_counts[style_code] if biome_total_counts[style_code] > 0 else 1\n",
    "\n",
    "    sorted_biome_frequency_dict[style_code] = {\n",
    "        biome: biome_counts[biome] / total_biomes\n",
    "        for biome in sorted(biome_counts.keys())  # Sort by biome name\n",
    "    }\n",
    "\n",
    "print(sorted_biome_frequency_dict[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {np.str_('beaches'): 0.012664664858581944, np.str_('birch_forest'): 0.04071086977991507, np.str_('cave'): 0.3224620340193419, np.str_('desert'): 0.042630535360211504, np.str_('extreme_hills'): 0.15213599000235506, np.str_('forest'): 0.14629722481786206, np.str_('mutated_extreme_hills'): 0.00047291291565056863, np.str_('ocean'): 0.02534025039694297, np.str_('plains'): 0.12334860329253747, np.str_('river'): 0.02441863998602153, np.str_('savanna'): 0.029020994294657033, np.str_('swampland'): 0.04432703922328327, np.str_('taiga'): 0.03617024105263958}, 1: {np.str_('beaches'): 0.029120701688594434, np.str_('birch_forest'): 0.032540730439492904, np.str_('cave'): 0.1486817296373845, np.str_('desert'): 0.014762180068127129, np.str_('extreme_hills'): 0.10217897434294822, np.str_('forest'): 0.12550913049366125, np.str_('mutated_extreme_hills'): 0.0002714928591518485, np.str_('ocean'): 0.2171942873214788, np.str_('plains'): 0.1211893340416888, np.str_('river'): 0.07409541444003459, np.str_('savanna'): 0.017540261674844005, np.str_('swampland'): 0.09189284560975863, np.str_('taiga'): 0.02502291738283488}, 2: {np.str_('beaches'): 0.02980474686194972, np.str_('birch_forest'): 0.03586546170322241, np.str_('cave'): 0.111150997355542, np.str_('desert'): 0.07502826937574039, np.str_('extreme_hills'): 0.1028406983283675, np.str_('forest'): 0.1201792487824724, np.str_('mutated_extreme_hills'): 0.00020715918201289922, np.str_('ocean'): 0.24832627346806907, np.str_('plains'): 0.13344641083629488, np.str_('river'): 0.03707775424489356, np.str_('savanna'): 0.02084380347249644, np.str_('swampland'): 0.053080614088619256, np.str_('taiga'): 0.03214856230031949}, 3: {np.str_('beaches'): 0.03713339101921851, np.str_('birch_forest'): 0.047485540577466075, np.str_('cave'): 0.19168412423718006, np.str_('desert'): 0.025711016486018763, np.str_('extreme_hills'): 0.15817697422351762, np.str_('forest'): 0.18330164404772747, np.str_('mutated_extreme_hills'): 0.0013292421896347573, np.str_('ocean'): 0.03528042171418162, np.str_('plains'): 0.12490037799435286, np.str_('river'): 0.07793856453228891, np.str_('savanna'): 0.01757901448219328, np.str_('swampland'): 0.0637353128700246, np.str_('taiga'): 0.035744375626195465}, 4: {np.str_('beaches'): 0.023657995610374356, np.str_('birch_forest'): 0.04372237163436273, np.str_('cave'): 0.04850227209496429, np.str_('desert'): 0.010523895638195925, np.str_('extreme_hills'): 0.11762728368728555, np.str_('forest'): 0.17233917586324152, np.str_('mutated_extreme_hills'): 0.0007129277566539924, np.str_('ocean'): 0.2706285356579802, np.str_('plains'): 0.11846772697765, np.str_('river'): 0.058222433460076044, np.str_('savanna'): 0.02508385112368234, np.str_('swampland'): 0.07390877616000495, np.str_('taiga'): 0.03660275433552815}, 5: {np.str_('beaches'): 0.01814133863552468, np.str_('birch_forest'): 0.06803185509580859, np.str_('cave'): 0.10747004933051445, np.str_('desert'): 0.04834264572636666, np.str_('extreme_hills'): 0.09578026947212993, np.str_('forest'): 0.24044535890466123, np.str_('mutated_extreme_hills'): 0.00013423269237222726, np.str_('ocean'): 0.04680106714990436, np.str_('plains'): 0.20963895600523508, np.str_('river'): 0.03147756636128729, np.str_('savanna'): 0.03221427312997081, np.str_('swampland'): 0.041716479579851674, np.str_('taiga'): 0.05980590791637303}, 6: {np.str_('beaches'): 0.01549413973600364, np.str_('birch_forest'): 0.0534251251706873, np.str_('cave'): 0.019048702776513427, np.str_('desert'): 0.0028191852526172055, np.str_('extreme_hills'): 0.11777281520254892, np.str_('forest'): 0.1850449476558944, np.str_('mutated_extreme_hills'): 0.00032003868912152935, np.str_('ocean'): 0.3322627446517979, np.str_('plains'): 0.10470385753299954, np.str_('river'): 0.07886322257624033, np.str_('savanna'): 0.016287835685025034, np.str_('swampland'): 0.024780951297223488, np.str_('taiga'): 0.04917643377332726}, 7: {np.str_('beaches'): 0.02931348069052115, np.str_('birch_forest'): 0.021098924535355182, np.str_('cave'): 0.04234761854553471, np.str_('desert'): 0.025847815043085684, np.str_('extreme_hills'): 0.05343235691623453, np.str_('forest'): 0.06757359072914969, np.str_('mutated_extreme_hills'): 0.00013838699473769974, np.str_('ocean'): 0.5398728277435372, np.str_('plains'): 0.054918070106492085, np.str_('river'): 0.05797875909400251, np.str_('savanna'): 0.006280133618334659, np.str_('swampland'): 0.08490012173262913, np.str_('taiga'): 0.016297914250385806}, 8: {np.str_('beaches'): 0.017398688860810488, np.str_('birch_forest'): 0.02668287278653702, np.str_('cave'): 0.04614623963083008, np.str_('desert'): 0.006346252449229981, np.str_('extreme_hills'): 0.03863259719093922, np.str_('forest'): 0.10559385815524913, np.str_('mutated_extreme_hills'): 8.469299932245601e-05, np.str_('ocean'): 0.5784394513724844, np.str_('plains'): 0.056921707044626346, np.str_('river'): 0.05023668259810654, np.str_('savanna'): 0.007694473438444213, np.str_('swampland'): 0.042874114157007086, np.str_('taiga'): 0.022948369316413045}, 9: {np.str_('beaches'): 0.0202518830582403, np.str_('birch_forest'): 0.0823543752281767, np.str_('cave'): 0.044357550487097205, np.str_('desert'): 0.047261399227561826, np.str_('extreme_hills'): 0.08585448571373672, np.str_('forest'): 0.3149667102203947, np.str_('mutated_extreme_hills'): 0.0001501162500240186, np.str_('ocean'): 0.010285965451645755, np.str_('plains'): 0.1644319360913091, np.str_('river'): 0.057086207559133795, np.str_('savanna'): 0.030760020559921605, np.str_('swampland'): 0.05670851507407336, np.str_('taiga'): 0.08553083507868493}, 10: {np.str_('beaches'): 0.00784908933217693, np.str_('birch_forest'): 0.07856183716011625, np.str_('cave'): 0.11353752986107941, np.str_('desert'): 0.02420611372316307, np.str_('extreme_hills'): 0.0910188524216004, np.str_('forest'): 0.28375252963284187, np.str_('mutated_extreme_hills'): 8.083413216475708e-05, np.str_('ocean'): 0.00875728458179273, np.str_('plains'): 0.18532965110086577, np.str_('river'): 0.028476628474916693, np.str_('savanna'): 0.032718423335007074, np.str_('swampland'): 0.07891066021515193, np.str_('taiga'): 0.06680056602912311}, 11: {np.str_('beaches'): 0.05281861580847425, np.str_('birch_forest'): 0.042688936918107545, np.str_('cave'): 0.19228576857305718, np.str_('desert'): 0.1417590965228957, np.str_('extreme_hills'): 0.118076667300019, np.str_('forest'): 0.15472401672050162, np.str_('mutated_extreme_hills'): 0.0002464136424092723, np.str_('ocean'): 0.03170720121603648, np.str_('plains'): 0.10365107828234847, np.str_('river'): 0.06338471404142124, np.str_('savanna'): 0.023391482994489836, np.str_('swampland'): 0.03750831274938248, np.str_('taiga'): 0.037757695230856925}, 12: {np.str_('beaches'): 0.032008311922173176, np.str_('birch_forest'): 0.07009821683466193, np.str_('cave'): 0.07323714781386564, np.str_('desert'): 0.059840075103761776, np.str_('extreme_hills'): 0.10051441684783802, np.str_('forest'): 0.23395260996552253, np.str_('mutated_extreme_hills'): 0.00024705184794782267, np.str_('ocean'): 0.027816665568658454, np.str_('plains'): 0.17636756922941785, np.str_('river'): 0.05848952500164701, np.str_('savanna'): 0.02470243977424951, np.str_('swampland'): 0.0766999912159343, np.str_('taiga'): 0.06602597887432197}, 13: {np.str_('beaches'): 0.009667818740399386, np.str_('birch_forest'): 0.0771889400921659, np.str_('cave'): 0.009370199692780337, np.str_('desert'): 0.022532642089093703, np.str_('extreme_hills'): 0.04749423963133641, np.str_('forest'): 0.25304339477726573, np.str_('ocean'): 0.026171274961597542, np.str_('plains'): 0.20950460829493087, np.str_('river'): 0.10360983102918586, np.str_('savanna'): 0.042876344086021506, np.str_('swampland'): 0.14050499231950844, np.str_('taiga'): 0.05803571428571429}, 14: {np.str_('beaches'): 0.008585286368730462, np.str_('birch_forest'): 0.07605962820235544, np.str_('cave'): 0.05671339533237648, np.str_('desert'): 0.012169810857041741, np.str_('extreme_hills'): 0.05997289180314976, np.str_('forest'): 0.28399031183028256, np.str_('mutated_extreme_hills'): 0.0005621940189929416, np.str_('ocean'): 0.09127818957550972, np.str_('plains'): 0.13456590019858045, np.str_('river'): 0.05861932521971648, np.str_('savanna'): 0.024306443050667505, np.str_('swampland'): 0.1311872063073868, np.str_('taiga'): 0.06198941723520969}, 15: {np.str_('beaches'): 0.029045505309301133, np.str_('birch_forest'): 0.04831225140645716, np.str_('cave'): 0.11876948435876303, np.str_('desert'): 0.0531224991340285, np.str_('extreme_hills'): 0.11293462811003213, np.str_('forest'): 0.24127235102304082, np.str_('mutated_extreme_hills'): 0.0008013969016136931, np.str_('ocean'): 0.13322299661972503, np.str_('plains'): 0.10387962697531085, np.str_('river'): 0.04058569534525388, np.str_('savanna'): 0.02157164869029276, np.str_('swampland'): 0.04910767609082548, np.str_('taiga'): 0.04737424003535553}, 16: {np.str_('beaches'): 0.022919248101563743, np.str_('birch_forest'): 0.05416037892580004, np.str_('cave'): 0.2104512636628571, np.str_('desert'): 0.05840071022628945, np.str_('extreme_hills'): 0.09069705664504607, np.str_('forest'): 0.22900082029059746, np.str_('mutated_extreme_hills'): 0.0004304795065796305, np.str_('ocean'): 0.03860297935082825, np.str_('plains'): 0.13942430829081898, np.str_('river'): 0.03638914655166446, np.str_('savanna'): 0.026761980742207238, np.str_('swampland'): 0.04518607098109524, np.str_('taiga'): 0.04757555672465233}, 17: {np.str_('beaches'): 0.04234537412106389, np.str_('birch_forest'): 0.07343415622133904, np.str_('cave'): 0.07694512381534699, np.str_('desert'): 0.20602931060837665, np.str_('extreme_hills'): 0.10249398119841027, np.str_('forest'): 0.20474912106389484, np.str_('mutated_extreme_hills'): 0.00023287029960256803, np.str_('ocean'): 0.03763303462243962, np.str_('plains'): 0.0837377713237542, np.str_('river'): 0.030609905227759096, np.str_('savanna'): 0.018228370528890248, np.str_('swampland'): 0.04779812175175787, np.str_('taiga'): 0.07576285921736473}, 18: {np.str_('beaches'): 0.04211725603070175, np.str_('birch_forest'): 0.056186609100877194, np.str_('cave'): 0.12727521929824562, np.str_('desert'): 0.13887318393640352, np.str_('extreme_hills'): 0.08289645010964912, np.str_('forest'): 0.19103104440789473, np.str_('mutated_extreme_hills'): 8.223684210526316e-05, np.str_('ocean'): 0.03080283717105263, np.str_('plains'): 0.14412092242324562, np.str_('river'): 0.047384697094298246, np.str_('savanna'): 0.026475979989035087, np.str_('swampland'): 0.06325383771929824, np.str_('taiga'): 0.04949972587719298}, 19: {np.str_('beaches'): 0.045872522773309325, np.str_('birch_forest'): 0.04007524964759118, np.str_('cave'): 0.006463000035537024, np.str_('desert'): 0.10197052796171478, np.str_('extreme_hills'): 0.047480573093734824, np.str_('forest'): 0.14207169002238831, np.str_('mutated_extreme_hills'): 1.9545363010696643e-05, np.str_('ocean'): 0.3241078726353072, np.str_('plains'): 0.13812930146057167, np.str_('river'): 0.04700008291972187, np.str_('savanna'): 0.02326209147229889, np.str_('swampland'): 0.04909439818050439, np.str_('taiga'): 0.03445314443430981}}\n"
     ]
    }
   ],
   "source": [
    "print(sorted_biome_frequency_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'beaches', 1: 'birch_forest', 2: 'cave', 3: 'desert', 4: 'extreme_hills', 5: 'forest', 6: 'mutated_extreme_hills', 7: 'ocean', 8: 'plains', 9: 'river', 10: 'savanna', 11: 'swampland', 12: 'taiga'}\n"
     ]
    }
   ],
   "source": [
    "# Biome Mapping\n",
    "print(block_converter.index_to_biome)\n",
    "Biome_mapping = {0: 'beaches', 1: 'birch_forest', 2: 'cave', 3: 'desert', 4: 'extreme_hills', 5: 'forest', 6: 'mutated_extreme_hills', 7: 'ocean', 8: 'plains', 9: 'river', 10: 'savanna', 11: 'swampland', 12: 'taiga'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biome histograms saved in directory: /root/autodl-tmp/minecraft_diffusion-master/Dual_codebook_2.0/Biome_histograms_V1\n"
     ]
    }
   ],
   "source": [
    "# plot histogram of biome frequency. \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "histogram_dir = os.path.join(os.getcwd(), \"Biome_histograms_V1\")\n",
    "os.makedirs(histogram_dir, exist_ok=True)\n",
    "\n",
    "# Assume your biome frequency dictionary is called `sorted_biome_frequency_dict`\n",
    "for style_code, biome_data in sorted_biome_frequency_dict.items():\n",
    "    biome_names = list(biome_data.keys())         # Biome name labels (strings)\n",
    "    frequencies = list(biome_data.values())       # Corresponding frequency ratios\n",
    "\n",
    "    # Sort biome names and frequencies alphabetically\n",
    "    sorted_indices = sorted(range(len(biome_names)), key=lambda k: biome_names[k])\n",
    "    biome_names = [biome_names[i] for i in sorted_indices]\n",
    "    frequencies = [frequencies[i] for i in sorted_indices]\n",
    "\n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(biome_names, frequencies, color=\"seagreen\", alpha=0.75)\n",
    "    plt.xlabel(\"Biome Name\")\n",
    "    plt.ylabel(\"Frequency Ratio\")\n",
    "    plt.title(f\"Biome Frequency for Style Code {style_code}\")\n",
    "    plt.xticks(rotation=90, fontsize=7)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    # Save the histogram image\n",
    "    save_path = os.path.join(histogram_dir, f\"biome_histogram_style_code_{style_code}.png\")\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "print(f\"Biome histograms saved in directory: {histogram_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
