{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.widgets import Slider  # Add this import\n",
    "import numpy as np\n",
    "from diffusion_models3d import Transformer, AbsorbingDiffusion, Block, CausalSelfAttention\n",
    "from sampler_utils import retrieve_autoencoder_components_state_dicts, latent_ids_to_onehot3d, get_latent_loaders\n",
    "from models3d import VQAutoEncoder, Generator\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, fixed\n",
    "# from visualization_utils import MinecraftVisualizer\n",
    "from data_utils import MinecraftVisualizer, get_minecraft_dataloaders, BlockConverter, MinecraftDataset\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from PIL import Image\n",
    "import torch.distributions as dists\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_to_cols = {\n",
    "            0: (0.5, 0.25, 0.0),    # light brown\n",
    "            10: 'black', # bedrock\n",
    "            29: \"#006400\", # cacutus\n",
    "            38: \"#B8860B\",  # clay\n",
    "            60: \"brown\",  # dirt\n",
    "            92: \"gold\",  # gold ore\n",
    "            93: \"green\",  # grass\n",
    "            115: \"brown\",  # ladder...?\n",
    "            119: (.02, .28, .16, 0.8),  # transparent forest green (RGBA) for leaves\n",
    "            120: (.02, .28, .16, 0.8),  # leaves2\n",
    "            194: \"yellow\",  # sand\n",
    "            217: \"gray\",  # stone\n",
    "            240: (0.0, 0.0, 1.0, 0.4),  # water\n",
    "            227: (0.0, 1.0, 0.0, .3), # tall grass\n",
    "            237: (0.33, 0.7, 0.33, 0.3), # vine\n",
    "            40: \"#2F4F4F\",  # coal ore\n",
    "            62: \"#228B22\",  # double plant\n",
    "            108: \"#BEBEBE\",  # iron ore\n",
    "            131: \"saddlebrown\",  # log1\n",
    "            132: \"saddlebrown\",  #log2\n",
    "            95: \"lightgray\",  # gravel\n",
    "            243: \"wheat\",  # wheat. lmao\n",
    "            197: \"limegreen\",  # sapling\n",
    "            166: \"orange\",  #pumpkin\n",
    "            167: \"#FF8C00\",  # pumpkin stem\n",
    "            184: \"#FFA07A\",  # red flower\n",
    "            195: \"tan\",  # sandstone\n",
    "            250: \"white\",  #wool \n",
    "            251: \"gold\",   #yellow flower\n",
    "        }\n",
    "\n",
    "\n",
    "def draw_latent_cuboid(fig, latent_coords, size=4):\n",
    "    \"\"\"\n",
    "    Draw a transparent cuboid around the specified latent coordinates.\n",
    "    \n",
    "    Args:\n",
    "        fig: matplotlib figure to draw on\n",
    "        latent_coords: list of tuples, each containing (d,h,w) coordinates\n",
    "        size: size of each latent cell in final space (default 4 for 6->24 upscaling)\n",
    "    \"\"\"\n",
    "    def cuboid_data(o, sizes):\n",
    "        l, w, h = sizes\n",
    "        x = [[o[0], o[0] + l, o[0] + l, o[0], o[0]],  \n",
    "             [o[0], o[0] + l, o[0] + l, o[0], o[0]],  \n",
    "             [o[0], o[0] + l, o[0] + l, o[0], o[0]],  \n",
    "             [o[0], o[0] + l, o[0] + l, o[0], o[0]]]  \n",
    "        y = [[o[1], o[1], o[1] + w, o[1] + w, o[1]],  \n",
    "             [o[1], o[1], o[1] + w, o[1] + w, o[1]],  \n",
    "             [o[1], o[1], o[1], o[1], o[1]],          \n",
    "             [o[1] + w, o[1] + w, o[1] + w, o[1] + w, o[1] + w]]\n",
    "        z = [[o[2], o[2], o[2], o[2], o[2]],          \n",
    "             [o[2] + h, o[2] + h, o[2] + h, o[2] + h, o[2] + h],\n",
    "             [o[2], o[2], o[2] + h, o[2] + h, o[2]],  \n",
    "             [o[2], o[2], o[2] + h, o[2] + h, o[2]]]  \n",
    "        return np.array(x), np.array(y), np.array(z)\n",
    "\n",
    "    ax = fig.gca()\n",
    "    \n",
    "    # Convert coordinates to numpy array for easier manipulation\n",
    "    coords = np.array(latent_coords)\n",
    "    \n",
    "    # Find min and max for each dimension\n",
    "    d_min, h_min, w_min = coords.min(axis=0)\n",
    "    d_max, h_max, w_max = coords.max(axis=0)\n",
    "    \n",
    "    # Calculate origin and sizes\n",
    "    origin = np.array([abs(5 - d_max)*size, w_min*size, h_min*size])\n",
    "    sizes = (\n",
    "        abs(d_max - d_min + 1) * size,  # length\n",
    "        (w_max - w_min + 1) * size,     # width\n",
    "        (h_max - h_min + 1) * size      # height\n",
    "    )\n",
    "    \n",
    "    # Create and draw single cuboid\n",
    "    X, Y, Z = cuboid_data(origin, sizes)\n",
    "    ax.plot_surface(X, Y, Z, color='red', alpha=0.1)\n",
    "    \n",
    "    # Plot edges\n",
    "    for i in range(4):\n",
    "        ax.plot(X[i], Y[i], Z[i], color='red', linewidth=1)\n",
    "    for i in range(4):\n",
    "        ax.plot([X[0][i], X[1][i]], [Y[0][i], Y[1][i]], [Z[0][i], Z[1][i]], \n",
    "               color='red', linewidth=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def visualize_chunk(voxels, figsize=(10, 10), elev=20, azim=45, highlight_latents=None):\n",
    "    \"\"\"\n",
    "    Optimized version of the 3D visualization of a Minecraft chunk.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # Convert one-hot to block IDs if needed\n",
    "    if isinstance(voxels, torch.Tensor):\n",
    "        if voxels.dim() == 4:  # One-hot encoded [C,H,W,D]\n",
    "            voxels = voxels.detach().cpu()\n",
    "            voxels = torch.argmax(voxels, dim=0).numpy()\n",
    "        else:\n",
    "            voxels = voxels.detach().cpu().numpy()\n",
    "\n",
    "    # Apply the same transformations as original\n",
    "    voxels = voxels.transpose(2, 0, 1) # Moves axes from [D,H,W] to [W,D,H]\n",
    "    voxels = np.rot90(voxels, 1, (0, 1))  # Rotate 90 degrees around height axis\n",
    "    # print([block_id for block_id in np.unique(voxels) if block_id not in blocks_to_cols])\n",
    "    # Create figure and 3D axis\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    # depth, height, width = voxels.shape\n",
    "    # # Set the aspect ratio to match the data dimensions\n",
    "    # ax.set_box_aspect((depth, height, width))\n",
    "    # Generate a single boolean mask for each block type\n",
    "    block_masks = {block_id: (voxels == block_id) for block_id in np.unique(voxels) if block_id in blocks_to_cols}\n",
    "    \n",
    "    # Plot all block types with their respective colors\n",
    "    for block_id, mask in block_masks.items():\n",
    "        ax.voxels(mask, facecolors=blocks_to_cols[int(block_id)])\n",
    "    \n",
    "    # Plot remaining blocks in red with black edges\n",
    "    other_vox = (voxels != 5) & (voxels != -1) & (~np.any(np.stack(list(block_masks.values())), axis=0))\n",
    "    ax.voxels(other_vox, edgecolor=\"k\", facecolor=\"red\")\n",
    "    \n",
    "    # Set default view angle\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "\n",
    "    if highlight_latents is not None:\n",
    "        fig = draw_latent_cuboid(fig, highlight_latents)\n",
    "    \n",
    "    return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block Converter: Converts between block IDs and indices, needed for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_converter = BlockConverter.load_mappings('block_mappings.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minecraft Chunks Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "\n",
    "class MinecraftDataset(Dataset):\n",
    "    def __init__(self, data_path, converter):\n",
    "        # Load data and convert to int16 to save memory\n",
    "        self.chunks = torch.from_numpy(np.load(data_path)).to(torch.int16)\n",
    "        \n",
    "        # Load pre-saved mappings\n",
    "        self.converter = converter\n",
    "        self.num_block_types = len(self.converter.block_to_index)\n",
    "        \n",
    "        # Convert blocks to indices once at initialization\n",
    "        for old_block, new_idx in self.converter.block_to_index.items():\n",
    "            self.chunks[self.chunks == old_block] = new_idx\n",
    "            \n",
    "        # Store air block index\n",
    "        self.air_idx = self.converter.block_to_index[5]\n",
    "        self.target_size = 24\n",
    "\n",
    "        # Pad if needed\n",
    "        pad_size = self.target_size - self.chunks.size(-1)\n",
    "        if pad_size > 0:\n",
    "            self.chunks = F.pad(self.chunks, \n",
    "                              (0, pad_size, 0, pad_size, 0, pad_size), \n",
    "                              value=self.air_idx)\n",
    "\n",
    "        # Convert to one-hot [N, C, H, W, D]\n",
    "        self.processed_chunks = F.one_hot(\n",
    "            self.chunks.long(), \n",
    "            num_classes=self.num_block_types\n",
    "        ).permute(0, 4, 1, 2, 3).float()\n",
    "        \n",
    "        # Free up memory by deleting original chunks\n",
    "        del self.chunks\n",
    "        \n",
    "        print(f\"Loaded {len(self.processed_chunks)} chunks of size {self.processed_chunks.shape[1:]}\")\n",
    "        print(f\"Number of unique block types: {self.num_block_types}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.processed_chunks[idx]\n",
    "\n",
    "    def convert_to_original_blocks(self, data):\n",
    "        \"\"\"Convert from indices back to original block IDs\"\"\"\n",
    "        return self.converter.convert_to_original_blocks(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_chunks)\n",
    "\n",
    "\n",
    "\n",
    "def get_minecraft_dataloader(data_path, converter, batch_size=32, num_workers=0):\n",
    "    \"\"\"\n",
    "    Creates a single dataloader for exploring the entire Minecraft chunks dataset.\n",
    "    \"\"\"\n",
    "    # Create dataset\n",
    "    dataset = MinecraftDataset(data_path, converter)\n",
    "    \n",
    "    # Create dataloader with memory pinning\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,  # No need to shuffle for exploration\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    print(f\"\\nDataloader details:\")\n",
    "    print(f\"Total samples: {len(dataset)}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Number of batches: {len(dataloader)}\")\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load VQGAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from log_utils import log, load_stats, load_model\n",
    "import copy\n",
    "from hyperparams import HparamsVQGAN\n",
    "\n",
    "# Loads hparams from hparams.json file in saved model directory\n",
    "def load_hparams_from_json(log_dir):\n",
    "    import json\n",
    "    import os\n",
    "    json_path = os.path.join(log_dir, 'hparams.json')\n",
    "    \n",
    "    if not os.path.exists(json_path):\n",
    "        raise FileNotFoundError(f\"No hparams.json file found in {log_dir}\")\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        hparams = json.load(f)\n",
    "\n",
    "    return hparams\n",
    "\n",
    "# turns loaded hparams json into propery hyperparams object\n",
    "def dict_to_vcqgan_hparams(hparams_dict, dataset=None):\n",
    "    # Determine which hyperparameter class to use based on the dataset\n",
    "    if dataset == None:\n",
    "        dataset = hparams_dict.get('dataset', 'MNIST')  # Default to MNIST if not specified\n",
    "    \n",
    "    vq_hyper = HparamsVQGAN(dataset)\n",
    "    # Set attributes from the dictionary\n",
    "    for key, value in hparams_dict.items():\n",
    "        setattr(vq_hyper, key, value)\n",
    "    \n",
    "    return vq_hyper\n",
    "\n",
    "\n",
    "def load_vqgan_from_checkpoint(H, vqgan):\n",
    "    vqgan = load_model(vqgan, \"vqgan\", H.load_step, H.load_dir).cuda()\n",
    "    vqgan.eval()\n",
    "    return vqgan\n",
    "\n",
    "\n",
    "def encode_and_quantize(vqgan, terrain_chunks):\n",
    "    vqgan.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded = vqgan.ae.encoder(terrain_chunks)\n",
    "        quantized, _, quant_stats = vqgan.ae.quantize(encoded)\n",
    "        print(f'zq shape: {quantized.size()}')\n",
    "        latent_indices = quant_stats[\"min_encoding_indices\"]\n",
    "        print(f'latent_indices size: {latent_indices.size()}')\n",
    "        latent_indices = latent_indices.view((encoded.size()[0], encoded.size()[2], encoded.size()[3]))\n",
    "        print(f'latent_indices viewed size: {latent_indices.size()}')\n",
    "\n",
    "    return quantized, latent_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'saved_models/minecraft39ch_ce_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resolution: 24, num_resolutions: 3, num_res_blocks: 2, attn_resolutions: [6], in_channels: 256, out_channels: 39, block_in_ch: 256, curr_res: 6\n",
      "Loading vqgan_95000.th\n",
      "loaded from: minecraft39ch_ce_3\n"
     ]
    }
   ],
   "source": [
    "vqgan_hparams =  dict_to_vcqgan_hparams(load_hparams_from_json(f\"{model_path}\"), 'maps')\n",
    "vqgan = VQAutoEncoder(vqgan_hparams)\n",
    "vqgan = load_vqgan_from_checkpoint(vqgan_hparams, vqgan)\n",
    "print(f'loaded from: {vqgan_hparams.log_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10205 chunks of size torch.Size([39, 24, 24, 24])\n",
      "Number of unique block types: 39\n",
      "\n",
      "Dataloader details:\n",
      "Total samples: 10205\n",
      "Batch size: 8\n",
      "Number of batches: 1276\n"
     ]
    }
   ],
   "source": [
    "# This takes a while\n",
    "\n",
    "train_loader= get_minecraft_dataloader(\n",
    "        '../datasets/minecraft_chunks.npy',\n",
    "        block_converter,\n",
    "        batch_size=vqgan_hparams.batch_size,\n",
    "        num_workers=0,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1276/1276 [00:29<00:00, 42.74it/s]\n"
     ]
    }
   ],
   "source": [
    "from sampler_utils import generate_latents_from_loader3d\n",
    "latents = generate_latents_from_loader3d(vqgan_hparams, vqgan, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10205, 216])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
